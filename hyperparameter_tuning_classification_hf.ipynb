{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae3e98696ce5492ea2e8d71cbc1be280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9304457dbea2457da15ec276367e0a7a",
              "IPY_MODEL_942f19bea752465784d0b90cd0b29e32",
              "IPY_MODEL_04784391f85746e39e9d55b40787db62"
            ],
            "layout": "IPY_MODEL_1646345247f94adeacc2edf4488a6cc2"
          }
        },
        "9304457dbea2457da15ec276367e0a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebe8c625503a430b92be4976b4353c29",
            "placeholder": "​",
            "style": "IPY_MODEL_c8187288f2234b37ab60c160b9bba280",
            "value": "Generating test split:  90%"
          }
        },
        "942f19bea752465784d0b90cd0b29e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0d70be85544c669c9974d13575e178",
            "max": 1400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4577d976a01c4971b231cc730103be35",
            "value": 1400
          }
        },
        "04784391f85746e39e9d55b40787db62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_766a7816057a4b4096647d92c0c28326",
            "placeholder": "​",
            "style": "IPY_MODEL_4f89005d3a2c4d39ba0afead91cc66ce",
            "value": " 1261/1400 [00:03&lt;00:00, 787.03 examples/s]"
          }
        },
        "1646345247f94adeacc2edf4488a6cc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ebe8c625503a430b92be4976b4353c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8187288f2234b37ab60c160b9bba280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b0d70be85544c669c9974d13575e178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4577d976a01c4971b231cc730103be35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "766a7816057a4b4096647d92c0c28326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f89005d3a2c4d39ba0afead91cc66ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46daf1d9b39b4b23a35e20971cd367be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ecaeab485c24b58a6e3af6cef04bb0c",
              "IPY_MODEL_ffd52bfccb4642a78ef1096ed59b11de",
              "IPY_MODEL_a47b5d640c7d468d80b57eeb4c12f37e"
            ],
            "layout": "IPY_MODEL_3f7a420fa9c54f97b1e80da260c4fcb9"
          }
        },
        "3ecaeab485c24b58a6e3af6cef04bb0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83e41e9c98a143058f54711ab26b6331",
            "placeholder": "​",
            "style": "IPY_MODEL_79298c9a8c6443e6a408f528dd82b2a3",
            "value": "Generating validation split:  96%"
          }
        },
        "ffd52bfccb4642a78ef1096ed59b11de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7782cdfab1649dba41984ae03150fdd",
            "max": 1400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef8eddffbbc54203846c5f6bd8a4bbe0",
            "value": 1400
          }
        },
        "a47b5d640c7d468d80b57eeb4c12f37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91abddd99e714b54afdb22103b7ffad6",
            "placeholder": "​",
            "style": "IPY_MODEL_3504e35b0b6f44d591ebd38253f9f5c0",
            "value": " 1345/1400 [00:03&lt;00:00, 835.02 examples/s]"
          }
        },
        "3f7a420fa9c54f97b1e80da260c4fcb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "83e41e9c98a143058f54711ab26b6331": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79298c9a8c6443e6a408f528dd82b2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7782cdfab1649dba41984ae03150fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef8eddffbbc54203846c5f6bd8a4bbe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91abddd99e714b54afdb22103b7ffad6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3504e35b0b6f44d591ebd38253f9f5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "781dabde8d80435a8787a35afc5e55a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e40cc2a5331244ca975c029d44f14bc2",
              "IPY_MODEL_65a92518040541ea9273424e288daf98",
              "IPY_MODEL_cdabaa99aed64b1caa2b721d50e2ef64"
            ],
            "layout": "IPY_MODEL_d17c8102551e4288be71898de98eed12"
          }
        },
        "e40cc2a5331244ca975c029d44f14bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_208a04c12cdc4fa385eff9925d4576e0",
            "placeholder": "​",
            "style": "IPY_MODEL_18cf9f398b8c49f38aed880bb450576c",
            "value": "100%"
          }
        },
        "65a92518040541ea9273424e288daf98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea9f1429db0247e1820b048c3ab14a14",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6052a5f0c1764cab9f193487f895ebd0",
            "value": 3
          }
        },
        "cdabaa99aed64b1caa2b721d50e2ef64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a40a005cbe9429689c59fc0cc9365b0",
            "placeholder": "​",
            "style": "IPY_MODEL_43489e30711c4f1f90768e291889f4fd",
            "value": " 3/3 [00:00&lt;00:00, 89.26it/s]"
          }
        },
        "d17c8102551e4288be71898de98eed12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "208a04c12cdc4fa385eff9925d4576e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18cf9f398b8c49f38aed880bb450576c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea9f1429db0247e1820b048c3ab14a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6052a5f0c1764cab9f193487f895ebd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a40a005cbe9429689c59fc0cc9365b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43489e30711c4f1f90768e291889f4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d09abbb011c4c389bfeffbd57a4bebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5081c0230cdc40b582600f69c1dcd4e7",
              "IPY_MODEL_ecb6dcf4354544acb4cbc56b76b4204d",
              "IPY_MODEL_96d794feefc4485482d85fa141ace98f"
            ],
            "layout": "IPY_MODEL_aaaa165e63e646e8af999a431c944af5"
          }
        },
        "5081c0230cdc40b582600f69c1dcd4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab290e58be04a04aa3384b1768ee076",
            "placeholder": "​",
            "style": "IPY_MODEL_03ae70e529514cabb97671e8efe6f64c",
            "value": "tokenizing the entire dataset: 100%"
          }
        },
        "ecb6dcf4354544acb4cbc56b76b4204d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b824ab93368546d4aea9896a99a94d2d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fda3dcea6bff428aa35a45fc32da749d",
            "value": 1
          }
        },
        "96d794feefc4485482d85fa141ace98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb83d36ebb724daaa896d0ed8ba62336",
            "placeholder": "​",
            "style": "IPY_MODEL_0fd1bd9e06c24a068f4cd7a4ac60c264",
            "value": " 1/1 [00:05&lt;00:00,  5.25s/ba]"
          }
        },
        "aaaa165e63e646e8af999a431c944af5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab290e58be04a04aa3384b1768ee076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03ae70e529514cabb97671e8efe6f64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b824ab93368546d4aea9896a99a94d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fda3dcea6bff428aa35a45fc32da749d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb83d36ebb724daaa896d0ed8ba62336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fd1bd9e06c24a068f4cd7a4ac60c264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/hyperparameter_tuning_classification_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebooks provides some hyperparameter tuning"
      ],
      "metadata": {
        "id": "p-QOMwMKAuIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Dataset"
      ],
      "metadata": {
        "id": "ux8okenbCQ1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65-gK429AtPx"
      },
      "outputs": [],
      "source": [
        "#load packages\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import TrainerCallback \n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "import torch as nn\n",
        "     "
      ],
      "metadata": {
        "id": "fvATIDR6gMhw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data \n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset=load_dataset(\"lex_glue\",\"scotus\")\n",
        "train_dataset=dataset['train']\n",
        "train_dataset = train_dataset.shard(index=1, num_shards=10)"
      ],
      "metadata": {
        "id": "skvLnAAXBXig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "0b7e1ea2883249a6bf4bf9f4ef65e17e",
            "ae3e98696ce5492ea2e8d71cbc1be280",
            "9304457dbea2457da15ec276367e0a7a",
            "942f19bea752465784d0b90cd0b29e32",
            "04784391f85746e39e9d55b40787db62",
            "1646345247f94adeacc2edf4488a6cc2",
            "ebe8c625503a430b92be4976b4353c29",
            "c8187288f2234b37ab60c160b9bba280",
            "5b0d70be85544c669c9974d13575e178",
            "4577d976a01c4971b231cc730103be35",
            "766a7816057a4b4096647d92c0c28326",
            "4f89005d3a2c4d39ba0afead91cc66ce",
            "46daf1d9b39b4b23a35e20971cd367be",
            "3ecaeab485c24b58a6e3af6cef04bb0c",
            "ffd52bfccb4642a78ef1096ed59b11de",
            "a47b5d640c7d468d80b57eeb4c12f37e",
            "3f7a420fa9c54f97b1e80da260c4fcb9",
            "83e41e9c98a143058f54711ab26b6331",
            "79298c9a8c6443e6a408f528dd82b2a3",
            "c7782cdfab1649dba41984ae03150fdd",
            "ef8eddffbbc54203846c5f6bd8a4bbe0",
            "91abddd99e714b54afdb22103b7ffad6",
            "3504e35b0b6f44d591ebd38253f9f5c0",
            "781dabde8d80435a8787a35afc5e55a2",
            "e40cc2a5331244ca975c029d44f14bc2",
            "65a92518040541ea9273424e288daf98",
            "cdabaa99aed64b1caa2b721d50e2ef64",
            "d17c8102551e4288be71898de98eed12",
            "208a04c12cdc4fa385eff9925d4576e0",
            "18cf9f398b8c49f38aed880bb450576c",
            "ea9f1429db0247e1820b048c3ab14a14",
            "6052a5f0c1764cab9f193487f895ebd0",
            "7a40a005cbe9429689c59fc0cc9365b0",
            "43489e30711c4f1f90768e291889f4fd"
          ]
        },
        "outputId": "5d24649a-056b-4a53-bc2b-302530f6ab64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b7e1ea2883249a6bf4bf9f4ef65e17e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae3e98696ce5492ea2e8d71cbc1be280"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/1400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46daf1d9b39b4b23a35e20971cd367be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset lex_glue downloaded and prepared to /root/.cache/huggingface/datasets/lex_glue/scotus/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "781dabde8d80435a8787a35afc5e55a2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset=dataset['test']\n",
        "test_dataset = test_dataset.shard(index=1, num_shards=10)"
      ],
      "metadata": {
        "id": "Qj26M-T3e7bs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = test_dataset.map(\n",
        "      preprocess_function,\n",
        "      batched=True,\n",
        "      desc=\"tokenizing the entire dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1d09abbb011c4c389bfeffbd57a4bebd",
            "5081c0230cdc40b582600f69c1dcd4e7",
            "ecb6dcf4354544acb4cbc56b76b4204d",
            "96d794feefc4485482d85fa141ace98f",
            "aaaa165e63e646e8af999a431c944af5",
            "6ab290e58be04a04aa3384b1768ee076",
            "03ae70e529514cabb97671e8efe6f64c",
            "b824ab93368546d4aea9896a99a94d2d",
            "fda3dcea6bff428aa35a45fc32da749d",
            "cb83d36ebb724daaa896d0ed8ba62336",
            "0fd1bd9e06c24a068f4cd7a4ac60c264"
          ]
        },
        "id": "rFnEni3VfDoG",
        "outputId": "f33b3537-4557-4904-b8c3-fa2d1e3e3bb5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizing the entire dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d09abbb011c4c389bfeffbd57a4bebd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "padding=\"max_length\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('danielsaggau/bregman_1.5', use_fast=True)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "      return tokenizer(examples[\"text\"], truncation=True, padding=padding)\n",
        "\n",
        "tokenized_data = train_dataset.map(\n",
        "      preprocess_function,\n",
        "      batched=True,\n",
        "      desc=\"tokenizing the entire dataset\")"
      ],
      "metadata": {
        "id": "bMqVrzJ9D0qL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df8ec72-49f8-4d2e-c379-d858db05efc2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/tokenizer_config.json\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/lex_glue/scotus/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a/cache-a8fffd4a6e0d370c.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Trainer "
      ],
      "metadata": {
        "id": "5q2aXdRJCPE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Metric Function\n"
      ],
      "metadata": {
        "id": "7njhUCt5DNbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "  metric1 = load_metric(\"f1\")\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  micro1 = metric1.compute(predictions=predictions, references=labels, average=\"micro\")[\"f1\"]\n",
        "  macro1 = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "  return { \"f1-micro\": micro1, \"f1-macro\": macro1} "
      ],
      "metadata": {
        "id": "1fTpoMJCDPiB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Arguments"
      ],
      "metadata": {
        "id": "sMqam_hLDSVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='scotus_max_linear',\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=6,\n",
        "    per_device_eval_batch_size=6,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        " #   push_to_hub=True,\n",
        "    metric_for_best_model=\"f1-micro\",\n",
        "    fp16=True,\n",
        "#    report_to=\"wandb\",\n",
        "    greater_is_better=True,\n",
        "    lr_scheduler_type='linear',\n",
        " #   run_name=\"max\",\n",
        "    load_best_model_at_end = True\n",
        ")"
      ],
      "metadata": {
        "id": "DlIr2paDDVMe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) # fp16"
      ],
      "metadata": {
        "id": "9LKsuWW6Lb03"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oe1b4VzEDUUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained('danielsaggau/bregman_1.5', num_labels=14)\n"
      ],
      "metadata": {
        "id": "aLpJjGZOOgth"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, EarlyStoppingCallback\n",
        "trainer = Trainer(\n",
        "    compute_metrics=compute_metrics,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,    \n",
        "    model_init=model_init,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "      )"
      ],
      "metadata": {
        "id": "pOSJjE6ACHW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612bc5ba-89e6-44b4-f7e3-f3cd9a0b573a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "Using cuda_amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define hyperparameter space "
      ],
      "metadata": {
        "id": "lcu7FEnwDGBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01,0.05),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2,3,4,6,8]),\n",
        "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [4,5,6,7, 10,15]),\n",
        "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", ['linear', 'cosine', 'polynomial','constant', 'constant_with_warmup'])\n",
        "    }"
      ],
      "metadata": {
        "id": "SPmxIxVtCG1N"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "!pip install optuna\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb_kwargs = {\"project\": \"hyperparameters\"}"
      ],
      "metadata": {
        "id": "Av3es_N7CLWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"hyperparameters\",name=\"short_run\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "opMZITXaipN6",
        "outputId": "01841bfc-6705-4896-b46c-24c3a82c4b91"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221212_085500-1i2gqumy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/danielsaggau/hyperparameters/runs/1i2gqumy\" target=\"_blank\">short_run</a></strong> to <a href=\"https://wandb.ai/danielsaggau/hyperparameters\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/danielsaggau/hyperparameters/runs/1i2gqumy?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f29b64a3ee0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install wandb\n",
        "import optuna\n",
        "!pip install shortuuid==1.0.1\n",
        "import wandb\n",
        "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
        "wandb_kwargs = {\"project\": \"hyperparameters\"}\n",
        "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)"
      ],
      "metadata": {
        "id": "dfztFh8NjlFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = trainer.hyperparameter_search(\n",
        "    n_trials=15, direction=\"maximize\", hp_space=hp_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wSDcohY_ieGD",
        "outputId": "d5af8afc-f6ea-48af-8d69-5d58241b18bc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-12 09:00:49,138]\u001b[0m A new study created in memory with name: no-name-f198c3e0-e7dd-450a-9a8e-b1d028794013\u001b[0m\n",
            "Trial: {'learning_rate': 3.6107212281458205e-05, 'weight_decay': 0.026672900647824342, 'per_device_train_batch_size': 6, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 588\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='588' max='588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [588/588 09:10, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.530931</td>\n",
              "      <td>0.585714</td>\n",
              "      <td>0.204468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.300048</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.265071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.189087</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.304178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.164882</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.332559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.197506</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.332186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.021600</td>\n",
              "      <td>1.177126</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.373905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.021600</td>\n",
              "      <td>1.184705</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.376037</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='836' max='2505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 836/2505 06:43 < 13:27, 2.07 it/s, Epoch 5/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.482125</td>\n",
              "      <td>0.564286</td>\n",
              "      <td>0.183688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.290993</td>\n",
              "      <td>0.621429</td>\n",
              "      <td>0.299403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.442200</td>\n",
              "      <td>1.202354</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.330360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.442200</td>\n",
              "      <td>1.294336</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.356255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24/24 11:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-0/checkpoint-84\n",
            "Configuration saved in scotus_max_linear/run-0/checkpoint-84/config.json\n",
            "Model weights saved in scotus_max_linear/run-0/checkpoint-84/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-0/checkpoint-84/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-0/checkpoint-84/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-0/checkpoint-168\n",
            "Configuration saved in scotus_max_linear/run-0/checkpoint-168/config.json\n",
            "Model weights saved in scotus_max_linear/run-0/checkpoint-168/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-0/checkpoint-168/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-0/checkpoint-168/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-0/checkpoint-252\n",
            "Configuration saved in scotus_max_linear/run-0/checkpoint-252/config.json\n",
            "Model weights saved in scotus_max_linear/run-0/checkpoint-252/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-0/checkpoint-252/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-0/checkpoint-252/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-0/checkpoint-336\n",
            "Configuration saved in scotus_max_linear/run-0/checkpoint-336/config.json\n",
            "Model weights saved in scotus_max_linear/run-0/checkpoint-336/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-0/checkpoint-336/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-0/checkpoint-336/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-0/checkpoint-420\n",
            "Configuration saved in scotus_max_linear/run-0/checkpoint-420/config.json\n",
            "Model weights saved in scotus_max_linear/run-0/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-0/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-0/checkpoint-420/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-0/checkpoint-504\n",
            "Configuration saved in scotus_max_linear/run-0/checkpoint-504/config.json\n",
            "Model weights saved in scotus_max_linear/run-0/checkpoint-504/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-0/checkpoint-504/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-0/checkpoint-504/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-0/checkpoint-588\n",
            "Configuration saved in scotus_max_linear/run-0/checkpoint-588/config.json\n",
            "Model weights saved in scotus_max_linear/run-0/checkpoint-588/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-0/checkpoint-588/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-0/checkpoint-588/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-0/checkpoint-588 (score: 0.6857142857142857).\n",
            "\u001b[32m[I 2022-12-12 09:10:02,196]\u001b[0m Trial 0 finished with value: 1.0617515822621284 and parameters: {'learning_rate': 3.6107212281458205e-05, 'weight_decay': 0.026672900647824342, 'per_device_train_batch_size': 6, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 1.3810706024791814e-05, 'weight_decay': 0.026917118672782826, 'per_device_train_batch_size': 6, 'num_train_epochs': 10, 'lr_scheduler_type': 'polynomial'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 840\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='840' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [840/840 13:13, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.942724</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.113813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.644531</td>\n",
              "      <td>0.528571</td>\n",
              "      <td>0.174646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.473308</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.260807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.360925</td>\n",
              "      <td>0.621429</td>\n",
              "      <td>0.273482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.280127</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.296961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.579800</td>\n",
              "      <td>1.234453</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.323276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.579800</td>\n",
              "      <td>1.219307</td>\n",
              "      <td>0.635714</td>\n",
              "      <td>0.294854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.579800</td>\n",
              "      <td>1.193324</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.299283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.579800</td>\n",
              "      <td>1.198131</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.322689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.579800</td>\n",
              "      <td>1.190861</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.328529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-84\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-84/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-84/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-84/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-84/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-168\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-168/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-168/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-168/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-168/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-252\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-252/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-252/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-252/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-252/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-336\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-336/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-336/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-336/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-336/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-420\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-420/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-420/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-504\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-504/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-504/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-504/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-504/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-588\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-588/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-588/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-588/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-588/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-672\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-672/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-672/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-672/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-672/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-756\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-756/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-756/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-756/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-756/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-1/checkpoint-840\n",
            "Configuration saved in scotus_max_linear/run-1/checkpoint-840/config.json\n",
            "Model weights saved in scotus_max_linear/run-1/checkpoint-840/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-1/checkpoint-840/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-1/checkpoint-840/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-1/checkpoint-420 (score: 0.6785714285714286).\n",
            "\u001b[32m[I 2022-12-12 09:23:17,520]\u001b[0m Trial 1 finished with value: 0.9856715481715481 and parameters: {'learning_rate': 1.3810706024791814e-05, 'weight_decay': 0.026917118672782826, 'per_device_train_batch_size': 6, 'num_train_epochs': 10, 'lr_scheduler_type': 'polynomial'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 6.309690931314233e-05, 'weight_decay': 0.04028605100206237, 'per_device_train_batch_size': 6, 'num_train_epochs': 15, 'lr_scheduler_type': 'constant_with_warmup'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 15\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1260\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='756' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 756/1260 11:52 < 07:56, 1.06 it/s, Epoch 9/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.380272</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.208101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.345717</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.284519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.274755</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.363149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.182037</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.459264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.588723</td>\n",
              "      <td>0.635714</td>\n",
              "      <td>0.386589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>1.923206</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.352489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>1.848000</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.404662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>1.907236</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.430701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>2.293891</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.389957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-84\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-84/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-84/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-84/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-84/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-168\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-168/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-168/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-168/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-168/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-252\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-252/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-252/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-252/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-252/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-336\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-336/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-336/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-336/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-336/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-420\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-420/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-420/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-504\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-504/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-504/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-504/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-504/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-588\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-588/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-588/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-588/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-588/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-672\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-672/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-672/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-672/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-672/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-2/checkpoint-756\n",
            "Configuration saved in scotus_max_linear/run-2/checkpoint-756/config.json\n",
            "Model weights saved in scotus_max_linear/run-2/checkpoint-756/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-2/checkpoint-756/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-2/checkpoint-756/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-2/checkpoint-336 (score: 0.6857142857142857).\n",
            "\u001b[32m[I 2022-12-12 09:35:11,703]\u001b[0m Trial 2 finished with value: 1.0042432047725458 and parameters: {'learning_rate': 6.309690931314233e-05, 'weight_decay': 0.04028605100206237, 'per_device_train_batch_size': 6, 'num_train_epochs': 15, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 5.3525617113392966e-05, 'weight_decay': 0.04496685779733763, 'per_device_train_batch_size': 6, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 588\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='588' max='588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [588/588 09:14, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.410125</td>\n",
              "      <td>0.592857</td>\n",
              "      <td>0.205222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.323292</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.286314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.174889</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.324960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.191770</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.377162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.244882</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.370238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.875600</td>\n",
              "      <td>1.230967</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.378502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.875600</td>\n",
              "      <td>1.236089</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.379854</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-3/checkpoint-84\n",
            "Configuration saved in scotus_max_linear/run-3/checkpoint-84/config.json\n",
            "Model weights saved in scotus_max_linear/run-3/checkpoint-84/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-3/checkpoint-84/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-3/checkpoint-84/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-3/checkpoint-168\n",
            "Configuration saved in scotus_max_linear/run-3/checkpoint-168/config.json\n",
            "Model weights saved in scotus_max_linear/run-3/checkpoint-168/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-3/checkpoint-168/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-3/checkpoint-168/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-3/checkpoint-252\n",
            "Configuration saved in scotus_max_linear/run-3/checkpoint-252/config.json\n",
            "Model weights saved in scotus_max_linear/run-3/checkpoint-252/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-3/checkpoint-252/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-3/checkpoint-252/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-3/checkpoint-336\n",
            "Configuration saved in scotus_max_linear/run-3/checkpoint-336/config.json\n",
            "Model weights saved in scotus_max_linear/run-3/checkpoint-336/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-3/checkpoint-336/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-3/checkpoint-336/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-3/checkpoint-420\n",
            "Configuration saved in scotus_max_linear/run-3/checkpoint-420/config.json\n",
            "Model weights saved in scotus_max_linear/run-3/checkpoint-420/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-3/checkpoint-420/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-3/checkpoint-420/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-3/checkpoint-504\n",
            "Configuration saved in scotus_max_linear/run-3/checkpoint-504/config.json\n",
            "Model weights saved in scotus_max_linear/run-3/checkpoint-504/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-3/checkpoint-504/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-3/checkpoint-504/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-3/checkpoint-588\n",
            "Configuration saved in scotus_max_linear/run-3/checkpoint-588/config.json\n",
            "Model weights saved in scotus_max_linear/run-3/checkpoint-588/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-3/checkpoint-588/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-3/checkpoint-588/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-3/checkpoint-504 (score: 0.6785714285714286).\n",
            "\u001b[32m[I 2022-12-12 09:44:28,503]\u001b[0m Trial 3 finished with value: 1.058425562906695 and parameters: {'learning_rate': 5.3525617113392966e-05, 'weight_decay': 0.04496685779733763, 'per_device_train_batch_size': 6, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 0.0007536701130343977, 'weight_decay': 0.04525587306517933, 'per_device_train_batch_size': 4, 'num_train_epochs': 5, 'lr_scheduler_type': 'constant_with_warmup'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 625\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 06:43, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.127093</td>\n",
              "      <td>0.135714</td>\n",
              "      <td>0.019916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.340670</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.021739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.051695</td>\n",
              "      <td>0.314286</td>\n",
              "      <td>0.039855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.296900</td>\n",
              "      <td>2.131682</td>\n",
              "      <td>0.314286</td>\n",
              "      <td>0.039855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.296900</td>\n",
              "      <td>2.117508</td>\n",
              "      <td>0.135714</td>\n",
              "      <td>0.019916</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-4/checkpoint-125\n",
            "Configuration saved in scotus_max_linear/run-4/checkpoint-125/config.json\n",
            "Model weights saved in scotus_max_linear/run-4/checkpoint-125/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-4/checkpoint-125/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-4/checkpoint-125/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-4/checkpoint-250\n",
            "Configuration saved in scotus_max_linear/run-4/checkpoint-250/config.json\n",
            "Model weights saved in scotus_max_linear/run-4/checkpoint-250/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-4/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-4/checkpoint-250/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-4/checkpoint-375\n",
            "Configuration saved in scotus_max_linear/run-4/checkpoint-375/config.json\n",
            "Model weights saved in scotus_max_linear/run-4/checkpoint-375/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-4/checkpoint-375/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-4/checkpoint-375/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-4/checkpoint-500\n",
            "Configuration saved in scotus_max_linear/run-4/checkpoint-500/config.json\n",
            "Model weights saved in scotus_max_linear/run-4/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-4/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-4/checkpoint-500/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-4/checkpoint-625\n",
            "Configuration saved in scotus_max_linear/run-4/checkpoint-625/config.json\n",
            "Model weights saved in scotus_max_linear/run-4/checkpoint-625/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-4/checkpoint-625/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-4/checkpoint-625/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-4/checkpoint-375 (score: 0.3142857142857143).\n",
            "\u001b[32m[I 2022-12-12 09:51:13,184]\u001b[0m Trial 4 finished with value: 0.15563042827193768 and parameters: {'learning_rate': 0.0007536701130343977, 'weight_decay': 0.04525587306517933, 'per_device_train_batch_size': 4, 'num_train_epochs': 5, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 4.121335586414558e-05, 'weight_decay': 0.024327189817316897, 'per_device_train_batch_size': 6, 'num_train_epochs': 15, 'lr_scheduler_type': 'linear'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 15\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1260\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='168' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 168/1260 02:37 < 17:13, 1.06 it/s, Epoch 2/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.487922</td>\n",
              "      <td>0.592857</td>\n",
              "      <td>0.207953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.326248</td>\n",
              "      <td>0.592857</td>\n",
              "      <td>0.258489</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-5/checkpoint-84\n",
            "Configuration saved in scotus_max_linear/run-5/checkpoint-84/config.json\n",
            "Model weights saved in scotus_max_linear/run-5/checkpoint-84/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-5/checkpoint-84/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-5/checkpoint-84/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "\u001b[32m[I 2022-12-12 09:53:52,297]\u001b[0m Trial 5 pruned. \u001b[0m\n",
            "Trial: {'learning_rate': 0.00017815702860135248, 'weight_decay': 0.036272523096017584, 'per_device_train_batch_size': 8, 'num_train_epochs': 7, 'lr_scheduler_type': 'constant_with_warmup'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 441\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='441' max='441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [441/441 09:09, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.419182</td>\n",
              "      <td>0.621429</td>\n",
              "      <td>0.269489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.441407</td>\n",
              "      <td>0.528571</td>\n",
              "      <td>0.259981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.421885</td>\n",
              "      <td>0.578571</td>\n",
              "      <td>0.237446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.415152</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.373378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.600371</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.363704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.877621</td>\n",
              "      <td>0.592857</td>\n",
              "      <td>0.299391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.077620</td>\n",
              "      <td>0.557143</td>\n",
              "      <td>0.412049</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-6/checkpoint-63\n",
            "Configuration saved in scotus_max_linear/run-6/checkpoint-63/config.json\n",
            "Model weights saved in scotus_max_linear/run-6/checkpoint-63/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-6/checkpoint-63/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-6/checkpoint-63/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-6/checkpoint-126\n",
            "Configuration saved in scotus_max_linear/run-6/checkpoint-126/config.json\n",
            "Model weights saved in scotus_max_linear/run-6/checkpoint-126/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-6/checkpoint-126/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-6/checkpoint-126/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-6/checkpoint-189\n",
            "Configuration saved in scotus_max_linear/run-6/checkpoint-189/config.json\n",
            "Model weights saved in scotus_max_linear/run-6/checkpoint-189/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-6/checkpoint-189/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-6/checkpoint-189/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-6/checkpoint-252\n",
            "Configuration saved in scotus_max_linear/run-6/checkpoint-252/config.json\n",
            "Model weights saved in scotus_max_linear/run-6/checkpoint-252/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-6/checkpoint-252/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-6/checkpoint-252/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-6/checkpoint-315\n",
            "Configuration saved in scotus_max_linear/run-6/checkpoint-315/config.json\n",
            "Model weights saved in scotus_max_linear/run-6/checkpoint-315/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-6/checkpoint-315/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-6/checkpoint-315/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-6/checkpoint-378\n",
            "Configuration saved in scotus_max_linear/run-6/checkpoint-378/config.json\n",
            "Model weights saved in scotus_max_linear/run-6/checkpoint-378/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-6/checkpoint-378/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-6/checkpoint-378/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-6/checkpoint-441\n",
            "Configuration saved in scotus_max_linear/run-6/checkpoint-441/config.json\n",
            "Model weights saved in scotus_max_linear/run-6/checkpoint-441/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-6/checkpoint-441/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-6/checkpoint-441/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-6/checkpoint-252 (score: 0.65).\n",
            "\u001b[32m[I 2022-12-12 10:03:04,331]\u001b[0m Trial 6 finished with value: 0.9691919764015324 and parameters: {'learning_rate': 0.00017815702860135248, 'weight_decay': 0.036272523096017584, 'per_device_train_batch_size': 8, 'num_train_epochs': 7, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 0.00028761916445278583, 'weight_decay': 0.03134838750043154, 'per_device_train_batch_size': 6, 'num_train_epochs': 15, 'lr_scheduler_type': 'constant'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 15\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1260\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='84' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  84/1260 01:17 < 18:25, 1.06 it/s, Epoch 1/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.867848</td>\n",
              "      <td>0.442857</td>\n",
              "      <td>0.115692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "\u001b[32m[I 2022-12-12 10:04:23,302]\u001b[0m Trial 7 pruned. \u001b[0m\n",
            "Trial: {'learning_rate': 1.1175609944471508e-05, 'weight_decay': 0.031583427342734886, 'per_device_train_batch_size': 2, 'num_train_epochs': 7, 'lr_scheduler_type': 'linear'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1750\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1750/1750 10:21, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.750624</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.175888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.927300</td>\n",
              "      <td>1.468527</td>\n",
              "      <td>0.585714</td>\n",
              "      <td>0.222808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.927300</td>\n",
              "      <td>1.293567</td>\n",
              "      <td>0.628571</td>\n",
              "      <td>0.270660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.238400</td>\n",
              "      <td>1.207684</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.294242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.238400</td>\n",
              "      <td>1.199306</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.310634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.908800</td>\n",
              "      <td>1.199649</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.341555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.908800</td>\n",
              "      <td>1.204960</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.333014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-8/checkpoint-250\n",
            "Configuration saved in scotus_max_linear/run-8/checkpoint-250/config.json\n",
            "Model weights saved in scotus_max_linear/run-8/checkpoint-250/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-8/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-8/checkpoint-250/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-8/checkpoint-500\n",
            "Configuration saved in scotus_max_linear/run-8/checkpoint-500/config.json\n",
            "Model weights saved in scotus_max_linear/run-8/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-8/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-8/checkpoint-500/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-8/checkpoint-750\n",
            "Configuration saved in scotus_max_linear/run-8/checkpoint-750/config.json\n",
            "Model weights saved in scotus_max_linear/run-8/checkpoint-750/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-8/checkpoint-750/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-8/checkpoint-750/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-8/checkpoint-1000\n",
            "Configuration saved in scotus_max_linear/run-8/checkpoint-1000/config.json\n",
            "Model weights saved in scotus_max_linear/run-8/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-8/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-8/checkpoint-1000/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-8/checkpoint-1250\n",
            "Configuration saved in scotus_max_linear/run-8/checkpoint-1250/config.json\n",
            "Model weights saved in scotus_max_linear/run-8/checkpoint-1250/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-8/checkpoint-1250/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-8/checkpoint-1250/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-8/checkpoint-1500\n",
            "Configuration saved in scotus_max_linear/run-8/checkpoint-1500/config.json\n",
            "Model weights saved in scotus_max_linear/run-8/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-8/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-8/checkpoint-1500/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-8/checkpoint-1750\n",
            "Configuration saved in scotus_max_linear/run-8/checkpoint-1750/config.json\n",
            "Model weights saved in scotus_max_linear/run-8/checkpoint-1750/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-8/checkpoint-1750/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-8/checkpoint-1750/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-8/checkpoint-1500 (score: 0.6857142857142857).\n",
            "\u001b[32m[I 2022-12-12 10:14:45,792]\u001b[0m Trial 8 finished with value: 1.004442301237717 and parameters: {'learning_rate': 1.1175609944471508e-05, 'weight_decay': 0.031583427342734886, 'per_device_train_batch_size': 2, 'num_train_epochs': 7, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 0.0003981695713605193, 'weight_decay': 0.044925883869829014, 'per_device_train_batch_size': 2, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1750\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 250/1750 01:26 < 08:45, 2.85 it/s, Epoch 1/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.107275</td>\n",
              "      <td>0.314286</td>\n",
              "      <td>0.039855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "\u001b[32m[I 2022-12-12 10:16:14,068]\u001b[0m Trial 9 pruned. \u001b[0m\n",
            "Trial: {'learning_rate': 2.566260537998998e-05, 'weight_decay': 0.01284525176171247, 'per_device_train_batch_size': 3, 'num_train_epochs': 4, 'lr_scheduler_type': 'cosine'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 3\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 668\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='668' max='668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [668/668 05:36, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.505071</td>\n",
              "      <td>0.557143</td>\n",
              "      <td>0.180962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.274806</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.290269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.484600</td>\n",
              "      <td>1.210081</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.330163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.484600</td>\n",
              "      <td>1.216039</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.331175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-10/checkpoint-167\n",
            "Configuration saved in scotus_max_linear/run-10/checkpoint-167/config.json\n",
            "Model weights saved in scotus_max_linear/run-10/checkpoint-167/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-10/checkpoint-167/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-10/checkpoint-167/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-10/checkpoint-334\n",
            "Configuration saved in scotus_max_linear/run-10/checkpoint-334/config.json\n",
            "Model weights saved in scotus_max_linear/run-10/checkpoint-334/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-10/checkpoint-334/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-10/checkpoint-334/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-10/checkpoint-501\n",
            "Configuration saved in scotus_max_linear/run-10/checkpoint-501/config.json\n",
            "Model weights saved in scotus_max_linear/run-10/checkpoint-501/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-10/checkpoint-501/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-10/checkpoint-501/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-10/checkpoint-668\n",
            "Configuration saved in scotus_max_linear/run-10/checkpoint-668/config.json\n",
            "Model weights saved in scotus_max_linear/run-10/checkpoint-668/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-10/checkpoint-668/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-10/checkpoint-668/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-10/checkpoint-334 (score: 0.6642857142857143).\n",
            "\u001b[32m[I 2022-12-12 10:21:52,123]\u001b[0m Trial 10 finished with value: 0.9954610332545236 and parameters: {'learning_rate': 2.566260537998998e-05, 'weight_decay': 0.01284525176171247, 'per_device_train_batch_size': 3, 'num_train_epochs': 4, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 9.82051907328149e-05, 'weight_decay': 0.019729142322149, 'per_device_train_batch_size': 6, 'num_train_epochs': 6, 'lr_scheduler_type': 'cosine'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 504\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='84' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 84/504 01:17 < 06:35, 1.06 it/s, Epoch 1/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.443396</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.179336</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "\u001b[32m[I 2022-12-12 10:23:11,214]\u001b[0m Trial 11 pruned. \u001b[0m\n",
            "Trial: {'learning_rate': 3.1215077038601496e-05, 'weight_decay': 0.04887701760964189, 'per_device_train_batch_size': 8, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 441\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='441' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 63/441 01:16 < 07:53, 0.80 it/s, Epoch 1/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.713693</td>\n",
              "      <td>0.542857</td>\n",
              "      <td>0.176394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "\u001b[32m[I 2022-12-12 10:24:29,696]\u001b[0m Trial 12 pruned. \u001b[0m\n",
            "Trial: {'learning_rate': 9.141669532957721e-05, 'weight_decay': 0.01851059673861115, 'per_device_train_batch_size': 4, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 875\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/875 08:07 < 01:21, 1.53 it/s, Epoch 6/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.303014</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.255809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.568679</td>\n",
              "      <td>0.578571</td>\n",
              "      <td>0.291293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.659657</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.294559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.076000</td>\n",
              "      <td>1.599705</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.321477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.076000</td>\n",
              "      <td>1.898735</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.330249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.076000</td>\n",
              "      <td>1.854603</td>\n",
              "      <td>0.635714</td>\n",
              "      <td>0.375057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-13/checkpoint-125\n",
            "Configuration saved in scotus_max_linear/run-13/checkpoint-125/config.json\n",
            "Model weights saved in scotus_max_linear/run-13/checkpoint-125/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-13/checkpoint-125/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-13/checkpoint-125/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-13/checkpoint-250\n",
            "Configuration saved in scotus_max_linear/run-13/checkpoint-250/config.json\n",
            "Model weights saved in scotus_max_linear/run-13/checkpoint-250/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-13/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-13/checkpoint-250/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-13/checkpoint-375\n",
            "Configuration saved in scotus_max_linear/run-13/checkpoint-375/config.json\n",
            "Model weights saved in scotus_max_linear/run-13/checkpoint-375/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-13/checkpoint-375/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-13/checkpoint-375/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-13/checkpoint-500\n",
            "Configuration saved in scotus_max_linear/run-13/checkpoint-500/config.json\n",
            "Model weights saved in scotus_max_linear/run-13/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-13/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-13/checkpoint-500/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-13/checkpoint-625\n",
            "Configuration saved in scotus_max_linear/run-13/checkpoint-625/config.json\n",
            "Model weights saved in scotus_max_linear/run-13/checkpoint-625/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-13/checkpoint-625/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-13/checkpoint-625/special_tokens_map.json\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Saving model checkpoint to scotus_max_linear/run-13/checkpoint-750\n",
            "Configuration saved in scotus_max_linear/run-13/checkpoint-750/config.json\n",
            "Model weights saved in scotus_max_linear/run-13/checkpoint-750/pytorch_model.bin\n",
            "tokenizer config file saved in scotus_max_linear/run-13/checkpoint-750/tokenizer_config.json\n",
            "Special tokens file saved in scotus_max_linear/run-13/checkpoint-750/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from scotus_max_linear/run-13/checkpoint-125 (score: 0.6428571428571429).\n",
            "\u001b[32m[I 2022-12-12 10:32:39,099]\u001b[0m Trial 13 finished with value: 1.0107717636972955 and parameters: {'learning_rate': 9.141669532957721e-05, 'weight_decay': 0.01851059673861115, 'per_device_train_batch_size': 4, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 1.0617515822621284.\u001b[0m\n",
            "Trial: {'learning_rate': 1.947931609381037e-05, 'weight_decay': 0.03660325582046647, 'per_device_train_batch_size': 3, 'num_train_epochs': 10, 'lr_scheduler_type': 'constant'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 3\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1670\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='167' max='1670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 167/1670 01:21 < 12:26, 2.01 it/s, Epoch 1/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-micro</th>\n",
              "      <th>F1-macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.589788</td>\n",
              "      <td>0.542857</td>\n",
              "      <td>0.178207</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 140\n",
            "  Batch size = 6\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "\u001b[32m[I 2022-12-12 10:34:02,534]\u001b[0m Trial 14 pruned. \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPDuCwubh_XX",
        "outputId": "94f4877f-53e7-4efa-86d4-66cfd85f8e91"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BestRun(run_id='0', objective=1.0617515822621284, hyperparameters={'learning_rate': 3.6107212281458205e-05, 'weight_decay': 0.026672900647824342, 'per_device_train_batch_size': 6, 'num_train_epochs': 7, 'lr_scheduler_type': 'cosine'})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_run.importance()"
      ],
      "metadata": {
        "id": "q4_mCZhn8lPK",
        "outputId": "28889f3d-31af-41cd-a0ee-7c3b372c73d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-cf5135a084a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'BestRun' object has no attribute 'importance'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "khC7EHq-8caz"
      }
    }
  ]
}