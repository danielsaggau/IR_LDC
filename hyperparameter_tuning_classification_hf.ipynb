{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7BI1ymBBZr+xF1oAL/2aD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/hyperparameter_tuning_classification_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebooks provides some hyperparameter tuning"
      ],
      "metadata": {
        "id": "p-QOMwMKAuIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Dataset"
      ],
      "metadata": {
        "id": "ux8okenbCQ1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65-gK429AtPx",
        "outputId": "3e6cb4d4-aaba-4dbe-ca50-613faa0c2011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "#load packages\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load data \n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset=load_dataset(\"lex_glue\",\"scotus\")\n",
        "train_dataset=dataset['train']\n",
        "train_dataset = train_dataset.shard(index=1, num_shards=10)"
      ],
      "metadata": {
        "id": "skvLnAAXBXig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "padding=\"max_length\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('danielsaggau/bregman_1.5', use_fast=True)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "      return tokenizer(examples[\"text\"], truncation=True, padding=padding)\n",
        "\n",
        "tokenized_data = train_dataset.map(\n",
        "      preprocess_function,\n",
        "      batched=True,\n",
        "      desc=\"tokenizing the entire dataset\")"
      ],
      "metadata": {
        "id": "bMqVrzJ9D0qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Trainer "
      ],
      "metadata": {
        "id": "5q2aXdRJCPE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load model\n"
      ],
      "metadata": {
        "id": "5VKyunkUDJwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('danielsaggau/bregman_1.5',num_labels=14)"
      ],
      "metadata": {
        "id": "vgYzW4zADIoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdajXLktDx8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Metric Function\n"
      ],
      "metadata": {
        "id": "7njhUCt5DNbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "  metric1 = load_metric(\"f1\")\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  micro1 = metric1.compute(predictions=predictions, references=labels, average=\"micro\")[\"f1\"]\n",
        "  macro1 = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "  return { \"f1-micro\": micro1, \"f1-macro\": macro1} "
      ],
      "metadata": {
        "id": "1fTpoMJCDPiB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Arguments"
      ],
      "metadata": {
        "id": "sMqam_hLDSVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='scotus_max_linear',\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=6,\n",
        "    per_device_eval_batch_size=6,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        " #   push_to_hub=True,\n",
        "    metric_for_best_model=\"f1-micro\",\n",
        "   # fp16=True,\n",
        "#    report_to=\"wandb\",\n",
        "    greater_is_better=True,\n",
        "    lr_scheduler_type='linear',\n",
        " #   run_name=\"max\",\n",
        "    load_best_model_at_end = True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlIr2paDDVMe",
        "outputId": "4f47daf3-7d8a-4655-d419-23185a4c5792"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) # fp16"
      ],
      "metadata": {
        "id": "9LKsuWW6Lb03"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oe1b4VzEDUUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, EarlyStoppingCallback\n",
        "trainer = Trainer(\n",
        "    compute_metrics=compute_metrics,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,    \n",
        "    model_init=model_init,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "      )"
      ],
      "metadata": {
        "id": "pOSJjE6ACHW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define hyperparameter space "
      ],
      "metadata": {
        "id": "lcu7FEnwDGBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01,0.05),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2,3,4,6,8]),\n",
        "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [4,5,6,7, 10,15]),\n",
        "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", ['linear', 'cosine', 'polynomial','constant', 'constant_with_warmup'])\n",
        "    }"
      ],
      "metadata": {
        "id": "SPmxIxVtCG1N"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained('danielsaggau/bregman_1.5', num_labels=14)\n"
      ],
      "metadata": {
        "id": "aLpJjGZOOgth"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hide_output\n",
        "#!pip install optuna\n",
        "best_run = trainer.hyperparameter_search(\n",
        "    n_trials=20, direction=\"maximize\", hp_space=hp_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Av3es_N7CLWI",
        "outputId": "f3df11b3-fad7-46cf-f086-9be7965b909d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-11 22:10:54,162]\u001b[0m A new study created in memory with name: no-name-b9936bc6-457a-4fa4-ae98-d4e0d230b5e7\u001b[0m\n",
            "Trial: {'learning_rate': 4.3617949466081894e-05, 'weight_decay': 0.04718274235109328, 'per_device_train_batch_size': 4, 'num_train_epochs': 6, 'lr_scheduler_type': 'cosine'}\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/config.json\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_1.5\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_1.5/snapshots/363ae19253237bb845fc9861c93ac6033414e92d/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "All the weights of LongformerForSequenceClassification were initialized from the model checkpoint at danielsaggau/bregman_1.5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForSequenceClassification for predictions without further training.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 500\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 750\n",
            "  Number of trainable parameters = 41639438\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  3/750 01:05 < 13:40:34, 0.02 it/s, Epoch 0.02/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n"
          ]
        }
      ]
    }
  ]
}