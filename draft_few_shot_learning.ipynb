{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA9lJh1ewBJ4A5SMs11Zeg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/draft_few_shot_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import numpy\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, pipeline\n",
        "\n",
        "# test document\n",
        "text = 'Daniel uses SimCSE to train a legal-oriented Longformer model.'\n",
        "\n",
        "# Longformer as classifier\n",
        "tokenizer = AutoTokenizer.from_pretrained('danielsaggau/longformer_simcse_scotus', \n",
        "                                          use_auth_token='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',)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('danielsaggau/longformer_simcse_scotus', \n",
        "                                                           use_auth_token='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',\n",
        "                                                           num_labels=10)\n",
        "\n",
        "torch_inputs = tokenizer([text], padding='max_length', max_length=1024, truncation=True, return_tensors='pt')\n",
        "\n",
        "outputs = model(torch_inputs['input_ids'], attention_mask=torch_inputs['attention_mask'])\n",
        "\n",
        "print(outputs[0].shape)\n",
        "\n",
        "# LongformerModel as feature extractor\n",
        "model_featurizer = AutoModel.from_pretrained('danielsaggau/longformer_simcse_scotus',\n",
        "                                             use_auth_token='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')\n",
        "\n",
        "features = model_featurizer(torch_inputs['input_ids'], attention_mask=torch_inputs['attention_mask'])\n",
        "\n",
        "# cls pooling\n",
        "cls_pooled_doc_embedding = features[0][:, 0]\n",
        "print(cls_pooled_doc_embedding.shape)\n",
        "\n",
        "# mean pooling (default in SentenceTransformers)\n",
        "mean_pooled_doc_embedding = features[0].mean(dim=1)\n",
        "print(mean_pooled_doc_embedding.shape)\n",
        "\n",
        "# max pooling\n",
        "max_pooled_doc_embedding = features[0].max(dim=1)[0]\n",
        "print(max_pooled_doc_embedding.shape)\n",
        "\n",
        "# Longformer as feature extractor via pipeline\n",
        "featurizer = pipeline('feature-extraction', \n",
        "                      'danielsaggau/longformer_simcse_scotus', \n",
        "                      use_auth_token='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')\n",
        "\n",
        "features = featurizer(text)\n",
        "\n",
        "# cls pooling\n",
        "cls_pooled_doc_embedding = features[0][0]\n",
        "\n",
        "# mean pooling (default in SentenceTransformers)\n",
        "mean_pooled_doc_embedding = numpy.mean(features[0], axis=0)\n",
        "print(mean_pooled_doc_embedding.shape)\n",
        "\n",
        "# max pooling\n",
        "max_pooled_doc_embedding = numpy.max(features[0], axis=0)\n",
        "print(max_pooled_doc_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198,
          "referenced_widgets": [
            "225981f443ee493ebcdf327d7d21e008"
          ]
        },
        "id": "3sL3AGLwdyJN",
        "outputId": "5277884c-ba24-44c1-edab-8bac2cc7509f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/167M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "225981f443ee493ebcdf327d7d21e008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at danielsaggau/longformer_simcse_scotus and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n",
            "torch.Size([1, 512])\n",
            "torch.Size([1, 512])\n",
            "torch.Size([1, 512])\n",
            "(512,)\n",
            "(512,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXLOFoXae_LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"lex_glue\", \"scotus\")"
      ],
      "metadata": {
        "id": "GOlwGTQCe3Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "set_seed(123)"
      ],
      "metadata": {
        "id": "XJmSIS3me5NQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V9pH0yWdf9Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOERnwPcdnMX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# get doc features given automodel or pipelne\n",
        "test_features[k] = [F1, …, F512]\n",
        "\n",
        "label_1_repr = [F1, …, F512]  # a random training sample labeled with 1 (same for all Longformers, LegalLongformer, LegalLongformer+SimCSE, LegalLongformer+SimCSE+Bregman) need to fix the seed to do that ;) \n",
        "label_2_repr = [F1, …, F512] # a random training sample labeled with 2\n",
        "…\n",
        "label_14_repr = [F1, …, F512] # a random training sample labeled with 14\n",
        "\n",
        "\n",
        "# compute cosine similarities\n",
        "cos_sim_label_1 = cosine_sim(test_features[k], label_1_repr)\n",
        "cos_sim_label_2 = cosine_sim(test_features[k], label_1_repr)\n",
        "…\n",
        "cos_sim_label_14 = cosine_sim(test_features[k], label_1_repr)\n",
        "\n",
        "# find most likely label\n",
        "test_label = argmax(cos_sim_label_1, cos_sim_label_2, …, cos_sim_label_14)\n",
        "\n"
      ]
    }
  ]
}