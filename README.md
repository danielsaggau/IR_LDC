# thesis

# Potential Topics 

- Distilled Language Models for Legal Text 
- Multi-task Pretraining 
- 

# Literature Review 
 
AdapterFusion: Non-Destructive Task Composition for Transfer Learning
https://aclanthology.org/2021.eacl-main.39.pdf

Parameter-Efficient Transfer Learning for NLP
http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf


EXT5: TOWARDS EXTREME MULTI-TASK SCALING FOR TRANSFER LEARNING
https://arxiv.org/pdf/2111.10952.pdf

mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
https://arxiv.org/pdf/2010.11934.pdf

Swiss-Judgment-Prediction: A Multilingual Legal Judgment Prediction Benchmark
https://arxiv.org/pdf/2110.00806.pdf

Lex Rosetta: Transfer of Predictive Models Across Languages, Jurisdictions, and Legal Domains
https://dl.acm.org/doi/pdf/10.1145/3462757.3466149

Dynamic Knowledge Distillation for Pre-trained Language Models
https://aclanthology.org/2021.emnlp-main.31.pdf


Towards Zero-Shot Knowledge Distillation for Natural Language Processing
https://aclanthology.org/2021.emnlp-main.526.pdf

Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders
https://aclanthology.org/2021.emnlp-main.2.pdf

mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs
https://aclanthology.org/2021.emnlp-main.125.pdf

Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation
https://aclanthology.org/2021.emnlp-main.132.pdf

MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model
https://arxiv.org/pdf/2109.06605.pdf

Visually Grounded Reasoning across Languages and Cultures
https://arxiv.org/pdf/2109.13238.pdf
