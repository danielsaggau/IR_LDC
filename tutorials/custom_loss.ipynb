{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfuVStRvR9IR"
      },
      "source": [
        "This notebook regroups the code sample of the video below, which is a part of the [Hugging Face course](https://huggingface.co/course)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KQClyBtDR9IT",
        "outputId": "f1061b83-48b3-454f-b784-97afd6a2dc1a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Hm8_PgVTFuc?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Hm8_PgVTFuc?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs2KWxzSR9IV"
      },
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es1apEN0R9IW"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "hwilU4lh90dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOAss9zGR9IY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"huggingface-course/codeparrot-ds\")\n",
        "\n",
        "keytoken_ids = []\n",
        "for keyword in [\n",
        "    \"plt\",\n",
        "    \"pd\",\n",
        "    \"sk\",\n",
        "    \"fit\",\n",
        "    \"predict\",\n",
        "    \" plt\",\n",
        "    \" pd\",\n",
        "    \" sk\",\n",
        "    \" fit\",\n",
        "    \" predict\",\n",
        "]:\n",
        "    ids = tokenizer([keyword]).input_ids[0]\n",
        "    keytoken_ids.append(ids[0])\n",
        "\n",
        "batch = tokenizer([\"import numpy as np\"], return_tensors=\"pt\")\n",
        "model = accelerator.prepare(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "auBfS1URR9IZ"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "\n",
        "\n",
        "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
        "    # Shift so that tokens < n predict n\n",
        "    shift_labels = inputs[..., 1:].contiguous()\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    # Calculate per-token loss\n",
        "    loss_fct = CrossEntropyLoss(reduce=False)\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    # Resize and average loss per sample\n",
        "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
        "    # Calculate and scale weighting\n",
        "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
        "        axis=[0, 2]\n",
        "    )\n",
        "    weights = alpha * (1.0 + weights)\n",
        "    # Calculate weighted average\n",
        "    weighted_loss = (loss_per_sample * weights).mean()\n",
        "    return weighted_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_f6I5eO9R9Ia",
        "outputId": "0abed2e3-8979-40c7-b9ba-b6cb9e015f86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "logits = model(batch[\"input_ids\"]).logits\n",
        "loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
        "accelerator.backward(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9rBMx2OmR9Ia"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        input_ids = inputs.get(\"input_ids\")\n",
        "        outputs = model(input_ids)\n",
        "        loss = keytoken_weighted_loss(input_ids, outputs.logits, keytoken_ids)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZkzOwHNuR9Ib",
        "outputId": "2be04347-aeb1-4e3d-d5fa-46ee3b22d190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b62452b020d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_model_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`Trainer` requires either a `model` or `model_init` argument\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: `Trainer` requires either a `model` or `model_init` argument"
          ]
        }
      ],
      "source": [
        "Trainer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s1J0TmcY-KwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Using a custom loss function",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}