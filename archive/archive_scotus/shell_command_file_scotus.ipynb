{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPwrd4FW9MMiu6w9r7cx09P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/SCOTUS/Archive/shell_command_file_scotus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naqIWr56jSUf"
      },
      "outputs": [],
      "source": [
        "!git clone https://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@github.com/danielsaggau/IR_LDC.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd IR_LDC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rw6QC9ojY5t",
        "outputId": "4858b66f-578f-4373-ac1d-8d4a02932e1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IR_LDC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ipRGPbTR45u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "!wandb login fd6f7deb3126d40be9abf77ee753bf45f00e2a9a\n",
        "wandb.init(project=\"IR_LDC\")\n",
        "%env WANDB_PROJECT=IR_LDC"
      ],
      "metadata": {
        "id": "XOCuFbIKgDss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IR_LDC/model/SCOTUS/scotus_clean.py \\\n",
        "    --output_dir logs/output_1 \\\n",
        "    --model_type 'max' \\\n",
        "    --load_best_model_at_end \\\n",
        "    --overwrite_output_dir \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --learning_rate 1e-3 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --per_device_eval_batch_size 6 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --fp16 \\\n",
        "    --freezing True \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --metric_for_best_model \"f1-micro\" \\\n",
        "    --greater_is_better 1 \\\n",
        "    --report_to 'wandb'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQYL6fodppBA",
        "outputId": "9a19b8e6-add9-488b-fd02-0e257fea871d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mDie letzten 5000Â Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n",
            " 89% 7416/8340 [47:26<04:14,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:00,181 >> Initializing global attention on CLS token...\n",
            " 89% 7417/8340 [47:26<04:16,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:00,466 >> Initializing global attention on CLS token...\n",
            " 89% 7418/8340 [47:27<04:18,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:00,745 >> Initializing global attention on CLS token...\n",
            " 89% 7419/8340 [47:27<04:17,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:01,025 >> Initializing global attention on CLS token...\n",
            " 89% 7420/8340 [47:27<04:15,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:01,298 >> Initializing global attention on CLS token...\n",
            " 89% 7421/8340 [47:28<04:16,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:01,591 >> Initializing global attention on CLS token...\n",
            " 89% 7422/8340 [47:28<04:17,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:01,864 >> Initializing global attention on CLS token...\n",
            " 89% 7423/8340 [47:28<04:15,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:02,139 >> Initializing global attention on CLS token...\n",
            " 89% 7424/8340 [47:28<04:14,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:02,415 >> Initializing global attention on CLS token...\n",
            " 89% 7425/8340 [47:29<04:15,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:02,697 >> Initializing global attention on CLS token...\n",
            " 89% 7426/8340 [47:29<04:16,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:02,987 >> Initializing global attention on CLS token...\n",
            " 89% 7427/8340 [47:29<04:17,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:03,265 >> Initializing global attention on CLS token...\n",
            " 89% 7428/8340 [47:30<04:17,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:03,547 >> Initializing global attention on CLS token...\n",
            " 89% 7429/8340 [47:30<04:14,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:03,821 >> Initializing global attention on CLS token...\n",
            " 89% 7430/8340 [47:30<04:13,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:04,101 >> Initializing global attention on CLS token...\n",
            " 89% 7431/8340 [47:30<04:14,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:04,381 >> Initializing global attention on CLS token...\n",
            " 89% 7432/8340 [47:31<04:12,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:04,654 >> Initializing global attention on CLS token...\n",
            " 89% 7433/8340 [47:31<04:12,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:04,952 >> Initializing global attention on CLS token...\n",
            " 89% 7434/8340 [47:31<04:16,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:05,228 >> Initializing global attention on CLS token...\n",
            " 89% 7435/8340 [47:31<04:13,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:05,503 >> Initializing global attention on CLS token...\n",
            " 89% 7436/8340 [47:32<04:13,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:05,782 >> Initializing global attention on CLS token...\n",
            " 89% 7437/8340 [47:32<04:11,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:06,057 >> Initializing global attention on CLS token...\n",
            " 89% 7438/8340 [47:32<04:11,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:06,344 >> Initializing global attention on CLS token...\n",
            " 89% 7439/8340 [47:33<04:12,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:06,628 >> Initializing global attention on CLS token...\n",
            " 89% 7440/8340 [47:33<04:13,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:06,902 >> Initializing global attention on CLS token...\n",
            " 89% 7441/8340 [47:33<04:11,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:07,183 >> Initializing global attention on CLS token...\n",
            " 89% 7442/8340 [47:33<04:09,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:07,456 >> Initializing global attention on CLS token...\n",
            " 89% 7443/8340 [47:34<04:10,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:07,737 >> Initializing global attention on CLS token...\n",
            " 89% 7444/8340 [47:34<04:10,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:08,015 >> Initializing global attention on CLS token...\n",
            " 89% 7445/8340 [47:34<04:07,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:08,288 >> Initializing global attention on CLS token...\n",
            " 89% 7446/8340 [47:35<04:09,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:08,572 >> Initializing global attention on CLS token...\n",
            " 89% 7447/8340 [47:35<04:10,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:08,863 >> Initializing global attention on CLS token...\n",
            " 89% 7448/8340 [47:35<04:11,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:09,142 >> Initializing global attention on CLS token...\n",
            " 89% 7449/8340 [47:35<04:11,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:09,424 >> Initializing global attention on CLS token...\n",
            " 89% 7450/8340 [47:36<04:11,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:09,704 >> Initializing global attention on CLS token...\n",
            " 89% 7451/8340 [47:36<04:08,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:09,980 >> Initializing global attention on CLS token...\n",
            " 89% 7452/8340 [47:36<04:07,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:10,261 >> Initializing global attention on CLS token...\n",
            " 89% 7453/8340 [47:37<04:08,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:10,539 >> Initializing global attention on CLS token...\n",
            " 89% 7454/8340 [47:37<04:06,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:10,813 >> Initializing global attention on CLS token...\n",
            " 89% 7455/8340 [47:37<04:06,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:11,096 >> Initializing global attention on CLS token...\n",
            " 89% 7456/8340 [47:37<04:07,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:11,374 >> Initializing global attention on CLS token...\n",
            " 89% 7457/8340 [47:38<04:07,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:11,659 >> Initializing global attention on CLS token...\n",
            " 89% 7458/8340 [47:38<04:08,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:11,944 >> Initializing global attention on CLS token...\n",
            " 89% 7459/8340 [47:38<04:09,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:12,234 >> Initializing global attention on CLS token...\n",
            " 89% 7460/8340 [47:38<04:09,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:12,516 >> Initializing global attention on CLS token...\n",
            " 89% 7461/8340 [47:39<04:09,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:12,797 >> Initializing global attention on CLS token...\n",
            " 89% 7462/8340 [47:39<04:08,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:13,079 >> Initializing global attention on CLS token...\n",
            " 89% 7463/8340 [47:39<04:08,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:13,360 >> Initializing global attention on CLS token...\n",
            " 89% 7464/8340 [47:40<04:05,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:13,635 >> Initializing global attention on CLS token...\n",
            " 90% 7465/8340 [47:40<04:05,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:13,913 >> Initializing global attention on CLS token...\n",
            " 90% 7466/8340 [47:40<04:04,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:14,205 >> Initializing global attention on CLS token...\n",
            " 90% 7467/8340 [47:40<04:06,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:14,484 >> Initializing global attention on CLS token...\n",
            " 90% 7468/8340 [47:41<04:06,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:14,762 >> Initializing global attention on CLS token...\n",
            " 90% 7469/8340 [47:41<04:02,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:15,034 >> Initializing global attention on CLS token...\n",
            " 90% 7470/8340 [47:41<04:02,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:15,315 >> Initializing global attention on CLS token...\n",
            " 90% 7471/8340 [47:42<04:03,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:15,597 >> Initializing global attention on CLS token...\n",
            " 90% 7472/8340 [47:42<04:02,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:15,872 >> Initializing global attention on CLS token...\n",
            " 90% 7473/8340 [47:42<04:00,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:16,148 >> Initializing global attention on CLS token...\n",
            " 90% 7474/8340 [47:42<04:01,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:16,429 >> Initializing global attention on CLS token...\n",
            " 90% 7475/8340 [47:43<04:00,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:16,706 >> Initializing global attention on CLS token...\n",
            " 90% 7476/8340 [47:43<03:59,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:16,980 >> Initializing global attention on CLS token...\n",
            " 90% 7477/8340 [47:43<04:00,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:17,264 >> Initializing global attention on CLS token...\n",
            " 90% 7478/8340 [47:44<04:00,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:17,540 >> Initializing global attention on CLS token...\n",
            " 90% 7479/8340 [47:44<03:58,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:17,816 >> Initializing global attention on CLS token...\n",
            " 90% 7480/8340 [47:44<03:59,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:18,097 >> Initializing global attention on CLS token...\n",
            " 90% 7481/8340 [47:44<03:59,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:18,392 >> Initializing global attention on CLS token...\n",
            " 90% 7482/8340 [47:45<04:02,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:18,669 >> Initializing global attention on CLS token...\n",
            " 90% 7483/8340 [47:45<04:02,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:18,951 >> Initializing global attention on CLS token...\n",
            " 90% 7484/8340 [47:45<04:01,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:19,232 >> Initializing global attention on CLS token...\n",
            " 90% 7485/8340 [47:45<03:59,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:19,507 >> Initializing global attention on CLS token...\n",
            " 90% 7486/8340 [47:46<03:59,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:19,788 >> Initializing global attention on CLS token...\n",
            " 90% 7487/8340 [47:46<03:59,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:20,073 >> Initializing global attention on CLS token...\n",
            " 90% 7488/8340 [47:46<04:00,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:20,355 >> Initializing global attention on CLS token...\n",
            " 90% 7489/8340 [47:47<04:00,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:20,636 >> Initializing global attention on CLS token...\n",
            " 90% 7490/8340 [47:47<03:59,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:20,920 >> Initializing global attention on CLS token...\n",
            " 90% 7491/8340 [47:47<03:58,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:21,198 >> Initializing global attention on CLS token...\n",
            " 90% 7492/8340 [47:47<03:56,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:21,488 >> Initializing global attention on CLS token...\n",
            " 90% 7493/8340 [47:48<03:59,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:21,761 >> Initializing global attention on CLS token...\n",
            " 90% 7494/8340 [47:48<03:57,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:22,039 >> Initializing global attention on CLS token...\n",
            " 90% 7495/8340 [47:48<03:57,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:22,323 >> Initializing global attention on CLS token...\n",
            " 90% 7496/8340 [47:49<03:58,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:22,605 >> Initializing global attention on CLS token...\n",
            " 90% 7497/8340 [47:49<03:55,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:22,884 >> Initializing global attention on CLS token...\n",
            " 90% 7498/8340 [47:49<03:55,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:23,160 >> Initializing global attention on CLS token...\n",
            " 90% 7499/8340 [47:49<03:56,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:23,448 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.6595, 'learning_rate': 0.00010107913669064749, 'epoch': 8.99}\n",
            " 90% 7500/8340 [47:50<03:57,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:23,737 >> Initializing global attention on CLS token...\n",
            " 90% 7501/8340 [47:50<03:56,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:24,026 >> Initializing global attention on CLS token...\n",
            " 90% 7502/8340 [47:50<03:58,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:24,303 >> Initializing global attention on CLS token...\n",
            " 90% 7503/8340 [47:51<03:56,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:24,583 >> Initializing global attention on CLS token...\n",
            " 90% 7504/8340 [47:51<03:56,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:24,864 >> Initializing global attention on CLS token...\n",
            " 90% 7505/8340 [47:51<03:56,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:56:25,129 >> Initializing global attention on CLS token...\n",
            " 90% 7506/8340 [47:51<03:10,  4.38it/s][INFO|trainer.py:726] 2022-11-24 17:56:25,218 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2022-11-24 17:56:25,220 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-11-24 17:56:25,221 >>   Num examples = 1400\n",
            "[INFO|trainer.py:2912] 2022-11-24 17:56:25,221 >>   Batch size = 6\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:25,252 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 0/234 [00:00<?, ?it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:25,516 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 2/234 [00:00<00:30,  7.52it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:25,782 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 3/234 [00:00<00:43,  5.28it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:26,053 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 4/234 [00:00<00:50,  4.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:26,327 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 5/234 [00:01<00:54,  4.21it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:26,591 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 6/234 [00:01<00:56,  4.07it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:26,871 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 7/234 [00:01<00:58,  3.89it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:27,142 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 8/234 [00:01<00:59,  3.83it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:27,407 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 9/234 [00:02<00:59,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:27,676 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 10/234 [00:02<00:59,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:27,957 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 11/234 [00:02<01:00,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:28,237 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 12/234 [00:02<01:00,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:28,501 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 13/234 [00:03<00:59,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:28,768 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 14/234 [00:03<00:58,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:29,033 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 15/234 [00:03<00:58,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:29,296 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 16/234 [00:04<00:57,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:29,557 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 17/234 [00:04<00:57,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:29,820 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 18/234 [00:04<00:57,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:30,086 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 19/234 [00:04<00:57,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:30,355 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 20/234 [00:05<00:56,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:30,620 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 21/234 [00:05<00:56,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:30,888 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 22/234 [00:05<00:56,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:31,171 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 23/234 [00:05<00:57,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:31,440 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 24/234 [00:06<00:56,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:31,714 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 25/234 [00:06<00:56,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:31,987 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 26/234 [00:06<00:56,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:32,259 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 27/234 [00:07<00:56,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:32,526 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 28/234 [00:07<00:55,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:32,792 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 29/234 [00:07<00:55,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:33,061 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 30/234 [00:07<00:55,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:33,334 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 31/234 [00:08<00:54,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:33,602 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 32/234 [00:08<00:54,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:33,864 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 33/234 [00:08<00:53,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:34,131 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 34/234 [00:08<00:53,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:34,404 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 35/234 [00:09<00:53,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:34,673 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 36/234 [00:09<00:53,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:34,938 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 37/234 [00:09<00:52,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:35,209 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 38/234 [00:09<00:52,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:35,474 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 39/234 [00:10<00:52,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:35,739 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 40/234 [00:10<00:51,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:36,000 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 41/234 [00:10<00:51,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:36,263 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 42/234 [00:11<00:50,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:36,545 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 43/234 [00:11<00:51,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:36,812 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 44/234 [00:11<00:50,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:37,077 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 45/234 [00:11<00:50,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:37,343 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 46/234 [00:12<00:50,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:37,612 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 47/234 [00:12<00:50,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:37,884 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 48/234 [00:12<00:50,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:38,169 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 49/234 [00:12<00:50,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:38,437 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 50/234 [00:13<00:49,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:38,701 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 51/234 [00:13<00:49,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:38,968 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 52/234 [00:13<00:49,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:39,235 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 53/234 [00:13<00:48,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:39,501 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 54/234 [00:14<00:48,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:39,781 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 55/234 [00:14<00:48,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:40,049 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 56/234 [00:14<00:48,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:40,314 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 57/234 [00:15<00:47,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:40,583 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 58/234 [00:15<00:47,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:40,854 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 59/234 [00:15<00:47,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:41,124 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 60/234 [00:15<00:46,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:41,388 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 61/234 [00:16<00:46,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:41,656 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 62/234 [00:16<00:45,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:41,917 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 63/234 [00:16<00:45,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:42,189 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 64/234 [00:16<00:45,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:42,458 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 65/234 [00:17<00:45,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:42,739 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 66/234 [00:17<00:45,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:43,009 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 67/234 [00:17<00:45,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:43,274 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 68/234 [00:18<00:44,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:43,539 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 69/234 [00:18<00:44,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:43,807 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 70/234 [00:18<00:43,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:44,072 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 71/234 [00:18<00:43,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:44,337 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 72/234 [00:19<00:43,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:44,604 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 73/234 [00:19<00:43,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:44,875 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 74/234 [00:19<00:42,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:45,150 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 75/234 [00:19<00:42,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:45,425 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 76/234 [00:20<00:42,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:45,693 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 77/234 [00:20<00:42,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:45,961 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 78/234 [00:20<00:41,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:46,225 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 79/234 [00:20<00:41,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:46,489 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 80/234 [00:21<00:40,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:46,752 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 81/234 [00:21<00:40,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:47,015 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 82/234 [00:21<00:40,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:47,280 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 83/234 [00:22<00:40,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:47,548 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 84/234 [00:22<00:39,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:47,810 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 85/234 [00:22<00:39,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:48,075 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 86/234 [00:22<00:39,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:48,339 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 87/234 [00:23<00:38,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:48,605 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 88/234 [00:23<00:38,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:48,887 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 89/234 [00:23<00:39,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:49,160 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 90/234 [00:23<00:39,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:49,430 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 91/234 [00:24<00:38,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:49,702 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 92/234 [00:24<00:38,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:49,968 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 93/234 [00:24<00:37,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:50,232 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 94/234 [00:24<00:37,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:50,504 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 95/234 [00:25<00:37,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:50,776 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 96/234 [00:25<00:37,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:51,042 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 97/234 [00:25<00:36,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:51,306 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 98/234 [00:26<00:36,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:51,575 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 99/234 [00:26<00:36,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:51,843 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 100/234 [00:26<00:35,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:52,108 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 101/234 [00:26<00:35,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:52,372 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 102/234 [00:27<00:35,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:52,638 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 103/234 [00:27<00:34,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:52,903 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 104/234 [00:27<00:34,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:53,168 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 105/234 [00:27<00:34,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:53,437 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 106/234 [00:28<00:34,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:53,716 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 107/234 [00:28<00:34,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:53,985 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 108/234 [00:28<00:34,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:54,262 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 109/234 [00:29<00:33,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:54,538 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 110/234 [00:29<00:34,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:54,813 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 111/234 [00:29<00:33,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:55,084 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 112/234 [00:29<00:33,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:55,358 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 113/234 [00:30<00:32,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:55,634 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 114/234 [00:30<00:33,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:55,904 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 115/234 [00:30<00:32,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:56,173 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 116/234 [00:30<00:31,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:56,438 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 117/234 [00:31<00:31,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:56,703 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 118/234 [00:31<00:31,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:56,972 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 119/234 [00:31<00:30,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:57,237 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 120/234 [00:31<00:30,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:57,502 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 121/234 [00:32<00:30,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:57,776 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 122/234 [00:32<00:30,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:58,048 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 123/234 [00:32<00:29,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:58,310 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 124/234 [00:33<00:29,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:58,585 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 125/234 [00:33<00:29,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:58,861 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 126/234 [00:33<00:29,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:59,124 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 127/234 [00:33<00:28,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:59,393 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 128/234 [00:34<00:28,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:59,669 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 129/234 [00:34<00:28,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:56:59,935 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 130/234 [00:34<00:28,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:00,201 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 131/234 [00:34<00:27,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:00,468 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 132/234 [00:35<00:27,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:00,735 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 133/234 [00:35<00:26,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:01,005 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 134/234 [00:35<00:26,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:01,288 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 135/234 [00:36<00:27,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:01,567 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 136/234 [00:36<00:26,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:01,831 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 137/234 [00:36<00:26,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:02,104 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 138/234 [00:36<00:26,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:02,377 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 139/234 [00:37<00:25,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:02,644 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 140/234 [00:37<00:25,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:02,914 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 141/234 [00:37<00:25,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:03,189 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 142/234 [00:37<00:24,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:03,454 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 143/234 [00:38<00:24,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:03,719 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 144/234 [00:38<00:24,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:03,985 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 145/234 [00:38<00:23,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:04,250 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 146/234 [00:38<00:23,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:04,516 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 147/234 [00:39<00:23,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:04,780 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 148/234 [00:39<00:22,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:05,047 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 149/234 [00:39<00:22,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:05,327 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 150/234 [00:40<00:22,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:05,590 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 151/234 [00:40<00:22,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:05,864 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 152/234 [00:40<00:22,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:06,136 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 153/234 [00:40<00:21,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:06,401 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 154/234 [00:41<00:21,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:06,665 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 155/234 [00:41<00:21,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:06,931 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 156/234 [00:41<00:20,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:07,200 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 157/234 [00:41<00:20,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:07,474 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 158/234 [00:42<00:20,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:07,744 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 159/234 [00:42<00:20,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:08,007 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 160/234 [00:42<00:19,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:08,275 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 161/234 [00:43<00:19,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:08,545 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 162/234 [00:43<00:19,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:08,814 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 163/234 [00:43<00:19,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:09,083 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 164/234 [00:43<00:18,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:09,347 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 165/234 [00:44<00:18,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:09,612 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 166/234 [00:44<00:18,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:09,884 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 167/234 [00:44<00:18,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:10,158 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 168/234 [00:44<00:17,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:10,424 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 169/234 [00:45<00:17,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:10,692 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 170/234 [00:45<00:17,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:10,959 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 171/234 [00:45<00:17,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:11,233 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 172/234 [00:45<00:16,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:11,501 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 173/234 [00:46<00:16,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:11,768 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 174/234 [00:46<00:16,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:12,037 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 175/234 [00:46<00:15,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:12,308 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 176/234 [00:47<00:15,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:12,576 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 177/234 [00:47<00:15,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:12,843 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 178/234 [00:47<00:15,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:13,114 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 179/234 [00:47<00:14,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:13,387 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 180/234 [00:48<00:14,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:13,652 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 181/234 [00:48<00:14,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:13,919 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 182/234 [00:48<00:14,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:14,190 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 183/234 [00:48<00:13,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:14,462 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 184/234 [00:49<00:13,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:14,737 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 185/234 [00:49<00:13,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:15,011 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 186/234 [00:49<00:13,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:15,278 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 187/234 [00:50<00:12,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:15,544 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 188/234 [00:50<00:12,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:15,807 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 189/234 [00:50<00:11,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:16,071 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 190/234 [00:50<00:11,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:16,341 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 191/234 [00:51<00:11,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:16,613 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 192/234 [00:51<00:11,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:16,881 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 193/234 [00:51<00:10,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:17,157 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 194/234 [00:51<00:10,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:17,431 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 195/234 [00:52<00:10,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:17,694 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 196/234 [00:52<00:10,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:17,961 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 197/234 [00:52<00:09,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:18,226 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 198/234 [00:52<00:09,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:18,488 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 199/234 [00:53<00:09,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:18,753 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 200/234 [00:53<00:09,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:19,019 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 201/234 [00:53<00:08,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:19,291 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 202/234 [00:54<00:08,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:19,564 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 203/234 [00:54<00:08,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:19,828 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 204/234 [00:54<00:08,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:20,100 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 205/234 [00:54<00:07,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:20,381 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 206/234 [00:55<00:07,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:20,644 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 207/234 [00:55<00:07,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:20,909 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 208/234 [00:55<00:06,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:21,175 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 209/234 [00:55<00:06,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:21,442 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 210/234 [00:56<00:06,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:21,709 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 211/234 [00:56<00:06,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:21,981 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 212/234 [00:56<00:05,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:22,248 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 213/234 [00:56<00:05,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:22,527 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 214/234 [00:57<00:05,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:22,797 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 215/234 [00:57<00:05,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:23,062 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 216/234 [00:57<00:04,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:23,346 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 217/234 [00:58<00:04,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:23,613 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 218/234 [00:58<00:04,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:23,876 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 219/234 [00:58<00:04,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:24,144 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 220/234 [00:58<00:03,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:24,422 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 221/234 [00:59<00:03,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:24,690 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 222/234 [00:59<00:03,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:24,962 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 223/234 [00:59<00:02,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:25,237 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 224/234 [00:59<00:02,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:25,509 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 225/234 [01:00<00:02,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:25,799 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 226/234 [01:00<00:02,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:26,083 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 227/234 [01:00<00:01,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:26,364 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 228/234 [01:01<00:01,  3.57it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:26,640 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 229/234 [01:01<00:01,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:26,913 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 230/234 [01:01<00:01,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:27,204 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 231/234 [01:01<00:00,  3.57it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:27,488 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 232/234 [01:02<00:00,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:27,760 >> Initializing global attention on CLS token...\n",
            "\n",
            "100% 233/234 [01:02<00:00,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 17:57:28,011 >> Initializing global attention on CLS token...\n",
            "\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.2387529611587524, 'eval_f1-micro': 0.6592857142857143, 'eval_f1-macro': 0.5136279671332988, 'eval_runtime': 64.0822, 'eval_samples_per_second': 21.847, 'eval_steps_per_second': 3.652, 'epoch': 9.0}\n",
            " 90% 7506/8340 [48:55<03:10,  4.38it/s]\n",
            "100% 234/234 [01:03<00:00,  4.43it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-11-24 17:57:29,305 >> Saving model checkpoint to logs/output_1/checkpoint-7506\n",
            "[INFO|configuration_utils.py:447] 2022-11-24 17:57:29,307 >> Configuration saved in logs/output_1/checkpoint-7506/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-24 17:57:29,704 >> Model weights saved in logs/output_1/checkpoint-7506/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-24 17:57:29,705 >> tokenizer config file saved in logs/output_1/checkpoint-7506/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-24 17:57:29,705 >> Special tokens file saved in logs/output_1/checkpoint-7506/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-24 17:57:31,960 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-24 17:57:31,961 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-24 17:58:01,355 >> Initializing global attention on CLS token...\n",
            " 90% 7507/8340 [49:28<6:43:51, 29.09s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:01,682 >> Initializing global attention on CLS token...\n",
            " 90% 7508/8340 [49:28<4:43:30, 20.45s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:01,954 >> Initializing global attention on CLS token...\n",
            " 90% 7509/8340 [49:28<3:19:19, 14.39s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:02,221 >> Initializing global attention on CLS token...\n",
            " 90% 7510/8340 [49:28<2:20:28, 10.15s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:02,502 >> Initializing global attention on CLS token...\n",
            " 90% 7511/8340 [49:29<1:39:22,  7.19s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:02,774 >> Initializing global attention on CLS token...\n",
            " 90% 7512/8340 [49:29<1:10:37,  5.12s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:03,047 >> Initializing global attention on CLS token...\n",
            " 90% 7513/8340 [49:29<50:30,  3.66s/it]  [INFO|modeling_longformer.py:1932] 2022-11-24 17:58:03,332 >> Initializing global attention on CLS token...\n",
            " 90% 7514/8340 [49:30<36:28,  2.65s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:03,604 >> Initializing global attention on CLS token...\n",
            " 90% 7515/8340 [49:30<26:36,  1.94s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:03,871 >> Initializing global attention on CLS token...\n",
            " 90% 7516/8340 [49:30<19:43,  1.44s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:04,144 >> Initializing global attention on CLS token...\n",
            " 90% 7517/8340 [49:30<14:55,  1.09s/it][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:04,426 >> Initializing global attention on CLS token...\n",
            " 90% 7518/8340 [49:31<11:35,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:04,705 >> Initializing global attention on CLS token...\n",
            " 90% 7519/8340 [49:31<09:14,  1.48it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:04,978 >> Initializing global attention on CLS token...\n",
            " 90% 7520/8340 [49:31<07:35,  1.80it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:05,257 >> Initializing global attention on CLS token...\n",
            " 90% 7521/8340 [49:32<06:26,  2.12it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:05,547 >> Initializing global attention on CLS token...\n",
            " 90% 7522/8340 [49:32<05:42,  2.39it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:05,838 >> Initializing global attention on CLS token...\n",
            " 90% 7523/8340 [49:32<05:09,  2.64it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:06,112 >> Initializing global attention on CLS token...\n",
            " 90% 7524/8340 [49:32<04:44,  2.87it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:06,391 >> Initializing global attention on CLS token...\n",
            " 90% 7525/8340 [49:33<04:26,  3.06it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:06,668 >> Initializing global attention on CLS token...\n",
            " 90% 7526/8340 [49:33<04:13,  3.21it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:06,944 >> Initializing global attention on CLS token...\n",
            " 90% 7527/8340 [49:33<04:04,  3.32it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:07,220 >> Initializing global attention on CLS token...\n",
            " 90% 7528/8340 [49:33<03:58,  3.41it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:07,494 >> Initializing global attention on CLS token...\n",
            " 90% 7529/8340 [49:34<03:53,  3.47it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:07,774 >> Initializing global attention on CLS token...\n",
            " 90% 7530/8340 [49:34<03:50,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:08,049 >> Initializing global attention on CLS token...\n",
            " 90% 7531/8340 [49:34<03:49,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:08,326 >> Initializing global attention on CLS token...\n",
            " 90% 7532/8340 [49:35<03:46,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:08,601 >> Initializing global attention on CLS token...\n",
            " 90% 7533/8340 [49:35<03:45,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:08,893 >> Initializing global attention on CLS token...\n",
            " 90% 7534/8340 [49:35<03:47,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:09,168 >> Initializing global attention on CLS token...\n",
            " 90% 7535/8340 [49:35<03:46,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:09,447 >> Initializing global attention on CLS token...\n",
            " 90% 7536/8340 [49:36<03:44,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:09,723 >> Initializing global attention on CLS token...\n",
            " 90% 7537/8340 [49:36<03:43,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:09,999 >> Initializing global attention on CLS token...\n",
            " 90% 7538/8340 [49:36<03:43,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:10,279 >> Initializing global attention on CLS token...\n",
            " 90% 7539/8340 [49:37<03:42,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:10,551 >> Initializing global attention on CLS token...\n",
            " 90% 7540/8340 [49:37<03:42,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:10,829 >> Initializing global attention on CLS token...\n",
            " 90% 7541/8340 [49:37<03:41,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:11,111 >> Initializing global attention on CLS token...\n",
            " 90% 7542/8340 [49:37<03:41,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:11,387 >> Initializing global attention on CLS token...\n",
            " 90% 7543/8340 [49:38<03:42,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:11,663 >> Initializing global attention on CLS token...\n",
            " 90% 7544/8340 [49:38<03:40,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:11,936 >> Initializing global attention on CLS token...\n",
            " 90% 7545/8340 [49:38<03:39,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:12,213 >> Initializing global attention on CLS token...\n",
            " 90% 7546/8340 [49:38<03:38,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:12,494 >> Initializing global attention on CLS token...\n",
            " 90% 7547/8340 [49:39<03:39,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:12,767 >> Initializing global attention on CLS token...\n",
            " 91% 7548/8340 [49:39<03:40,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:13,047 >> Initializing global attention on CLS token...\n",
            " 91% 7549/8340 [49:39<03:39,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:13,325 >> Initializing global attention on CLS token...\n",
            " 91% 7550/8340 [49:40<03:38,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:13,601 >> Initializing global attention on CLS token...\n",
            " 91% 7551/8340 [49:40<03:39,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:13,879 >> Initializing global attention on CLS token...\n",
            " 91% 7552/8340 [49:40<03:38,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:14,152 >> Initializing global attention on CLS token...\n",
            " 91% 7553/8340 [49:40<03:37,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:14,436 >> Initializing global attention on CLS token...\n",
            " 91% 7554/8340 [49:41<03:38,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:14,716 >> Initializing global attention on CLS token...\n",
            " 91% 7555/8340 [49:41<03:39,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:14,993 >> Initializing global attention on CLS token...\n",
            " 91% 7556/8340 [49:41<03:37,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:15,276 >> Initializing global attention on CLS token...\n",
            " 91% 7557/8340 [49:42<03:38,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:15,553 >> Initializing global attention on CLS token...\n",
            " 91% 7558/8340 [49:42<03:38,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:15,832 >> Initializing global attention on CLS token...\n",
            " 91% 7559/8340 [49:42<03:38,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:16,109 >> Initializing global attention on CLS token...\n",
            " 91% 7560/8340 [49:42<03:35,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:16,379 >> Initializing global attention on CLS token...\n",
            " 91% 7561/8340 [49:43<03:35,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:16,655 >> Initializing global attention on CLS token...\n",
            " 91% 7562/8340 [49:43<03:33,  3.64it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:16,934 >> Initializing global attention on CLS token...\n",
            " 91% 7563/8340 [49:43<03:35,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:17,209 >> Initializing global attention on CLS token...\n",
            " 91% 7564/8340 [49:43<03:33,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:17,481 >> Initializing global attention on CLS token...\n",
            " 91% 7565/8340 [49:44<03:33,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:17,759 >> Initializing global attention on CLS token...\n",
            " 91% 7566/8340 [49:44<03:34,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:18,036 >> Initializing global attention on CLS token...\n",
            " 91% 7567/8340 [49:44<03:32,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:18,307 >> Initializing global attention on CLS token...\n",
            " 91% 7568/8340 [49:45<03:32,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:18,585 >> Initializing global attention on CLS token...\n",
            " 91% 7569/8340 [49:45<03:31,  3.64it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:18,864 >> Initializing global attention on CLS token...\n",
            " 91% 7570/8340 [49:45<03:33,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:19,142 >> Initializing global attention on CLS token...\n",
            " 91% 7571/8340 [49:45<03:34,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:19,429 >> Initializing global attention on CLS token...\n",
            " 91% 7572/8340 [49:46<03:36,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:19,713 >> Initializing global attention on CLS token...\n",
            " 91% 7573/8340 [49:46<03:35,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:19,993 >> Initializing global attention on CLS token...\n",
            " 91% 7574/8340 [49:46<03:35,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:20,273 >> Initializing global attention on CLS token...\n",
            " 91% 7575/8340 [49:47<03:33,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:20,545 >> Initializing global attention on CLS token...\n",
            " 91% 7576/8340 [49:47<03:32,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:20,827 >> Initializing global attention on CLS token...\n",
            " 91% 7577/8340 [49:47<03:33,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:21,116 >> Initializing global attention on CLS token...\n",
            " 91% 7578/8340 [49:47<03:35,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:21,400 >> Initializing global attention on CLS token...\n",
            " 91% 7579/8340 [49:48<03:35,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:21,681 >> Initializing global attention on CLS token...\n",
            " 91% 7580/8340 [49:48<03:35,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:21,962 >> Initializing global attention on CLS token...\n",
            " 91% 7581/8340 [49:48<03:33,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:22,241 >> Initializing global attention on CLS token...\n",
            " 91% 7582/8340 [49:48<03:32,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:22,529 >> Initializing global attention on CLS token...\n",
            " 91% 7583/8340 [49:49<03:33,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:22,813 >> Initializing global attention on CLS token...\n",
            " 91% 7584/8340 [49:49<03:34,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:23,100 >> Initializing global attention on CLS token...\n",
            " 91% 7585/8340 [49:49<03:34,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:23,375 >> Initializing global attention on CLS token...\n",
            " 91% 7586/8340 [49:50<03:31,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:23,651 >> Initializing global attention on CLS token...\n",
            " 91% 7587/8340 [49:50<03:31,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:23,932 >> Initializing global attention on CLS token...\n",
            " 91% 7588/8340 [49:50<03:30,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:24,211 >> Initializing global attention on CLS token...\n",
            " 91% 7589/8340 [49:50<03:29,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:24,483 >> Initializing global attention on CLS token...\n",
            " 91% 7590/8340 [49:51<03:29,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:24,764 >> Initializing global attention on CLS token...\n",
            " 91% 7591/8340 [49:51<03:28,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:25,052 >> Initializing global attention on CLS token...\n",
            " 91% 7592/8340 [49:51<03:30,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:25,343 >> Initializing global attention on CLS token...\n",
            " 91% 7593/8340 [49:52<03:32,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:25,627 >> Initializing global attention on CLS token...\n",
            " 91% 7594/8340 [49:52<03:32,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:25,906 >> Initializing global attention on CLS token...\n",
            " 91% 7595/8340 [49:52<03:31,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:26,185 >> Initializing global attention on CLS token...\n",
            " 91% 7596/8340 [49:52<03:28,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:26,463 >> Initializing global attention on CLS token...\n",
            " 91% 7597/8340 [49:53<03:27,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:26,747 >> Initializing global attention on CLS token...\n",
            " 91% 7598/8340 [49:53<03:28,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:27,018 >> Initializing global attention on CLS token...\n",
            " 91% 7599/8340 [49:53<03:26,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:27,294 >> Initializing global attention on CLS token...\n",
            " 91% 7600/8340 [49:54<03:25,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:27,579 >> Initializing global attention on CLS token...\n",
            " 91% 7601/8340 [49:54<03:26,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:27,852 >> Initializing global attention on CLS token...\n",
            " 91% 7602/8340 [49:54<03:25,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:28,127 >> Initializing global attention on CLS token...\n",
            " 91% 7603/8340 [49:54<03:23,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:28,400 >> Initializing global attention on CLS token...\n",
            " 91% 7604/8340 [49:55<03:23,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:28,675 >> Initializing global attention on CLS token...\n",
            " 91% 7605/8340 [49:55<03:22,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:28,955 >> Initializing global attention on CLS token...\n",
            " 91% 7606/8340 [49:55<03:23,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:29,232 >> Initializing global attention on CLS token...\n",
            " 91% 7607/8340 [49:55<03:23,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:29,513 >> Initializing global attention on CLS token...\n",
            " 91% 7608/8340 [49:56<03:23,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:29,789 >> Initializing global attention on CLS token...\n",
            " 91% 7609/8340 [49:56<03:22,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:30,068 >> Initializing global attention on CLS token...\n",
            " 91% 7610/8340 [49:56<03:24,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:30,350 >> Initializing global attention on CLS token...\n",
            " 91% 7611/8340 [49:57<03:23,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:30,631 >> Initializing global attention on CLS token...\n",
            " 91% 7612/8340 [49:57<03:23,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:30,908 >> Initializing global attention on CLS token...\n",
            " 91% 7613/8340 [49:57<03:22,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:31,185 >> Initializing global attention on CLS token...\n",
            " 91% 7614/8340 [49:57<03:22,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:31,466 >> Initializing global attention on CLS token...\n",
            " 91% 7615/8340 [49:58<03:23,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:31,747 >> Initializing global attention on CLS token...\n",
            " 91% 7616/8340 [49:58<03:21,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:32,026 >> Initializing global attention on CLS token...\n",
            " 91% 7617/8340 [49:58<03:21,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:32,302 >> Initializing global attention on CLS token...\n",
            " 91% 7618/8340 [49:59<03:21,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:32,582 >> Initializing global attention on CLS token...\n",
            " 91% 7619/8340 [49:59<03:21,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:32,864 >> Initializing global attention on CLS token...\n",
            " 91% 7620/8340 [49:59<03:20,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:33,142 >> Initializing global attention on CLS token...\n",
            " 91% 7621/8340 [49:59<03:20,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:33,435 >> Initializing global attention on CLS token...\n",
            " 91% 7622/8340 [50:00<03:23,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:33,710 >> Initializing global attention on CLS token...\n",
            " 91% 7623/8340 [50:00<03:21,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:33,986 >> Initializing global attention on CLS token...\n",
            " 91% 7624/8340 [50:00<03:20,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:34,264 >> Initializing global attention on CLS token...\n",
            " 91% 7625/8340 [50:01<03:18,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:34,539 >> Initializing global attention on CLS token...\n",
            " 91% 7626/8340 [50:01<03:18,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:34,817 >> Initializing global attention on CLS token...\n",
            " 91% 7627/8340 [50:01<03:17,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:35,090 >> Initializing global attention on CLS token...\n",
            " 91% 7628/8340 [50:01<03:16,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:35,366 >> Initializing global attention on CLS token...\n",
            " 91% 7629/8340 [50:02<03:17,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:35,647 >> Initializing global attention on CLS token...\n",
            " 91% 7630/8340 [50:02<03:16,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:35,926 >> Initializing global attention on CLS token...\n",
            " 91% 7631/8340 [50:02<03:16,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:36,198 >> Initializing global attention on CLS token...\n",
            " 92% 7632/8340 [50:02<03:15,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:36,474 >> Initializing global attention on CLS token...\n",
            " 92% 7633/8340 [50:03<03:15,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:36,750 >> Initializing global attention on CLS token...\n",
            " 92% 7634/8340 [50:03<03:16,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:37,032 >> Initializing global attention on CLS token...\n",
            " 92% 7635/8340 [50:03<03:16,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:37,312 >> Initializing global attention on CLS token...\n",
            " 92% 7636/8340 [50:04<03:15,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:37,586 >> Initializing global attention on CLS token...\n",
            " 92% 7637/8340 [50:04<03:15,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:37,865 >> Initializing global attention on CLS token...\n",
            " 92% 7638/8340 [50:04<03:15,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:38,144 >> Initializing global attention on CLS token...\n",
            " 92% 7639/8340 [50:04<03:14,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:38,415 >> Initializing global attention on CLS token...\n",
            " 92% 7640/8340 [50:05<03:14,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:38,698 >> Initializing global attention on CLS token...\n",
            " 92% 7641/8340 [50:05<03:14,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:38,982 >> Initializing global attention on CLS token...\n",
            " 92% 7642/8340 [50:05<03:15,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:39,264 >> Initializing global attention on CLS token...\n",
            " 92% 7643/8340 [50:06<03:14,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:39,536 >> Initializing global attention on CLS token...\n",
            " 92% 7644/8340 [50:06<03:14,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:39,815 >> Initializing global attention on CLS token...\n",
            " 92% 7645/8340 [50:06<03:12,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:40,089 >> Initializing global attention on CLS token...\n",
            " 92% 7646/8340 [50:06<03:11,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:40,364 >> Initializing global attention on CLS token...\n",
            " 92% 7647/8340 [50:07<03:11,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:40,641 >> Initializing global attention on CLS token...\n",
            " 92% 7648/8340 [50:07<03:10,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:40,916 >> Initializing global attention on CLS token...\n",
            " 92% 7649/8340 [50:07<03:12,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:41,204 >> Initializing global attention on CLS token...\n",
            " 92% 7650/8340 [50:07<03:12,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:41,487 >> Initializing global attention on CLS token...\n",
            " 92% 7651/8340 [50:08<03:13,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:41,768 >> Initializing global attention on CLS token...\n",
            " 92% 7652/8340 [50:08<03:12,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:42,045 >> Initializing global attention on CLS token...\n",
            " 92% 7653/8340 [50:08<03:11,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:42,322 >> Initializing global attention on CLS token...\n",
            " 92% 7654/8340 [50:09<03:11,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:42,620 >> Initializing global attention on CLS token...\n",
            " 92% 7655/8340 [50:09<03:14,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:42,896 >> Initializing global attention on CLS token...\n",
            " 92% 7656/8340 [50:09<03:12,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:43,166 >> Initializing global attention on CLS token...\n",
            " 92% 7657/8340 [50:09<03:09,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:43,449 >> Initializing global attention on CLS token...\n",
            " 92% 7658/8340 [50:10<03:10,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:43,731 >> Initializing global attention on CLS token...\n",
            " 92% 7659/8340 [50:10<03:10,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:44,006 >> Initializing global attention on CLS token...\n",
            " 92% 7660/8340 [50:10<03:10,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:44,286 >> Initializing global attention on CLS token...\n",
            " 92% 7661/8340 [50:11<03:09,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:44,563 >> Initializing global attention on CLS token...\n",
            " 92% 7662/8340 [50:11<03:08,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:44,843 >> Initializing global attention on CLS token...\n",
            " 92% 7663/8340 [50:11<03:09,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:45,133 >> Initializing global attention on CLS token...\n",
            " 92% 7664/8340 [50:11<03:10,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:45,405 >> Initializing global attention on CLS token...\n",
            " 92% 7665/8340 [50:12<03:09,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:45,683 >> Initializing global attention on CLS token...\n",
            " 92% 7666/8340 [50:12<03:07,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:45,957 >> Initializing global attention on CLS token...\n",
            " 92% 7667/8340 [50:12<03:07,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:46,246 >> Initializing global attention on CLS token...\n",
            " 92% 7668/8340 [50:12<03:08,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:46,518 >> Initializing global attention on CLS token...\n",
            " 92% 7669/8340 [50:13<03:06,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:46,792 >> Initializing global attention on CLS token...\n",
            " 92% 7670/8340 [50:13<03:06,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:47,085 >> Initializing global attention on CLS token...\n",
            " 92% 7671/8340 [50:13<03:07,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:47,360 >> Initializing global attention on CLS token...\n",
            " 92% 7672/8340 [50:14<03:07,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:47,644 >> Initializing global attention on CLS token...\n",
            " 92% 7673/8340 [50:14<03:07,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:47,920 >> Initializing global attention on CLS token...\n",
            " 92% 7674/8340 [50:14<03:05,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:48,195 >> Initializing global attention on CLS token...\n",
            " 92% 7675/8340 [50:14<03:04,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:48,471 >> Initializing global attention on CLS token...\n",
            " 92% 7676/8340 [50:15<03:04,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:48,751 >> Initializing global attention on CLS token...\n",
            " 92% 7677/8340 [50:15<03:03,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:49,021 >> Initializing global attention on CLS token...\n",
            " 92% 7678/8340 [50:15<03:03,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:49,297 >> Initializing global attention on CLS token...\n",
            " 92% 7679/8340 [50:16<03:01,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:49,570 >> Initializing global attention on CLS token...\n",
            " 92% 7680/8340 [50:16<03:02,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:49,852 >> Initializing global attention on CLS token...\n",
            " 92% 7681/8340 [50:16<03:02,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:50,128 >> Initializing global attention on CLS token...\n",
            " 92% 7682/8340 [50:16<03:01,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:50,406 >> Initializing global attention on CLS token...\n",
            " 92% 7683/8340 [50:17<03:02,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:50,687 >> Initializing global attention on CLS token...\n",
            " 92% 7684/8340 [50:17<03:02,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:50,966 >> Initializing global attention on CLS token...\n",
            " 92% 7685/8340 [50:17<03:01,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:51,238 >> Initializing global attention on CLS token...\n",
            " 92% 7686/8340 [50:17<03:01,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:51,522 >> Initializing global attention on CLS token...\n",
            " 92% 7687/8340 [50:18<03:02,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:51,800 >> Initializing global attention on CLS token...\n",
            " 92% 7688/8340 [50:18<03:00,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:52,075 >> Initializing global attention on CLS token...\n",
            " 92% 7689/8340 [50:18<03:00,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:52,351 >> Initializing global attention on CLS token...\n",
            " 92% 7690/8340 [50:19<03:00,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:52,629 >> Initializing global attention on CLS token...\n",
            " 92% 7691/8340 [50:19<02:59,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:52,904 >> Initializing global attention on CLS token...\n",
            " 92% 7692/8340 [50:19<02:59,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:53,183 >> Initializing global attention on CLS token...\n",
            " 92% 7693/8340 [50:19<02:59,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:53,458 >> Initializing global attention on CLS token...\n",
            " 92% 7694/8340 [50:20<02:58,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:53,738 >> Initializing global attention on CLS token...\n",
            " 92% 7695/8340 [50:20<03:00,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:54,022 >> Initializing global attention on CLS token...\n",
            " 92% 7696/8340 [50:20<03:00,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:54,311 >> Initializing global attention on CLS token...\n",
            " 92% 7697/8340 [50:21<03:01,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:54,587 >> Initializing global attention on CLS token...\n",
            " 92% 7698/8340 [50:21<03:00,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:54,864 >> Initializing global attention on CLS token...\n",
            " 92% 7699/8340 [50:21<02:58,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:55,149 >> Initializing global attention on CLS token...\n",
            " 92% 7700/8340 [50:21<02:59,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:55,428 >> Initializing global attention on CLS token...\n",
            " 92% 7701/8340 [50:22<02:59,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:55,707 >> Initializing global attention on CLS token...\n",
            " 92% 7702/8340 [50:22<02:59,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:55,985 >> Initializing global attention on CLS token...\n",
            " 92% 7703/8340 [50:22<02:57,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:56,258 >> Initializing global attention on CLS token...\n",
            " 92% 7704/8340 [50:23<02:57,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:56,539 >> Initializing global attention on CLS token...\n",
            " 92% 7705/8340 [50:23<02:56,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:56,815 >> Initializing global attention on CLS token...\n",
            " 92% 7706/8340 [50:23<02:55,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:57,091 >> Initializing global attention on CLS token...\n",
            " 92% 7707/8340 [50:23<02:56,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:57,371 >> Initializing global attention on CLS token...\n",
            " 92% 7708/8340 [50:24<02:55,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:57,648 >> Initializing global attention on CLS token...\n",
            " 92% 7709/8340 [50:24<02:55,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:57,922 >> Initializing global attention on CLS token...\n",
            " 92% 7710/8340 [50:24<02:54,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:58,198 >> Initializing global attention on CLS token...\n",
            " 92% 7711/8340 [50:24<02:53,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:58,470 >> Initializing global attention on CLS token...\n",
            " 92% 7712/8340 [50:25<02:53,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:58,750 >> Initializing global attention on CLS token...\n",
            " 92% 7713/8340 [50:25<02:52,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:59,022 >> Initializing global attention on CLS token...\n",
            " 92% 7714/8340 [50:25<02:52,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:59,306 >> Initializing global attention on CLS token...\n",
            " 93% 7715/8340 [50:26<02:53,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:59,585 >> Initializing global attention on CLS token...\n",
            " 93% 7716/8340 [50:26<02:53,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:58:59,868 >> Initializing global attention on CLS token...\n",
            " 93% 7717/8340 [50:26<02:53,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:00,139 >> Initializing global attention on CLS token...\n",
            " 93% 7718/8340 [50:26<02:52,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:00,418 >> Initializing global attention on CLS token...\n",
            " 93% 7719/8340 [50:27<02:52,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:00,697 >> Initializing global attention on CLS token...\n",
            " 93% 7720/8340 [50:27<02:52,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:00,973 >> Initializing global attention on CLS token...\n",
            " 93% 7721/8340 [50:27<02:51,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:01,246 >> Initializing global attention on CLS token...\n",
            " 93% 7722/8340 [50:27<02:51,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:01,527 >> Initializing global attention on CLS token...\n",
            " 93% 7723/8340 [50:28<02:52,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:01,821 >> Initializing global attention on CLS token...\n",
            " 93% 7724/8340 [50:28<02:53,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:02,096 >> Initializing global attention on CLS token...\n",
            " 93% 7725/8340 [50:28<02:53,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:02,377 >> Initializing global attention on CLS token...\n",
            " 93% 7726/8340 [50:29<02:51,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:02,652 >> Initializing global attention on CLS token...\n",
            " 93% 7727/8340 [50:29<02:50,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:02,928 >> Initializing global attention on CLS token...\n",
            " 93% 7728/8340 [50:29<02:50,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:03,206 >> Initializing global attention on CLS token...\n",
            " 93% 7729/8340 [50:29<02:49,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:03,477 >> Initializing global attention on CLS token...\n",
            " 93% 7730/8340 [50:30<02:48,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:03,753 >> Initializing global attention on CLS token...\n",
            " 93% 7731/8340 [50:30<02:47,  3.64it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:04,028 >> Initializing global attention on CLS token...\n",
            " 93% 7732/8340 [50:30<02:48,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:04,308 >> Initializing global attention on CLS token...\n",
            " 93% 7733/8340 [50:31<02:47,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:04,581 >> Initializing global attention on CLS token...\n",
            " 93% 7734/8340 [50:31<02:47,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:04,861 >> Initializing global attention on CLS token...\n",
            " 93% 7735/8340 [50:31<02:48,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:05,147 >> Initializing global attention on CLS token...\n",
            " 93% 7736/8340 [50:31<02:49,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:05,434 >> Initializing global attention on CLS token...\n",
            " 93% 7737/8340 [50:32<02:50,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:05,717 >> Initializing global attention on CLS token...\n",
            " 93% 7738/8340 [50:32<02:49,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:05,995 >> Initializing global attention on CLS token...\n",
            " 93% 7739/8340 [50:32<02:48,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:06,274 >> Initializing global attention on CLS token...\n",
            " 93% 7740/8340 [50:33<02:47,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:06,555 >> Initializing global attention on CLS token...\n",
            " 93% 7741/8340 [50:33<02:48,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:06,846 >> Initializing global attention on CLS token...\n",
            " 93% 7742/8340 [50:33<02:48,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:07,121 >> Initializing global attention on CLS token...\n",
            " 93% 7743/8340 [50:33<02:47,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:07,397 >> Initializing global attention on CLS token...\n",
            " 93% 7744/8340 [50:34<02:46,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:07,674 >> Initializing global attention on CLS token...\n",
            " 93% 7745/8340 [50:34<02:46,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:07,954 >> Initializing global attention on CLS token...\n",
            " 93% 7746/8340 [50:34<02:46,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:08,237 >> Initializing global attention on CLS token...\n",
            " 93% 7747/8340 [50:34<02:46,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:08,518 >> Initializing global attention on CLS token...\n",
            " 93% 7748/8340 [50:35<02:45,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:08,795 >> Initializing global attention on CLS token...\n",
            " 93% 7749/8340 [50:35<02:45,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:09,090 >> Initializing global attention on CLS token...\n",
            " 93% 7750/8340 [50:35<02:47,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:09,369 >> Initializing global attention on CLS token...\n",
            " 93% 7751/8340 [50:36<02:45,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:09,644 >> Initializing global attention on CLS token...\n",
            " 93% 7752/8340 [50:36<02:45,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:09,925 >> Initializing global attention on CLS token...\n",
            " 93% 7753/8340 [50:36<02:44,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:10,205 >> Initializing global attention on CLS token...\n",
            " 93% 7754/8340 [50:36<02:43,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:10,481 >> Initializing global attention on CLS token...\n",
            " 93% 7755/8340 [50:37<02:44,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:10,765 >> Initializing global attention on CLS token...\n",
            " 93% 7756/8340 [50:37<02:44,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:11,046 >> Initializing global attention on CLS token...\n",
            " 93% 7757/8340 [50:37<02:43,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:11,327 >> Initializing global attention on CLS token...\n",
            " 93% 7758/8340 [50:38<02:42,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:11,605 >> Initializing global attention on CLS token...\n",
            " 93% 7759/8340 [50:38<02:42,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:11,889 >> Initializing global attention on CLS token...\n",
            " 93% 7760/8340 [50:38<02:42,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:12,164 >> Initializing global attention on CLS token...\n",
            " 93% 7761/8340 [50:38<02:41,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:12,443 >> Initializing global attention on CLS token...\n",
            " 93% 7762/8340 [50:39<02:41,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:12,719 >> Initializing global attention on CLS token...\n",
            " 93% 7763/8340 [50:39<02:41,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:13,004 >> Initializing global attention on CLS token...\n",
            " 93% 7764/8340 [50:39<02:41,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:13,283 >> Initializing global attention on CLS token...\n",
            " 93% 7765/8340 [50:40<02:40,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:13,557 >> Initializing global attention on CLS token...\n",
            " 93% 7766/8340 [50:40<02:39,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:13,844 >> Initializing global attention on CLS token...\n",
            " 93% 7767/8340 [50:40<02:40,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:14,123 >> Initializing global attention on CLS token...\n",
            " 93% 7768/8340 [50:40<02:40,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:14,412 >> Initializing global attention on CLS token...\n",
            " 93% 7769/8340 [50:41<02:41,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:14,690 >> Initializing global attention on CLS token...\n",
            " 93% 7770/8340 [50:41<02:41,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:14,976 >> Initializing global attention on CLS token...\n",
            " 93% 7771/8340 [50:41<02:41,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:15,260 >> Initializing global attention on CLS token...\n",
            " 93% 7772/8340 [50:42<02:41,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:15,543 >> Initializing global attention on CLS token...\n",
            " 93% 7773/8340 [50:42<02:40,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:15,824 >> Initializing global attention on CLS token...\n",
            " 93% 7774/8340 [50:42<02:39,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:16,099 >> Initializing global attention on CLS token...\n",
            " 93% 7775/8340 [50:42<02:37,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:16,374 >> Initializing global attention on CLS token...\n",
            " 93% 7776/8340 [50:43<02:37,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:16,663 >> Initializing global attention on CLS token...\n",
            " 93% 7777/8340 [50:43<02:39,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:16,949 >> Initializing global attention on CLS token...\n",
            " 93% 7778/8340 [50:43<02:39,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:17,231 >> Initializing global attention on CLS token...\n",
            " 93% 7779/8340 [50:43<02:38,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:17,512 >> Initializing global attention on CLS token...\n",
            " 93% 7780/8340 [50:44<02:37,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:17,791 >> Initializing global attention on CLS token...\n",
            " 93% 7781/8340 [50:44<02:36,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:18,067 >> Initializing global attention on CLS token...\n",
            " 93% 7782/8340 [50:44<02:36,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:18,349 >> Initializing global attention on CLS token...\n",
            " 93% 7783/8340 [50:45<02:36,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:18,630 >> Initializing global attention on CLS token...\n",
            " 93% 7784/8340 [50:45<02:35,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:18,913 >> Initializing global attention on CLS token...\n",
            " 93% 7785/8340 [50:45<02:35,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:19,193 >> Initializing global attention on CLS token...\n",
            " 93% 7786/8340 [50:45<02:35,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:19,475 >> Initializing global attention on CLS token...\n",
            " 93% 7787/8340 [50:46<02:35,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:19,761 >> Initializing global attention on CLS token...\n",
            " 93% 7788/8340 [50:46<02:35,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:20,036 >> Initializing global attention on CLS token...\n",
            " 93% 7789/8340 [50:46<02:35,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:20,316 >> Initializing global attention on CLS token...\n",
            " 93% 7790/8340 [50:47<02:33,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:20,595 >> Initializing global attention on CLS token...\n",
            " 93% 7791/8340 [50:47<02:33,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:20,885 >> Initializing global attention on CLS token...\n",
            " 93% 7792/8340 [50:47<02:34,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:21,161 >> Initializing global attention on CLS token...\n",
            " 93% 7793/8340 [50:47<02:33,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:21,437 >> Initializing global attention on CLS token...\n",
            " 93% 7794/8340 [50:48<02:33,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:21,723 >> Initializing global attention on CLS token...\n",
            " 93% 7795/8340 [50:48<02:33,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:22,003 >> Initializing global attention on CLS token...\n",
            " 93% 7796/8340 [50:48<02:32,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:22,285 >> Initializing global attention on CLS token...\n",
            " 93% 7797/8340 [50:49<02:32,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:22,558 >> Initializing global attention on CLS token...\n",
            " 94% 7798/8340 [50:49<02:31,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:22,840 >> Initializing global attention on CLS token...\n",
            " 94% 7799/8340 [50:49<02:32,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:23,125 >> Initializing global attention on CLS token...\n",
            " 94% 7800/8340 [50:49<02:32,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:23,418 >> Initializing global attention on CLS token...\n",
            " 94% 7801/8340 [50:50<02:32,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:23,704 >> Initializing global attention on CLS token...\n",
            " 94% 7802/8340 [50:50<02:33,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:23,981 >> Initializing global attention on CLS token...\n",
            " 94% 7803/8340 [50:50<02:32,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:24,262 >> Initializing global attention on CLS token...\n",
            " 94% 7804/8340 [50:51<02:32,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:24,546 >> Initializing global attention on CLS token...\n",
            " 94% 7805/8340 [50:51<02:31,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:24,830 >> Initializing global attention on CLS token...\n",
            " 94% 7806/8340 [50:51<02:31,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:25,117 >> Initializing global attention on CLS token...\n",
            " 94% 7807/8340 [50:51<02:31,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:25,400 >> Initializing global attention on CLS token...\n",
            " 94% 7808/8340 [50:52<02:30,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:25,681 >> Initializing global attention on CLS token...\n",
            " 94% 7809/8340 [50:52<02:30,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:25,970 >> Initializing global attention on CLS token...\n",
            " 94% 7810/8340 [50:52<02:30,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:26,257 >> Initializing global attention on CLS token...\n",
            " 94% 7811/8340 [50:53<02:31,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:26,550 >> Initializing global attention on CLS token...\n",
            " 94% 7812/8340 [50:53<02:30,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:26,829 >> Initializing global attention on CLS token...\n",
            " 94% 7813/8340 [50:53<02:29,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:27,108 >> Initializing global attention on CLS token...\n",
            " 94% 7814/8340 [50:53<02:29,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:27,390 >> Initializing global attention on CLS token...\n",
            " 94% 7815/8340 [50:54<02:29,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:27,687 >> Initializing global attention on CLS token...\n",
            " 94% 7816/8340 [50:54<02:29,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:27,968 >> Initializing global attention on CLS token...\n",
            " 94% 7817/8340 [50:54<02:28,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:28,246 >> Initializing global attention on CLS token...\n",
            " 94% 7818/8340 [50:54<02:28,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:28,528 >> Initializing global attention on CLS token...\n",
            " 94% 7819/8340 [50:55<02:27,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:28,812 >> Initializing global attention on CLS token...\n",
            " 94% 7820/8340 [50:55<02:27,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:29,096 >> Initializing global attention on CLS token...\n",
            " 94% 7821/8340 [50:55<02:27,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:29,381 >> Initializing global attention on CLS token...\n",
            " 94% 7822/8340 [50:56<02:27,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:29,665 >> Initializing global attention on CLS token...\n",
            " 94% 7823/8340 [50:56<02:27,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:29,958 >> Initializing global attention on CLS token...\n",
            " 94% 7824/8340 [50:56<02:27,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:30,237 >> Initializing global attention on CLS token...\n",
            " 94% 7825/8340 [50:56<02:26,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:30,520 >> Initializing global attention on CLS token...\n",
            " 94% 7826/8340 [50:57<02:25,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:30,799 >> Initializing global attention on CLS token...\n",
            " 94% 7827/8340 [50:57<02:24,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:31,078 >> Initializing global attention on CLS token...\n",
            " 94% 7828/8340 [50:57<02:23,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:31,356 >> Initializing global attention on CLS token...\n",
            " 94% 7829/8340 [50:58<02:23,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:31,639 >> Initializing global attention on CLS token...\n",
            " 94% 7830/8340 [50:58<02:23,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:31,923 >> Initializing global attention on CLS token...\n",
            " 94% 7831/8340 [50:58<02:23,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:32,208 >> Initializing global attention on CLS token...\n",
            " 94% 7832/8340 [50:58<02:24,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:32,495 >> Initializing global attention on CLS token...\n",
            " 94% 7833/8340 [50:59<02:24,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:32,780 >> Initializing global attention on CLS token...\n",
            " 94% 7834/8340 [50:59<02:23,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:33,061 >> Initializing global attention on CLS token...\n",
            " 94% 7835/8340 [50:59<02:23,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:33,346 >> Initializing global attention on CLS token...\n",
            " 94% 7836/8340 [51:00<02:23,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:33,633 >> Initializing global attention on CLS token...\n",
            " 94% 7837/8340 [51:00<02:22,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:33,916 >> Initializing global attention on CLS token...\n",
            " 94% 7838/8340 [51:00<02:22,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:34,200 >> Initializing global attention on CLS token...\n",
            " 94% 7839/8340 [51:00<02:22,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:34,480 >> Initializing global attention on CLS token...\n",
            " 94% 7840/8340 [51:01<02:21,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:34,762 >> Initializing global attention on CLS token...\n",
            " 94% 7841/8340 [51:01<02:20,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:35,044 >> Initializing global attention on CLS token...\n",
            " 94% 7842/8340 [51:01<02:19,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:35,323 >> Initializing global attention on CLS token...\n",
            " 94% 7843/8340 [51:02<02:20,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:35,603 >> Initializing global attention on CLS token...\n",
            " 94% 7844/8340 [51:02<02:20,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:35,887 >> Initializing global attention on CLS token...\n",
            " 94% 7845/8340 [51:02<02:20,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:36,172 >> Initializing global attention on CLS token...\n",
            " 94% 7846/8340 [51:02<02:20,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:36,463 >> Initializing global attention on CLS token...\n",
            " 94% 7847/8340 [51:03<02:20,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:36,741 >> Initializing global attention on CLS token...\n",
            " 94% 7848/8340 [51:03<02:19,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:37,024 >> Initializing global attention on CLS token...\n",
            " 94% 7849/8340 [51:03<02:19,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:37,307 >> Initializing global attention on CLS token...\n",
            " 94% 7850/8340 [51:04<02:18,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:37,588 >> Initializing global attention on CLS token...\n",
            " 94% 7851/8340 [51:04<02:17,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:37,869 >> Initializing global attention on CLS token...\n",
            " 94% 7852/8340 [51:04<02:16,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:38,148 >> Initializing global attention on CLS token...\n",
            " 94% 7853/8340 [51:04<02:17,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:38,429 >> Initializing global attention on CLS token...\n",
            " 94% 7854/8340 [51:05<02:17,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:38,712 >> Initializing global attention on CLS token...\n",
            " 94% 7855/8340 [51:05<02:17,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:38,997 >> Initializing global attention on CLS token...\n",
            " 94% 7856/8340 [51:05<02:17,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:39,283 >> Initializing global attention on CLS token...\n",
            " 94% 7857/8340 [51:06<02:17,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:39,584 >> Initializing global attention on CLS token...\n",
            " 94% 7858/8340 [51:06<02:19,  3.47it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:39,866 >> Initializing global attention on CLS token...\n",
            " 94% 7859/8340 [51:06<02:17,  3.49it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:40,147 >> Initializing global attention on CLS token...\n",
            " 94% 7860/8340 [51:06<02:15,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:40,421 >> Initializing global attention on CLS token...\n",
            " 94% 7861/8340 [51:07<02:15,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:40,710 >> Initializing global attention on CLS token...\n",
            " 94% 7862/8340 [51:07<02:15,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:40,995 >> Initializing global attention on CLS token...\n",
            " 94% 7863/8340 [51:07<02:15,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:41,275 >> Initializing global attention on CLS token...\n",
            " 94% 7864/8340 [51:08<02:14,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:41,555 >> Initializing global attention on CLS token...\n",
            " 94% 7865/8340 [51:08<02:14,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:41,841 >> Initializing global attention on CLS token...\n",
            " 94% 7866/8340 [51:08<02:14,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:42,127 >> Initializing global attention on CLS token...\n",
            " 94% 7867/8340 [51:08<02:14,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:42,413 >> Initializing global attention on CLS token...\n",
            " 94% 7868/8340 [51:09<02:14,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:42,698 >> Initializing global attention on CLS token...\n",
            " 94% 7869/8340 [51:09<02:13,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:42,980 >> Initializing global attention on CLS token...\n",
            " 94% 7870/8340 [51:09<02:13,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:43,264 >> Initializing global attention on CLS token...\n",
            " 94% 7871/8340 [51:10<02:13,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:43,559 >> Initializing global attention on CLS token...\n",
            " 94% 7872/8340 [51:10<02:13,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:43,841 >> Initializing global attention on CLS token...\n",
            " 94% 7873/8340 [51:10<02:12,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:44,120 >> Initializing global attention on CLS token...\n",
            " 94% 7874/8340 [51:10<02:12,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:44,407 >> Initializing global attention on CLS token...\n",
            " 94% 7875/8340 [51:11<02:12,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:44,685 >> Initializing global attention on CLS token...\n",
            " 94% 7876/8340 [51:11<02:11,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:44,971 >> Initializing global attention on CLS token...\n",
            " 94% 7877/8340 [51:11<02:11,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:45,257 >> Initializing global attention on CLS token...\n",
            " 94% 7878/8340 [51:12<02:11,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:45,539 >> Initializing global attention on CLS token...\n",
            " 94% 7879/8340 [51:12<02:10,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:45,828 >> Initializing global attention on CLS token...\n",
            " 94% 7880/8340 [51:12<02:10,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:46,108 >> Initializing global attention on CLS token...\n",
            " 94% 7881/8340 [51:12<02:10,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:46,393 >> Initializing global attention on CLS token...\n",
            " 95% 7882/8340 [51:13<02:10,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:46,678 >> Initializing global attention on CLS token...\n",
            " 95% 7883/8340 [51:13<02:10,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:46,968 >> Initializing global attention on CLS token...\n",
            " 95% 7884/8340 [51:13<02:09,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:47,248 >> Initializing global attention on CLS token...\n",
            " 95% 7885/8340 [51:13<02:09,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:47,532 >> Initializing global attention on CLS token...\n",
            " 95% 7886/8340 [51:14<02:09,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:47,814 >> Initializing global attention on CLS token...\n",
            " 95% 7887/8340 [51:14<02:08,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:48,100 >> Initializing global attention on CLS token...\n",
            " 95% 7888/8340 [51:14<02:08,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:48,388 >> Initializing global attention on CLS token...\n",
            " 95% 7889/8340 [51:15<02:08,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:48,686 >> Initializing global attention on CLS token...\n",
            " 95% 7890/8340 [51:15<02:10,  3.45it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:48,969 >> Initializing global attention on CLS token...\n",
            " 95% 7891/8340 [51:15<02:09,  3.48it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:49,263 >> Initializing global attention on CLS token...\n",
            " 95% 7892/8340 [51:16<02:08,  3.48it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:49,540 >> Initializing global attention on CLS token...\n",
            " 95% 7893/8340 [51:16<02:08,  3.49it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:49,823 >> Initializing global attention on CLS token...\n",
            " 95% 7894/8340 [51:16<02:07,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:50,111 >> Initializing global attention on CLS token...\n",
            " 95% 7895/8340 [51:16<02:06,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:50,393 >> Initializing global attention on CLS token...\n",
            " 95% 7896/8340 [51:17<02:06,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:50,682 >> Initializing global attention on CLS token...\n",
            " 95% 7897/8340 [51:17<02:06,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:50,965 >> Initializing global attention on CLS token...\n",
            " 95% 7898/8340 [51:17<02:05,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:51,247 >> Initializing global attention on CLS token...\n",
            " 95% 7899/8340 [51:17<02:05,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:51,533 >> Initializing global attention on CLS token...\n",
            " 95% 7900/8340 [51:18<02:05,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:51,815 >> Initializing global attention on CLS token...\n",
            " 95% 7901/8340 [51:18<02:04,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:52,100 >> Initializing global attention on CLS token...\n",
            " 95% 7902/8340 [51:18<02:04,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:52,384 >> Initializing global attention on CLS token...\n",
            " 95% 7903/8340 [51:19<02:04,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:52,665 >> Initializing global attention on CLS token...\n",
            " 95% 7904/8340 [51:19<02:03,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:52,945 >> Initializing global attention on CLS token...\n",
            " 95% 7905/8340 [51:19<02:02,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:53,220 >> Initializing global attention on CLS token...\n",
            " 95% 7906/8340 [51:19<02:01,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:53,510 >> Initializing global attention on CLS token...\n",
            " 95% 7907/8340 [51:20<02:02,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:53,785 >> Initializing global attention on CLS token...\n",
            " 95% 7908/8340 [51:20<02:01,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:54,068 >> Initializing global attention on CLS token...\n",
            " 95% 7909/8340 [51:20<02:01,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:54,352 >> Initializing global attention on CLS token...\n",
            " 95% 7910/8340 [51:21<02:01,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:54,635 >> Initializing global attention on CLS token...\n",
            " 95% 7911/8340 [51:21<02:00,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:54,928 >> Initializing global attention on CLS token...\n",
            " 95% 7912/8340 [51:21<02:02,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:55,215 >> Initializing global attention on CLS token...\n",
            " 95% 7913/8340 [51:21<02:01,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:55,492 >> Initializing global attention on CLS token...\n",
            " 95% 7914/8340 [51:22<02:01,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:55,782 >> Initializing global attention on CLS token...\n",
            " 95% 7915/8340 [51:22<02:01,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:56,070 >> Initializing global attention on CLS token...\n",
            " 95% 7916/8340 [51:22<02:01,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:56,364 >> Initializing global attention on CLS token...\n",
            " 95% 7917/8340 [51:23<02:02,  3.47it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:56,651 >> Initializing global attention on CLS token...\n",
            " 95% 7918/8340 [51:23<02:01,  3.46it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:56,948 >> Initializing global attention on CLS token...\n",
            " 95% 7919/8340 [51:23<02:01,  3.47it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:57,226 >> Initializing global attention on CLS token...\n",
            " 95% 7920/8340 [51:23<02:00,  3.48it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:57,508 >> Initializing global attention on CLS token...\n",
            " 95% 7921/8340 [51:24<01:59,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:57,802 >> Initializing global attention on CLS token...\n",
            " 95% 7922/8340 [51:24<01:59,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:58,082 >> Initializing global attention on CLS token...\n",
            " 95% 7923/8340 [51:24<01:59,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:58,369 >> Initializing global attention on CLS token...\n",
            " 95% 7924/8340 [51:25<01:58,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:58,655 >> Initializing global attention on CLS token...\n",
            " 95% 7925/8340 [51:25<01:58,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:58,930 >> Initializing global attention on CLS token...\n",
            " 95% 7926/8340 [51:25<01:57,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:59,212 >> Initializing global attention on CLS token...\n",
            " 95% 7927/8340 [51:25<01:57,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:59,498 >> Initializing global attention on CLS token...\n",
            " 95% 7928/8340 [51:26<01:57,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 17:59:59,781 >> Initializing global attention on CLS token...\n",
            " 95% 7929/8340 [51:26<01:56,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:00,067 >> Initializing global attention on CLS token...\n",
            " 95% 7930/8340 [51:26<01:56,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:00,350 >> Initializing global attention on CLS token...\n",
            " 95% 7931/8340 [51:27<01:56,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:00,633 >> Initializing global attention on CLS token...\n",
            " 95% 7932/8340 [51:27<01:55,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:00,916 >> Initializing global attention on CLS token...\n",
            " 95% 7933/8340 [51:27<01:55,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:01,216 >> Initializing global attention on CLS token...\n",
            " 95% 7934/8340 [51:27<01:56,  3.49it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:01,494 >> Initializing global attention on CLS token...\n",
            " 95% 7935/8340 [51:28<01:54,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:01,771 >> Initializing global attention on CLS token...\n",
            " 95% 7936/8340 [51:28<01:54,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:02,052 >> Initializing global attention on CLS token...\n",
            " 95% 7937/8340 [51:28<01:54,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:02,334 >> Initializing global attention on CLS token...\n",
            " 95% 7938/8340 [51:29<01:53,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:02,615 >> Initializing global attention on CLS token...\n",
            " 95% 7939/8340 [51:29<01:52,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:02,901 >> Initializing global attention on CLS token...\n",
            " 95% 7940/8340 [51:29<01:52,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:03,176 >> Initializing global attention on CLS token...\n",
            " 95% 7941/8340 [51:29<01:52,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:03,457 >> Initializing global attention on CLS token...\n",
            " 95% 7942/8340 [51:30<01:52,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:03,757 >> Initializing global attention on CLS token...\n",
            " 95% 7943/8340 [51:30<01:53,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:04,043 >> Initializing global attention on CLS token...\n",
            " 95% 7944/8340 [51:30<01:53,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:04,323 >> Initializing global attention on CLS token...\n",
            " 95% 7945/8340 [51:31<01:51,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:04,601 >> Initializing global attention on CLS token...\n",
            " 95% 7946/8340 [51:31<01:51,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:04,883 >> Initializing global attention on CLS token...\n",
            " 95% 7947/8340 [51:31<01:51,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:05,167 >> Initializing global attention on CLS token...\n",
            " 95% 7948/8340 [51:31<01:51,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:05,453 >> Initializing global attention on CLS token...\n",
            " 95% 7949/8340 [51:32<01:50,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:05,741 >> Initializing global attention on CLS token...\n",
            " 95% 7950/8340 [51:32<01:50,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:06,026 >> Initializing global attention on CLS token...\n",
            " 95% 7951/8340 [51:32<01:50,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:06,304 >> Initializing global attention on CLS token...\n",
            " 95% 7952/8340 [51:33<01:50,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:06,586 >> Initializing global attention on CLS token...\n",
            " 95% 7953/8340 [51:33<01:49,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:06,870 >> Initializing global attention on CLS token...\n",
            " 95% 7954/8340 [51:33<01:49,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:07,160 >> Initializing global attention on CLS token...\n",
            " 95% 7955/8340 [51:33<01:49,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:07,444 >> Initializing global attention on CLS token...\n",
            " 95% 7956/8340 [51:34<01:49,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:07,728 >> Initializing global attention on CLS token...\n",
            " 95% 7957/8340 [51:34<01:48,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:08,009 >> Initializing global attention on CLS token...\n",
            " 95% 7958/8340 [51:34<01:48,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:08,289 >> Initializing global attention on CLS token...\n",
            " 95% 7959/8340 [51:35<01:47,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:08,567 >> Initializing global attention on CLS token...\n",
            " 95% 7960/8340 [51:35<01:46,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:08,846 >> Initializing global attention on CLS token...\n",
            " 95% 7961/8340 [51:35<01:46,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:09,128 >> Initializing global attention on CLS token...\n",
            " 95% 7962/8340 [51:35<01:46,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:09,411 >> Initializing global attention on CLS token...\n",
            " 95% 7963/8340 [51:36<01:45,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:09,705 >> Initializing global attention on CLS token...\n",
            " 95% 7964/8340 [51:36<01:47,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:09,989 >> Initializing global attention on CLS token...\n",
            " 96% 7965/8340 [51:36<01:46,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:10,268 >> Initializing global attention on CLS token...\n",
            " 96% 7966/8340 [51:37<01:46,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:10,550 >> Initializing global attention on CLS token...\n",
            " 96% 7967/8340 [51:37<01:45,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:10,829 >> Initializing global attention on CLS token...\n",
            " 96% 7968/8340 [51:37<01:44,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:11,105 >> Initializing global attention on CLS token...\n",
            " 96% 7969/8340 [51:37<01:44,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:11,396 >> Initializing global attention on CLS token...\n",
            " 96% 7970/8340 [51:38<01:44,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:11,672 >> Initializing global attention on CLS token...\n",
            " 96% 7971/8340 [51:38<01:43,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:11,953 >> Initializing global attention on CLS token...\n",
            " 96% 7972/8340 [51:38<01:43,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:12,231 >> Initializing global attention on CLS token...\n",
            " 96% 7973/8340 [51:38<01:42,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:12,512 >> Initializing global attention on CLS token...\n",
            " 96% 7974/8340 [51:39<01:42,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:12,796 >> Initializing global attention on CLS token...\n",
            " 96% 7975/8340 [51:39<01:42,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:13,078 >> Initializing global attention on CLS token...\n",
            " 96% 7976/8340 [51:39<01:42,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:13,360 >> Initializing global attention on CLS token...\n",
            " 96% 7977/8340 [51:40<01:41,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:13,640 >> Initializing global attention on CLS token...\n",
            " 96% 7978/8340 [51:40<01:41,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:13,915 >> Initializing global attention on CLS token...\n",
            " 96% 7979/8340 [51:40<01:41,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:14,198 >> Initializing global attention on CLS token...\n",
            " 96% 7980/8340 [51:40<01:41,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:14,480 >> Initializing global attention on CLS token...\n",
            " 96% 7981/8340 [51:41<01:40,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:14,757 >> Initializing global attention on CLS token...\n",
            " 96% 7982/8340 [51:41<01:40,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:15,038 >> Initializing global attention on CLS token...\n",
            " 96% 7983/8340 [51:41<01:40,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:15,324 >> Initializing global attention on CLS token...\n",
            " 96% 7984/8340 [51:42<01:40,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:15,603 >> Initializing global attention on CLS token...\n",
            " 96% 7985/8340 [51:42<01:40,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:15,888 >> Initializing global attention on CLS token...\n",
            " 96% 7986/8340 [51:42<01:40,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:16,170 >> Initializing global attention on CLS token...\n",
            " 96% 7987/8340 [51:42<01:39,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:16,463 >> Initializing global attention on CLS token...\n",
            " 96% 7988/8340 [51:43<01:40,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:16,745 >> Initializing global attention on CLS token...\n",
            " 96% 7989/8340 [51:43<01:39,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:17,024 >> Initializing global attention on CLS token...\n",
            " 96% 7990/8340 [51:43<01:38,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:17,298 >> Initializing global attention on CLS token...\n",
            " 96% 7991/8340 [51:44<01:37,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:17,581 >> Initializing global attention on CLS token...\n",
            " 96% 7992/8340 [51:44<01:38,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:17,862 >> Initializing global attention on CLS token...\n",
            " 96% 7993/8340 [51:44<01:37,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:18,156 >> Initializing global attention on CLS token...\n",
            " 96% 7994/8340 [51:44<01:38,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:18,432 >> Initializing global attention on CLS token...\n",
            " 96% 7995/8340 [51:45<01:37,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:18,712 >> Initializing global attention on CLS token...\n",
            " 96% 7996/8340 [51:45<01:36,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:18,988 >> Initializing global attention on CLS token...\n",
            " 96% 7997/8340 [51:45<01:36,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:19,281 >> Initializing global attention on CLS token...\n",
            " 96% 7998/8340 [51:46<01:36,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:19,560 >> Initializing global attention on CLS token...\n",
            " 96% 7999/8340 [51:46<01:36,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:19,838 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.6134, 'learning_rate': 4.1127098321342924e-05, 'epoch': 9.59}\n",
            " 96% 8000/8340 [51:46<01:36,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:20,127 >> Initializing global attention on CLS token...\n",
            " 96% 8001/8340 [51:46<01:35,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:20,409 >> Initializing global attention on CLS token...\n",
            " 96% 8002/8340 [51:47<01:35,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:20,691 >> Initializing global attention on CLS token...\n",
            " 96% 8003/8340 [51:47<01:35,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:20,976 >> Initializing global attention on CLS token...\n",
            " 96% 8004/8340 [51:47<01:34,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:21,254 >> Initializing global attention on CLS token...\n",
            " 96% 8005/8340 [51:47<01:33,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:21,529 >> Initializing global attention on CLS token...\n",
            " 96% 8006/8340 [51:48<01:33,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:21,813 >> Initializing global attention on CLS token...\n",
            " 96% 8007/8340 [51:48<01:33,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:22,093 >> Initializing global attention on CLS token...\n",
            " 96% 8008/8340 [51:48<01:33,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:22,373 >> Initializing global attention on CLS token...\n",
            " 96% 8009/8340 [51:49<01:32,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:22,649 >> Initializing global attention on CLS token...\n",
            " 96% 8010/8340 [51:49<01:32,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:22,932 >> Initializing global attention on CLS token...\n",
            " 96% 8011/8340 [51:49<01:32,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:23,211 >> Initializing global attention on CLS token...\n",
            " 96% 8012/8340 [51:49<01:31,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:23,491 >> Initializing global attention on CLS token...\n",
            " 96% 8013/8340 [51:50<01:31,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:23,766 >> Initializing global attention on CLS token...\n",
            " 96% 8014/8340 [51:50<01:31,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:24,056 >> Initializing global attention on CLS token...\n",
            " 96% 8015/8340 [51:50<01:31,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:24,347 >> Initializing global attention on CLS token...\n",
            " 96% 8016/8340 [51:51<01:31,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:24,636 >> Initializing global attention on CLS token...\n",
            " 96% 8017/8340 [51:51<01:32,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:24,920 >> Initializing global attention on CLS token...\n",
            " 96% 8018/8340 [51:51<01:31,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:25,194 >> Initializing global attention on CLS token...\n",
            " 96% 8019/8340 [51:51<01:31,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:25,477 >> Initializing global attention on CLS token...\n",
            " 96% 8020/8340 [51:52<01:30,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:25,760 >> Initializing global attention on CLS token...\n",
            " 96% 8021/8340 [51:52<01:30,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:26,059 >> Initializing global attention on CLS token...\n",
            " 96% 8022/8340 [51:52<01:30,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:26,334 >> Initializing global attention on CLS token...\n",
            " 96% 8023/8340 [51:53<01:30,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:26,621 >> Initializing global attention on CLS token...\n",
            " 96% 8024/8340 [51:53<01:30,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:26,909 >> Initializing global attention on CLS token...\n",
            " 96% 8025/8340 [51:53<01:29,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:27,189 >> Initializing global attention on CLS token...\n",
            " 96% 8026/8340 [51:53<01:29,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:27,489 >> Initializing global attention on CLS token...\n",
            " 96% 8027/8340 [51:54<01:30,  3.47it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:27,770 >> Initializing global attention on CLS token...\n",
            " 96% 8028/8340 [51:54<01:28,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:28,058 >> Initializing global attention on CLS token...\n",
            " 96% 8029/8340 [51:54<01:28,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:28,332 >> Initializing global attention on CLS token...\n",
            " 96% 8030/8340 [51:55<01:27,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:28,614 >> Initializing global attention on CLS token...\n",
            " 96% 8031/8340 [51:55<01:27,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:28,897 >> Initializing global attention on CLS token...\n",
            " 96% 8032/8340 [51:55<01:27,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:29,180 >> Initializing global attention on CLS token...\n",
            " 96% 8033/8340 [51:55<01:27,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:29,467 >> Initializing global attention on CLS token...\n",
            " 96% 8034/8340 [51:56<01:26,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:29,746 >> Initializing global attention on CLS token...\n",
            " 96% 8035/8340 [51:56<01:26,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:30,027 >> Initializing global attention on CLS token...\n",
            " 96% 8036/8340 [51:56<01:25,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:30,303 >> Initializing global attention on CLS token...\n",
            " 96% 8037/8340 [51:57<01:24,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:30,583 >> Initializing global attention on CLS token...\n",
            " 96% 8038/8340 [51:57<01:24,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:30,868 >> Initializing global attention on CLS token...\n",
            " 96% 8039/8340 [51:57<01:24,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:31,148 >> Initializing global attention on CLS token...\n",
            " 96% 8040/8340 [51:57<01:24,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:31,432 >> Initializing global attention on CLS token...\n",
            " 96% 8041/8340 [51:58<01:24,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:31,731 >> Initializing global attention on CLS token...\n",
            " 96% 8042/8340 [51:58<01:25,  3.50it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:32,012 >> Initializing global attention on CLS token...\n",
            " 96% 8043/8340 [51:58<01:24,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:32,295 >> Initializing global attention on CLS token...\n",
            " 96% 8044/8340 [51:59<01:24,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:32,571 >> Initializing global attention on CLS token...\n",
            " 96% 8045/8340 [51:59<01:23,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:32,853 >> Initializing global attention on CLS token...\n",
            " 96% 8046/8340 [51:59<01:23,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:33,143 >> Initializing global attention on CLS token...\n",
            " 96% 8047/8340 [51:59<01:23,  3.51it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:33,425 >> Initializing global attention on CLS token...\n",
            " 96% 8048/8340 [52:00<01:23,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:33,712 >> Initializing global attention on CLS token...\n",
            " 97% 8049/8340 [52:00<01:22,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:33,992 >> Initializing global attention on CLS token...\n",
            " 97% 8050/8340 [52:00<01:22,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:34,276 >> Initializing global attention on CLS token...\n",
            " 97% 8051/8340 [52:01<01:21,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:34,552 >> Initializing global attention on CLS token...\n",
            " 97% 8052/8340 [52:01<01:20,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:34,826 >> Initializing global attention on CLS token...\n",
            " 97% 8053/8340 [52:01<01:20,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:35,106 >> Initializing global attention on CLS token...\n",
            " 97% 8054/8340 [52:01<01:19,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:35,382 >> Initializing global attention on CLS token...\n",
            " 97% 8055/8340 [52:02<01:19,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:35,663 >> Initializing global attention on CLS token...\n",
            " 97% 8056/8340 [52:02<01:19,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:35,948 >> Initializing global attention on CLS token...\n",
            " 97% 8057/8340 [52:02<01:19,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:36,231 >> Initializing global attention on CLS token...\n",
            " 97% 8058/8340 [52:02<01:19,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:36,514 >> Initializing global attention on CLS token...\n",
            " 97% 8059/8340 [52:03<01:19,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:36,794 >> Initializing global attention on CLS token...\n",
            " 97% 8060/8340 [52:03<01:18,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:37,080 >> Initializing global attention on CLS token...\n",
            " 97% 8061/8340 [52:03<01:18,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:37,355 >> Initializing global attention on CLS token...\n",
            " 97% 8062/8340 [52:04<01:17,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:37,632 >> Initializing global attention on CLS token...\n",
            " 97% 8063/8340 [52:04<01:17,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:37,912 >> Initializing global attention on CLS token...\n",
            " 97% 8064/8340 [52:04<01:16,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:38,187 >> Initializing global attention on CLS token...\n",
            " 97% 8065/8340 [52:04<01:16,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:38,465 >> Initializing global attention on CLS token...\n",
            " 97% 8066/8340 [52:05<01:16,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:38,746 >> Initializing global attention on CLS token...\n",
            " 97% 8067/8340 [52:05<01:16,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:39,024 >> Initializing global attention on CLS token...\n",
            " 97% 8068/8340 [52:05<01:15,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:39,296 >> Initializing global attention on CLS token...\n",
            " 97% 8069/8340 [52:06<01:15,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:39,578 >> Initializing global attention on CLS token...\n",
            " 97% 8070/8340 [52:06<01:14,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:39,860 >> Initializing global attention on CLS token...\n",
            " 97% 8071/8340 [52:06<01:14,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:40,131 >> Initializing global attention on CLS token...\n",
            " 97% 8072/8340 [52:06<01:14,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:40,412 >> Initializing global attention on CLS token...\n",
            " 97% 8073/8340 [52:07<01:14,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:40,686 >> Initializing global attention on CLS token...\n",
            " 97% 8074/8340 [52:07<01:13,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:40,961 >> Initializing global attention on CLS token...\n",
            " 97% 8075/8340 [52:07<01:13,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:41,244 >> Initializing global attention on CLS token...\n",
            " 97% 8076/8340 [52:07<01:13,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:41,518 >> Initializing global attention on CLS token...\n",
            " 97% 8077/8340 [52:08<01:12,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:41,795 >> Initializing global attention on CLS token...\n",
            " 97% 8078/8340 [52:08<01:12,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:42,079 >> Initializing global attention on CLS token...\n",
            " 97% 8079/8340 [52:08<01:12,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:42,357 >> Initializing global attention on CLS token...\n",
            " 97% 8080/8340 [52:09<01:12,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:42,629 >> Initializing global attention on CLS token...\n",
            " 97% 8081/8340 [52:09<01:11,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:42,905 >> Initializing global attention on CLS token...\n",
            " 97% 8082/8340 [52:09<01:11,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:43,184 >> Initializing global attention on CLS token...\n",
            " 97% 8083/8340 [52:09<01:11,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:43,460 >> Initializing global attention on CLS token...\n",
            " 97% 8084/8340 [52:10<01:11,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:43,737 >> Initializing global attention on CLS token...\n",
            " 97% 8085/8340 [52:10<01:10,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:44,012 >> Initializing global attention on CLS token...\n",
            " 97% 8086/8340 [52:10<01:10,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:44,295 >> Initializing global attention on CLS token...\n",
            " 97% 8087/8340 [52:11<01:10,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:44,573 >> Initializing global attention on CLS token...\n",
            " 97% 8088/8340 [52:11<01:09,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:44,845 >> Initializing global attention on CLS token...\n",
            " 97% 8089/8340 [52:11<01:09,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:45,126 >> Initializing global attention on CLS token...\n",
            " 97% 8090/8340 [52:11<01:09,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:45,402 >> Initializing global attention on CLS token...\n",
            " 97% 8091/8340 [52:12<01:08,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:45,675 >> Initializing global attention on CLS token...\n",
            " 97% 8092/8340 [52:12<01:08,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:45,964 >> Initializing global attention on CLS token...\n",
            " 97% 8093/8340 [52:12<01:09,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:46,252 >> Initializing global attention on CLS token...\n",
            " 97% 8094/8340 [52:13<01:09,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:46,534 >> Initializing global attention on CLS token...\n",
            " 97% 8095/8340 [52:13<01:09,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:46,823 >> Initializing global attention on CLS token...\n",
            " 97% 8096/8340 [52:13<01:09,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:47,097 >> Initializing global attention on CLS token...\n",
            " 97% 8097/8340 [52:13<01:08,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:47,378 >> Initializing global attention on CLS token...\n",
            " 97% 8098/8340 [52:14<01:07,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:47,651 >> Initializing global attention on CLS token...\n",
            " 97% 8099/8340 [52:14<01:07,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:47,930 >> Initializing global attention on CLS token...\n",
            " 97% 8100/8340 [52:14<01:06,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:48,206 >> Initializing global attention on CLS token...\n",
            " 97% 8101/8340 [52:14<01:06,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:48,481 >> Initializing global attention on CLS token...\n",
            " 97% 8102/8340 [52:15<01:06,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:48,759 >> Initializing global attention on CLS token...\n",
            " 97% 8103/8340 [52:15<01:05,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:49,030 >> Initializing global attention on CLS token...\n",
            " 97% 8104/8340 [52:15<01:05,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:49,309 >> Initializing global attention on CLS token...\n",
            " 97% 8105/8340 [52:16<01:04,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:49,581 >> Initializing global attention on CLS token...\n",
            " 97% 8106/8340 [52:16<01:04,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:49,868 >> Initializing global attention on CLS token...\n",
            " 97% 8107/8340 [52:16<01:05,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:50,148 >> Initializing global attention on CLS token...\n",
            " 97% 8108/8340 [52:16<01:04,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:50,427 >> Initializing global attention on CLS token...\n",
            " 97% 8109/8340 [52:17<01:04,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:50,701 >> Initializing global attention on CLS token...\n",
            " 97% 8110/8340 [52:17<01:03,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:50,981 >> Initializing global attention on CLS token...\n",
            " 97% 8111/8340 [52:17<01:03,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:51,261 >> Initializing global attention on CLS token...\n",
            " 97% 8112/8340 [52:18<01:03,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:51,533 >> Initializing global attention on CLS token...\n",
            " 97% 8113/8340 [52:18<01:02,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:51,811 >> Initializing global attention on CLS token...\n",
            " 97% 8114/8340 [52:18<01:02,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:52,090 >> Initializing global attention on CLS token...\n",
            " 97% 8115/8340 [52:18<01:02,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:52,362 >> Initializing global attention on CLS token...\n",
            " 97% 8116/8340 [52:19<01:02,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:52,642 >> Initializing global attention on CLS token...\n",
            " 97% 8117/8340 [52:19<01:01,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:52,916 >> Initializing global attention on CLS token...\n",
            " 97% 8118/8340 [52:19<01:01,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:53,195 >> Initializing global attention on CLS token...\n",
            " 97% 8119/8340 [52:19<01:01,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:53,474 >> Initializing global attention on CLS token...\n",
            " 97% 8120/8340 [52:20<01:00,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:53,748 >> Initializing global attention on CLS token...\n",
            " 97% 8121/8340 [52:20<01:00,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:54,027 >> Initializing global attention on CLS token...\n",
            " 97% 8122/8340 [52:20<01:00,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:54,315 >> Initializing global attention on CLS token...\n",
            " 97% 8123/8340 [52:21<01:00,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:54,597 >> Initializing global attention on CLS token...\n",
            " 97% 8124/8340 [52:21<01:00,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:54,874 >> Initializing global attention on CLS token...\n",
            " 97% 8125/8340 [52:21<00:59,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:55,146 >> Initializing global attention on CLS token...\n",
            " 97% 8126/8340 [52:21<00:59,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:55,431 >> Initializing global attention on CLS token...\n",
            " 97% 8127/8340 [52:22<00:59,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:55,719 >> Initializing global attention on CLS token...\n",
            " 97% 8128/8340 [52:22<00:59,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:56,003 >> Initializing global attention on CLS token...\n",
            " 97% 8129/8340 [52:22<00:59,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:56,282 >> Initializing global attention on CLS token...\n",
            " 97% 8130/8340 [52:23<00:58,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:56,559 >> Initializing global attention on CLS token...\n",
            " 97% 8131/8340 [52:23<00:58,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:56,846 >> Initializing global attention on CLS token...\n",
            " 98% 8132/8340 [52:23<00:58,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:57,124 >> Initializing global attention on CLS token...\n",
            " 98% 8133/8340 [52:23<00:58,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:57,403 >> Initializing global attention on CLS token...\n",
            " 98% 8134/8340 [52:24<00:58,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:57,685 >> Initializing global attention on CLS token...\n",
            " 98% 8135/8340 [52:24<00:57,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:57,972 >> Initializing global attention on CLS token...\n",
            " 98% 8136/8340 [52:24<00:57,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:58,255 >> Initializing global attention on CLS token...\n",
            " 98% 8137/8340 [52:25<00:57,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:58,539 >> Initializing global attention on CLS token...\n",
            " 98% 8138/8340 [52:25<00:57,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:58,821 >> Initializing global attention on CLS token...\n",
            " 98% 8139/8340 [52:25<00:56,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:59,098 >> Initializing global attention on CLS token...\n",
            " 98% 8140/8340 [52:25<00:55,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:59,375 >> Initializing global attention on CLS token...\n",
            " 98% 8141/8340 [52:26<00:55,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:59,669 >> Initializing global attention on CLS token...\n",
            " 98% 8142/8340 [52:26<00:55,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:00:59,956 >> Initializing global attention on CLS token...\n",
            " 98% 8143/8340 [52:26<00:55,  3.52it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:00,229 >> Initializing global attention on CLS token...\n",
            " 98% 8144/8340 [52:26<00:55,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:00,511 >> Initializing global attention on CLS token...\n",
            " 98% 8145/8340 [52:27<00:55,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:00,790 >> Initializing global attention on CLS token...\n",
            " 98% 8146/8340 [52:27<00:54,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:01,072 >> Initializing global attention on CLS token...\n",
            " 98% 8147/8340 [52:27<00:53,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:01,350 >> Initializing global attention on CLS token...\n",
            " 98% 8148/8340 [52:28<00:53,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:01,625 >> Initializing global attention on CLS token...\n",
            " 98% 8149/8340 [52:28<00:53,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:01,902 >> Initializing global attention on CLS token...\n",
            " 98% 8150/8340 [52:28<00:52,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:02,183 >> Initializing global attention on CLS token...\n",
            " 98% 8151/8340 [52:28<00:52,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:02,462 >> Initializing global attention on CLS token...\n",
            " 98% 8152/8340 [52:29<00:52,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:02,739 >> Initializing global attention on CLS token...\n",
            " 98% 8153/8340 [52:29<00:51,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:03,013 >> Initializing global attention on CLS token...\n",
            " 98% 8154/8340 [52:29<00:51,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:03,297 >> Initializing global attention on CLS token...\n",
            " 98% 8155/8340 [52:30<00:51,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:03,580 >> Initializing global attention on CLS token...\n",
            " 98% 8156/8340 [52:30<00:51,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:03,860 >> Initializing global attention on CLS token...\n",
            " 98% 8157/8340 [52:30<00:50,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:04,133 >> Initializing global attention on CLS token...\n",
            " 98% 8158/8340 [52:30<00:50,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:04,414 >> Initializing global attention on CLS token...\n",
            " 98% 8159/8340 [52:31<00:50,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:04,704 >> Initializing global attention on CLS token...\n",
            " 98% 8160/8340 [52:31<00:50,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:04,981 >> Initializing global attention on CLS token...\n",
            " 98% 8161/8340 [52:31<00:50,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:05,263 >> Initializing global attention on CLS token...\n",
            " 98% 8162/8340 [52:32<00:49,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:05,536 >> Initializing global attention on CLS token...\n",
            " 98% 8163/8340 [52:32<00:49,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:05,820 >> Initializing global attention on CLS token...\n",
            " 98% 8164/8340 [52:32<00:49,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:06,099 >> Initializing global attention on CLS token...\n",
            " 98% 8165/8340 [52:32<00:49,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:06,379 >> Initializing global attention on CLS token...\n",
            " 98% 8166/8340 [52:33<00:48,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:06,657 >> Initializing global attention on CLS token...\n",
            " 98% 8167/8340 [52:33<00:48,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:06,935 >> Initializing global attention on CLS token...\n",
            " 98% 8168/8340 [52:33<00:48,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:07,217 >> Initializing global attention on CLS token...\n",
            " 98% 8169/8340 [52:33<00:47,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:07,504 >> Initializing global attention on CLS token...\n",
            " 98% 8170/8340 [52:34<00:48,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:07,785 >> Initializing global attention on CLS token...\n",
            " 98% 8171/8340 [52:34<00:47,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:08,065 >> Initializing global attention on CLS token...\n",
            " 98% 8172/8340 [52:34<00:46,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:08,338 >> Initializing global attention on CLS token...\n",
            " 98% 8173/8340 [52:35<00:46,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:08,628 >> Initializing global attention on CLS token...\n",
            " 98% 8174/8340 [52:35<00:46,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:08,900 >> Initializing global attention on CLS token...\n",
            " 98% 8175/8340 [52:35<00:45,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:09,174 >> Initializing global attention on CLS token...\n",
            " 98% 8176/8340 [52:35<00:45,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:09,453 >> Initializing global attention on CLS token...\n",
            " 98% 8177/8340 [52:36<00:45,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:09,738 >> Initializing global attention on CLS token...\n",
            " 98% 8178/8340 [52:36<00:45,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:10,016 >> Initializing global attention on CLS token...\n",
            " 98% 8179/8340 [52:36<00:44,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:10,291 >> Initializing global attention on CLS token...\n",
            " 98% 8180/8340 [52:37<00:44,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:10,575 >> Initializing global attention on CLS token...\n",
            " 98% 8181/8340 [52:37<00:44,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:10,857 >> Initializing global attention on CLS token...\n",
            " 98% 8182/8340 [52:37<00:44,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:11,142 >> Initializing global attention on CLS token...\n",
            " 98% 8183/8340 [52:37<00:44,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:11,420 >> Initializing global attention on CLS token...\n",
            " 98% 8184/8340 [52:38<00:43,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:11,702 >> Initializing global attention on CLS token...\n",
            " 98% 8185/8340 [52:38<00:43,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:11,974 >> Initializing global attention on CLS token...\n",
            " 98% 8186/8340 [52:38<00:43,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:12,257 >> Initializing global attention on CLS token...\n",
            " 98% 8187/8340 [52:39<00:42,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:12,536 >> Initializing global attention on CLS token...\n",
            " 98% 8188/8340 [52:39<00:42,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:12,810 >> Initializing global attention on CLS token...\n",
            " 98% 8189/8340 [52:39<00:42,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:13,093 >> Initializing global attention on CLS token...\n",
            " 98% 8190/8340 [52:39<00:42,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:13,375 >> Initializing global attention on CLS token...\n",
            " 98% 8191/8340 [52:40<00:41,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:13,656 >> Initializing global attention on CLS token...\n",
            " 98% 8192/8340 [52:40<00:41,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:13,929 >> Initializing global attention on CLS token...\n",
            " 98% 8193/8340 [52:40<00:40,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:14,217 >> Initializing global attention on CLS token...\n",
            " 98% 8194/8340 [52:40<00:41,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:14,494 >> Initializing global attention on CLS token...\n",
            " 98% 8195/8340 [52:41<00:40,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:14,778 >> Initializing global attention on CLS token...\n",
            " 98% 8196/8340 [52:41<00:40,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:15,054 >> Initializing global attention on CLS token...\n",
            " 98% 8197/8340 [52:41<00:39,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:15,328 >> Initializing global attention on CLS token...\n",
            " 98% 8198/8340 [52:42<00:39,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:15,609 >> Initializing global attention on CLS token...\n",
            " 98% 8199/8340 [52:42<00:39,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:15,890 >> Initializing global attention on CLS token...\n",
            " 98% 8200/8340 [52:42<00:38,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:16,162 >> Initializing global attention on CLS token...\n",
            " 98% 8201/8340 [52:42<00:38,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:16,442 >> Initializing global attention on CLS token...\n",
            " 98% 8202/8340 [52:43<00:38,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:16,716 >> Initializing global attention on CLS token...\n",
            " 98% 8203/8340 [52:43<00:37,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:17,001 >> Initializing global attention on CLS token...\n",
            " 98% 8204/8340 [52:43<00:38,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:17,279 >> Initializing global attention on CLS token...\n",
            " 98% 8205/8340 [52:44<00:37,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:17,556 >> Initializing global attention on CLS token...\n",
            " 98% 8206/8340 [52:44<00:37,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:17,832 >> Initializing global attention on CLS token...\n",
            " 98% 8207/8340 [52:44<00:37,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:18,116 >> Initializing global attention on CLS token...\n",
            " 98% 8208/8340 [52:44<00:36,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:18,396 >> Initializing global attention on CLS token...\n",
            " 98% 8209/8340 [52:45<00:36,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:18,673 >> Initializing global attention on CLS token...\n",
            " 98% 8210/8340 [52:45<00:36,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:18,949 >> Initializing global attention on CLS token...\n",
            " 98% 8211/8340 [52:45<00:36,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:19,246 >> Initializing global attention on CLS token...\n",
            " 98% 8212/8340 [52:45<00:36,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:19,527 >> Initializing global attention on CLS token...\n",
            " 98% 8213/8340 [52:46<00:35,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:19,803 >> Initializing global attention on CLS token...\n",
            " 98% 8214/8340 [52:46<00:35,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:20,081 >> Initializing global attention on CLS token...\n",
            " 99% 8215/8340 [52:46<00:34,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:20,353 >> Initializing global attention on CLS token...\n",
            " 99% 8216/8340 [52:47<00:34,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:20,645 >> Initializing global attention on CLS token...\n",
            " 99% 8217/8340 [52:47<00:34,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:20,922 >> Initializing global attention on CLS token...\n",
            " 99% 8218/8340 [52:47<00:34,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:21,198 >> Initializing global attention on CLS token...\n",
            " 99% 8219/8340 [52:47<00:33,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:21,476 >> Initializing global attention on CLS token...\n",
            " 99% 8220/8340 [52:48<00:33,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:21,756 >> Initializing global attention on CLS token...\n",
            " 99% 8221/8340 [52:48<00:33,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:22,034 >> Initializing global attention on CLS token...\n",
            " 99% 8222/8340 [52:48<00:32,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:22,306 >> Initializing global attention on CLS token...\n",
            " 99% 8223/8340 [52:49<00:32,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:22,584 >> Initializing global attention on CLS token...\n",
            " 99% 8224/8340 [52:49<00:32,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:22,869 >> Initializing global attention on CLS token...\n",
            " 99% 8225/8340 [52:49<00:31,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:23,141 >> Initializing global attention on CLS token...\n",
            " 99% 8226/8340 [52:49<00:31,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:23,420 >> Initializing global attention on CLS token...\n",
            " 99% 8227/8340 [52:50<00:31,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:23,694 >> Initializing global attention on CLS token...\n",
            " 99% 8228/8340 [52:50<00:31,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:23,977 >> Initializing global attention on CLS token...\n",
            " 99% 8229/8340 [52:50<00:31,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:24,258 >> Initializing global attention on CLS token...\n",
            " 99% 8230/8340 [52:51<00:30,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:24,539 >> Initializing global attention on CLS token...\n",
            " 99% 8231/8340 [52:51<00:30,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:24,813 >> Initializing global attention on CLS token...\n",
            " 99% 8232/8340 [52:51<00:30,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:25,094 >> Initializing global attention on CLS token...\n",
            " 99% 8233/8340 [52:51<00:29,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:25,382 >> Initializing global attention on CLS token...\n",
            " 99% 8234/8340 [52:52<00:29,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:25,661 >> Initializing global attention on CLS token...\n",
            " 99% 8235/8340 [52:52<00:29,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:25,934 >> Initializing global attention on CLS token...\n",
            " 99% 8236/8340 [52:52<00:29,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:26,218 >> Initializing global attention on CLS token...\n",
            " 99% 8237/8340 [52:52<00:28,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:26,499 >> Initializing global attention on CLS token...\n",
            " 99% 8238/8340 [52:53<00:28,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:26,778 >> Initializing global attention on CLS token...\n",
            " 99% 8239/8340 [52:53<00:28,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:27,052 >> Initializing global attention on CLS token...\n",
            " 99% 8240/8340 [52:53<00:27,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:27,336 >> Initializing global attention on CLS token...\n",
            " 99% 8241/8340 [52:54<00:27,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:27,623 >> Initializing global attention on CLS token...\n",
            " 99% 8242/8340 [52:54<00:27,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:27,913 >> Initializing global attention on CLS token...\n",
            " 99% 8243/8340 [52:54<00:27,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:28,191 >> Initializing global attention on CLS token...\n",
            " 99% 8244/8340 [52:54<00:27,  3.54it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:28,473 >> Initializing global attention on CLS token...\n",
            " 99% 8245/8340 [52:55<00:26,  3.53it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:28,756 >> Initializing global attention on CLS token...\n",
            " 99% 8246/8340 [52:55<00:26,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:29,036 >> Initializing global attention on CLS token...\n",
            " 99% 8247/8340 [52:55<00:26,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:29,307 >> Initializing global attention on CLS token...\n",
            " 99% 8248/8340 [52:56<00:25,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:29,589 >> Initializing global attention on CLS token...\n",
            " 99% 8249/8340 [52:56<00:25,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:29,864 >> Initializing global attention on CLS token...\n",
            " 99% 8250/8340 [52:56<00:25,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:30,147 >> Initializing global attention on CLS token...\n",
            " 99% 8251/8340 [52:56<00:24,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:30,428 >> Initializing global attention on CLS token...\n",
            " 99% 8252/8340 [52:57<00:24,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:30,706 >> Initializing global attention on CLS token...\n",
            " 99% 8253/8340 [52:57<00:24,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:30,979 >> Initializing global attention on CLS token...\n",
            " 99% 8254/8340 [52:57<00:23,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:31,259 >> Initializing global attention on CLS token...\n",
            " 99% 8255/8340 [52:58<00:23,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:31,540 >> Initializing global attention on CLS token...\n",
            " 99% 8256/8340 [52:58<00:23,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:31,814 >> Initializing global attention on CLS token...\n",
            " 99% 8257/8340 [52:58<00:23,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:32,095 >> Initializing global attention on CLS token...\n",
            " 99% 8258/8340 [52:58<00:22,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:32,378 >> Initializing global attention on CLS token...\n",
            " 99% 8259/8340 [52:59<00:22,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:32,656 >> Initializing global attention on CLS token...\n",
            " 99% 8260/8340 [52:59<00:22,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:32,931 >> Initializing global attention on CLS token...\n",
            " 99% 8261/8340 [52:59<00:21,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:33,206 >> Initializing global attention on CLS token...\n",
            " 99% 8262/8340 [52:59<00:21,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:33,488 >> Initializing global attention on CLS token...\n",
            " 99% 8263/8340 [53:00<00:21,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:33,775 >> Initializing global attention on CLS token...\n",
            " 99% 8264/8340 [53:00<00:21,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:34,052 >> Initializing global attention on CLS token...\n",
            " 99% 8265/8340 [53:00<00:21,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:34,334 >> Initializing global attention on CLS token...\n",
            " 99% 8266/8340 [53:01<00:20,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:34,609 >> Initializing global attention on CLS token...\n",
            " 99% 8267/8340 [53:01<00:20,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:34,889 >> Initializing global attention on CLS token...\n",
            " 99% 8268/8340 [53:01<00:20,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:35,171 >> Initializing global attention on CLS token...\n",
            " 99% 8269/8340 [53:01<00:19,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:35,444 >> Initializing global attention on CLS token...\n",
            " 99% 8270/8340 [53:02<00:19,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:35,724 >> Initializing global attention on CLS token...\n",
            " 99% 8271/8340 [53:02<00:19,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:36,004 >> Initializing global attention on CLS token...\n",
            " 99% 8272/8340 [53:02<00:18,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:36,277 >> Initializing global attention on CLS token...\n",
            " 99% 8273/8340 [53:03<00:18,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:36,554 >> Initializing global attention on CLS token...\n",
            " 99% 8274/8340 [53:03<00:18,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:36,833 >> Initializing global attention on CLS token...\n",
            " 99% 8275/8340 [53:03<00:17,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:37,113 >> Initializing global attention on CLS token...\n",
            " 99% 8276/8340 [53:03<00:17,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:37,392 >> Initializing global attention on CLS token...\n",
            " 99% 8277/8340 [53:04<00:17,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:37,670 >> Initializing global attention on CLS token...\n",
            " 99% 8278/8340 [53:04<00:17,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:37,945 >> Initializing global attention on CLS token...\n",
            " 99% 8279/8340 [53:04<00:16,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:38,226 >> Initializing global attention on CLS token...\n",
            " 99% 8280/8340 [53:04<00:16,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:38,512 >> Initializing global attention on CLS token...\n",
            " 99% 8281/8340 [53:05<00:16,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:38,789 >> Initializing global attention on CLS token...\n",
            " 99% 8282/8340 [53:05<00:16,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:39,064 >> Initializing global attention on CLS token...\n",
            " 99% 8283/8340 [53:05<00:15,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:39,339 >> Initializing global attention on CLS token...\n",
            " 99% 8284/8340 [53:06<00:15,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:39,622 >> Initializing global attention on CLS token...\n",
            " 99% 8285/8340 [53:06<00:15,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:39,901 >> Initializing global attention on CLS token...\n",
            " 99% 8286/8340 [53:06<00:14,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:40,173 >> Initializing global attention on CLS token...\n",
            " 99% 8287/8340 [53:06<00:14,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:40,465 >> Initializing global attention on CLS token...\n",
            " 99% 8288/8340 [53:07<00:14,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:40,743 >> Initializing global attention on CLS token...\n",
            " 99% 8289/8340 [53:07<00:14,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:41,025 >> Initializing global attention on CLS token...\n",
            " 99% 8290/8340 [53:07<00:14,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:41,303 >> Initializing global attention on CLS token...\n",
            " 99% 8291/8340 [53:08<00:13,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:41,588 >> Initializing global attention on CLS token...\n",
            " 99% 8292/8340 [53:08<00:13,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:41,864 >> Initializing global attention on CLS token...\n",
            " 99% 8293/8340 [53:08<00:13,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:42,152 >> Initializing global attention on CLS token...\n",
            " 99% 8294/8340 [53:08<00:12,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:42,427 >> Initializing global attention on CLS token...\n",
            " 99% 8295/8340 [53:09<00:12,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:42,703 >> Initializing global attention on CLS token...\n",
            " 99% 8296/8340 [53:09<00:12,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:42,980 >> Initializing global attention on CLS token...\n",
            " 99% 8297/8340 [53:09<00:12,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:43,273 >> Initializing global attention on CLS token...\n",
            " 99% 8298/8340 [53:10<00:11,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:43,545 >> Initializing global attention on CLS token...\n",
            "100% 8299/8340 [53:10<00:11,  3.56it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:43,824 >> Initializing global attention on CLS token...\n",
            "100% 8300/8340 [53:10<00:11,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:44,098 >> Initializing global attention on CLS token...\n",
            "100% 8301/8340 [53:10<00:10,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:44,378 >> Initializing global attention on CLS token...\n",
            "100% 8302/8340 [53:11<00:10,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:44,655 >> Initializing global attention on CLS token...\n",
            "100% 8303/8340 [53:11<00:10,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:44,927 >> Initializing global attention on CLS token...\n",
            "100% 8304/8340 [53:11<00:09,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:45,205 >> Initializing global attention on CLS token...\n",
            "100% 8305/8340 [53:11<00:09,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:45,489 >> Initializing global attention on CLS token...\n",
            "100% 8306/8340 [53:12<00:09,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:45,763 >> Initializing global attention on CLS token...\n",
            "100% 8307/8340 [53:12<00:09,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:46,043 >> Initializing global attention on CLS token...\n",
            "100% 8308/8340 [53:12<00:08,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:46,320 >> Initializing global attention on CLS token...\n",
            "100% 8309/8340 [53:13<00:08,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:46,597 >> Initializing global attention on CLS token...\n",
            "100% 8310/8340 [53:13<00:08,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:46,884 >> Initializing global attention on CLS token...\n",
            "100% 8311/8340 [53:13<00:08,  3.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:47,165 >> Initializing global attention on CLS token...\n",
            "100% 8312/8340 [53:13<00:07,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:47,443 >> Initializing global attention on CLS token...\n",
            "100% 8313/8340 [53:14<00:07,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:47,717 >> Initializing global attention on CLS token...\n",
            "100% 8314/8340 [53:14<00:07,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:47,998 >> Initializing global attention on CLS token...\n",
            "100% 8315/8340 [53:14<00:06,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:48,273 >> Initializing global attention on CLS token...\n",
            "100% 8316/8340 [53:15<00:06,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:48,551 >> Initializing global attention on CLS token...\n",
            "100% 8317/8340 [53:15<00:06,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:48,832 >> Initializing global attention on CLS token...\n",
            "100% 8318/8340 [53:15<00:06,  3.57it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:49,113 >> Initializing global attention on CLS token...\n",
            "100% 8319/8340 [53:15<00:05,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:49,385 >> Initializing global attention on CLS token...\n",
            "100% 8320/8340 [53:16<00:05,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:49,669 >> Initializing global attention on CLS token...\n",
            "100% 8321/8340 [53:16<00:05,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:49,948 >> Initializing global attention on CLS token...\n",
            "100% 8322/8340 [53:16<00:05,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:50,232 >> Initializing global attention on CLS token...\n",
            "100% 8323/8340 [53:16<00:04,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:50,505 >> Initializing global attention on CLS token...\n",
            "100% 8324/8340 [53:17<00:04,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:50,783 >> Initializing global attention on CLS token...\n",
            "100% 8325/8340 [53:17<00:04,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:51,064 >> Initializing global attention on CLS token...\n",
            "100% 8326/8340 [53:17<00:03,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:51,343 >> Initializing global attention on CLS token...\n",
            "100% 8327/8340 [53:18<00:03,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:51,618 >> Initializing global attention on CLS token...\n",
            "100% 8328/8340 [53:18<00:03,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:51,898 >> Initializing global attention on CLS token...\n",
            "100% 8329/8340 [53:18<00:03,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:52,172 >> Initializing global attention on CLS token...\n",
            "100% 8330/8340 [53:18<00:02,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:52,453 >> Initializing global attention on CLS token...\n",
            "100% 8331/8340 [53:19<00:02,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:52,734 >> Initializing global attention on CLS token...\n",
            "100% 8332/8340 [53:19<00:02,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:53,007 >> Initializing global attention on CLS token...\n",
            "100% 8333/8340 [53:19<00:01,  3.59it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:53,287 >> Initializing global attention on CLS token...\n",
            "100% 8334/8340 [53:20<00:01,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:53,565 >> Initializing global attention on CLS token...\n",
            "100% 8335/8340 [53:20<00:01,  3.62it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:53,838 >> Initializing global attention on CLS token...\n",
            "100% 8336/8340 [53:20<00:01,  3.61it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:54,114 >> Initializing global attention on CLS token...\n",
            "100% 8337/8340 [53:20<00:00,  3.63it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:54,389 >> Initializing global attention on CLS token...\n",
            "100% 8338/8340 [53:21<00:00,  3.60it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:54,671 >> Initializing global attention on CLS token...\n",
            "100% 8339/8340 [53:21<00:00,  3.58it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:01:54,933 >> Initializing global attention on CLS token...\n",
            "100% 8340/8340 [53:21<00:00,  4.43it/s][INFO|trainer.py:726] 2022-11-24 18:01:55,024 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2022-11-24 18:01:55,026 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-11-24 18:01:55,026 >>   Num examples = 1400\n",
            "[INFO|trainer.py:2912] 2022-11-24 18:01:55,027 >>   Batch size = 6\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:55,058 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 0/234 [00:00<?, ?it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:55,322 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 2/234 [00:00<00:30,  7.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:55,590 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 3/234 [00:00<00:44,  5.24it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:55,863 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 4/234 [00:00<00:50,  4.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:56,127 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 5/234 [00:01<00:53,  4.26it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:56,391 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 6/234 [00:01<00:55,  4.09it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:56,657 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 7/234 [00:01<00:57,  3.98it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:56,924 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 8/234 [00:01<00:57,  3.92it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:57,189 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 9/234 [00:02<00:58,  3.87it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:57,451 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 10/234 [00:02<00:58,  3.85it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:57,719 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 11/234 [00:02<00:58,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:57,987 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 12/234 [00:02<00:59,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:58,262 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 13/234 [00:03<00:58,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:58,527 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 14/234 [00:03<00:58,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:58,795 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 15/234 [00:03<00:58,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:59,060 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 16/234 [00:03<00:57,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:59,328 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 17/234 [00:04<00:58,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:59,607 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 18/234 [00:04<00:58,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:01:59,872 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 19/234 [00:04<00:57,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:00,137 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 20/234 [00:05<00:57,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:00,403 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 21/234 [00:05<00:57,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:00,670 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 22/234 [00:05<00:56,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:00,943 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 23/234 [00:05<00:56,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:01,209 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 24/234 [00:06<00:56,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:01,477 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 25/234 [00:06<00:56,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:01,744 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 26/234 [00:06<00:55,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:02,009 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 27/234 [00:06<00:55,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:02,280 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 28/234 [00:07<00:55,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:02,553 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 29/234 [00:07<00:54,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:02,815 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 30/234 [00:07<00:54,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:03,078 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 31/234 [00:08<00:53,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:03,345 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 32/234 [00:08<00:53,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:03,615 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 33/234 [00:08<00:54,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:03,886 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 34/234 [00:08<00:53,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:04,154 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 35/234 [00:09<00:53,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:04,415 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 36/234 [00:09<00:52,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:04,681 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 37/234 [00:09<00:52,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:04,948 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 38/234 [00:09<00:52,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:05,211 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 39/234 [00:10<00:51,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:05,476 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 40/234 [00:10<00:51,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:05,738 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 41/234 [00:10<00:50,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:06,013 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 42/234 [00:10<00:51,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:06,299 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 43/234 [00:11<00:52,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:06,572 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 44/234 [00:11<00:51,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:06,835 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 45/234 [00:11<00:50,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:07,104 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 46/234 [00:12<00:50,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:07,375 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 47/234 [00:12<00:50,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:07,645 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 48/234 [00:12<00:50,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:07,908 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 49/234 [00:12<00:49,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:08,174 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 50/234 [00:13<00:49,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:08,443 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 51/234 [00:13<00:48,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:08,706 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 52/234 [00:13<00:48,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:08,968 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 53/234 [00:13<00:47,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:09,235 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 54/234 [00:14<00:47,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:09,502 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 55/234 [00:14<00:47,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:09,768 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 56/234 [00:14<00:47,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:10,038 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 57/234 [00:14<00:47,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:10,311 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 58/234 [00:15<00:47,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:10,572 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 59/234 [00:15<00:46,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:10,839 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 60/234 [00:15<00:46,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:11,104 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 61/234 [00:16<00:46,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:11,369 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 62/234 [00:16<00:45,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:11,643 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 63/234 [00:16<00:46,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:11,910 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 64/234 [00:16<00:45,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:12,177 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 65/234 [00:17<00:45,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:12,441 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 66/234 [00:17<00:44,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:12,702 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 67/234 [00:17<00:44,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:12,972 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 68/234 [00:17<00:44,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:13,238 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 69/234 [00:18<00:43,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:13,498 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 70/234 [00:18<00:43,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:13,781 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 71/234 [00:18<00:44,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:14,050 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 72/234 [00:18<00:43,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:14,315 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 73/234 [00:19<00:43,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:14,584 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 74/234 [00:19<00:43,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:14,854 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 75/234 [00:19<00:42,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:15,120 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 76/234 [00:20<00:42,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:15,386 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 77/234 [00:20<00:41,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:15,650 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 78/234 [00:20<00:41,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:15,915 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 79/234 [00:20<00:41,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:16,179 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 80/234 [00:21<00:40,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:16,443 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 81/234 [00:21<00:40,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:16,707 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 82/234 [00:21<00:40,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:16,976 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 83/234 [00:21<00:40,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:17,242 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 84/234 [00:22<00:39,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:17,508 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 85/234 [00:22<00:39,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:17,781 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 86/234 [00:22<00:39,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:18,046 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 87/234 [00:22<00:39,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:18,309 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 88/234 [00:23<00:38,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:18,575 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 89/234 [00:23<00:38,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:18,841 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 90/234 [00:23<00:38,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:19,109 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 91/234 [00:24<00:38,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:19,383 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 92/234 [00:24<00:38,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:19,654 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 93/234 [00:24<00:37,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:19,918 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 94/234 [00:24<00:37,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:20,193 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 95/234 [00:25<00:37,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:20,462 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 96/234 [00:25<00:37,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:20,744 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 97/234 [00:25<00:37,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:21,027 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 98/234 [00:25<00:37,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:21,306 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 99/234 [00:26<00:37,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:21,596 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 100/234 [00:26<00:37,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:21,882 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 101/234 [00:26<00:37,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:22,167 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 102/234 [00:27<00:37,  3.54it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:22,452 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 103/234 [00:27<00:37,  3.53it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:22,719 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 104/234 [00:27<00:36,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:22,983 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 105/234 [00:27<00:35,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:23,265 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 106/234 [00:28<00:35,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:23,531 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 107/234 [00:28<00:34,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:23,807 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 108/234 [00:28<00:34,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:24,083 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 109/234 [00:29<00:34,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:24,352 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 110/234 [00:29<00:33,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:24,623 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 111/234 [00:29<00:33,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:24,896 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 112/234 [00:29<00:33,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:25,161 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 113/234 [00:30<00:32,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:25,426 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 114/234 [00:30<00:32,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:25,701 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 115/234 [00:30<00:32,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:25,978 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 116/234 [00:30<00:32,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:26,246 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 117/234 [00:31<00:31,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:26,518 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 118/234 [00:31<00:31,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:26,786 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 119/234 [00:31<00:30,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:27,052 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 120/234 [00:31<00:30,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:27,319 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 121/234 [00:32<00:30,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:27,586 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 122/234 [00:32<00:30,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:27,856 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 123/234 [00:32<00:29,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:28,133 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 124/234 [00:33<00:29,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:28,403 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 125/234 [00:33<00:29,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:28,675 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 126/234 [00:33<00:29,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:28,942 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 127/234 [00:33<00:28,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:29,208 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 128/234 [00:34<00:28,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:29,478 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 129/234 [00:34<00:28,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:29,751 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 130/234 [00:34<00:28,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:30,030 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 131/234 [00:34<00:28,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:30,299 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 132/234 [00:35<00:27,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:30,568 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 133/234 [00:35<00:27,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:30,842 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 134/234 [00:35<00:27,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:31,115 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 135/234 [00:36<00:26,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:31,383 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 136/234 [00:36<00:26,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:31,662 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 137/234 [00:36<00:26,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:31,932 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 138/234 [00:36<00:26,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:32,194 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 139/234 [00:37<00:25,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:32,459 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 140/234 [00:37<00:25,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:32,725 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 141/234 [00:37<00:24,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:32,990 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 142/234 [00:37<00:24,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:33,263 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 143/234 [00:38<00:24,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:33,534 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 144/234 [00:38<00:24,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:33,804 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 145/234 [00:38<00:23,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:34,067 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 146/234 [00:39<00:23,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:34,329 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 147/234 [00:39<00:23,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:34,595 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 148/234 [00:39<00:22,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:34,871 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 149/234 [00:39<00:23,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:35,147 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 150/234 [00:40<00:22,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:35,409 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 151/234 [00:40<00:22,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:35,676 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 152/234 [00:40<00:22,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:35,947 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 153/234 [00:40<00:21,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:36,218 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 154/234 [00:41<00:21,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:36,488 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 155/234 [00:41<00:21,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:36,752 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 156/234 [00:41<00:20,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:37,024 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 157/234 [00:41<00:20,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:37,295 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 158/234 [00:42<00:20,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:37,564 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 159/234 [00:42<00:20,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:37,829 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 160/234 [00:42<00:19,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:38,097 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 161/234 [00:43<00:19,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:38,364 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 162/234 [00:43<00:19,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:38,631 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 163/234 [00:43<00:19,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:38,907 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 164/234 [00:43<00:18,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:39,171 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 165/234 [00:44<00:18,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:39,447 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 166/234 [00:44<00:18,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:39,722 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 167/234 [00:44<00:18,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:39,986 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 168/234 [00:44<00:17,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:40,254 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 169/234 [00:45<00:17,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:40,522 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 170/234 [00:45<00:17,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:40,786 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 171/234 [00:45<00:16,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:41,053 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 172/234 [00:45<00:16,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:41,325 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 173/234 [00:46<00:16,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:41,592 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 174/234 [00:46<00:16,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:41,859 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 175/234 [00:46<00:15,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:42,126 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 176/234 [00:47<00:15,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:42,392 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 177/234 [00:47<00:15,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:42,659 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 178/234 [00:47<00:14,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:42,929 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 179/234 [00:47<00:14,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:43,199 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 180/234 [00:48<00:14,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:43,463 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 181/234 [00:48<00:14,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:43,732 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 182/234 [00:48<00:13,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:43,999 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 183/234 [00:48<00:13,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:44,276 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 184/234 [00:49<00:13,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:44,543 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 185/234 [00:49<00:13,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:44,808 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 186/234 [00:49<00:12,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:45,075 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 187/234 [00:50<00:12,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:45,344 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 188/234 [00:50<00:12,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:45,614 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 189/234 [00:50<00:12,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:45,883 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 190/234 [00:50<00:11,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:46,145 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 191/234 [00:51<00:11,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:46,415 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 192/234 [00:51<00:11,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:46,682 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 193/234 [00:51<00:10,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:46,947 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 194/234 [00:51<00:10,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:47,220 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 195/234 [00:52<00:10,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:47,498 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 196/234 [00:52<00:10,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:47,769 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 197/234 [00:52<00:10,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:48,041 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 198/234 [00:52<00:09,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:48,314 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 199/234 [00:53<00:09,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:48,585 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 200/234 [00:53<00:09,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:48,850 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 201/234 [00:53<00:08,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:49,118 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 202/234 [00:54<00:08,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:49,384 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 203/234 [00:54<00:08,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:49,651 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 204/234 [00:54<00:08,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:49,921 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 205/234 [00:54<00:07,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:50,191 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 206/234 [00:55<00:07,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:50,455 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 207/234 [00:55<00:07,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:50,722 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 208/234 [00:55<00:06,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:50,986 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 209/234 [00:55<00:06,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:51,254 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 210/234 [00:56<00:06,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:51,520 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 211/234 [00:56<00:06,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:51,790 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 212/234 [00:56<00:05,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:52,065 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 213/234 [00:57<00:05,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:52,328 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 214/234 [00:57<00:05,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:52,592 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 215/234 [00:57<00:05,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:52,859 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 216/234 [00:57<00:04,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:53,133 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 217/234 [00:58<00:04,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:53,402 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 218/234 [00:58<00:04,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:53,674 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 219/234 [00:58<00:04,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:53,949 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 220/234 [00:58<00:03,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:54,211 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 221/234 [00:59<00:03,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:54,484 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 222/234 [00:59<00:03,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:54,755 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 223/234 [00:59<00:02,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:55,025 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 224/234 [00:59<00:02,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:55,293 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 225/234 [01:00<00:02,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:55,564 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 226/234 [01:00<00:02,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:55,835 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 227/234 [01:00<00:01,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:56,101 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 228/234 [01:01<00:01,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:56,366 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 229/234 [01:01<00:01,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:56,628 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 230/234 [01:01<00:01,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:56,893 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 231/234 [01:01<00:00,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:57,155 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 232/234 [01:02<00:00,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:57,417 >> Initializing global attention on CLS token...\n",
            "\n",
            "100% 233/234 [01:02<00:00,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-24 18:02:57,663 >> Initializing global attention on CLS token...\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.234939694404602, 'eval_f1-micro': 0.6614285714285715, 'eval_f1-macro': 0.5133117798029444, 'eval_runtime': 63.9624, 'eval_samples_per_second': 21.888, 'eval_steps_per_second': 3.658, 'epoch': 10.0}\n",
            "100% 8340/8340 [54:25<00:00,  4.43it/s]\n",
            "100% 234/234 [01:03<00:00,  3.78it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-11-24 18:02:58,991 >> Saving model checkpoint to logs/output_1/checkpoint-8340\n",
            "[INFO|configuration_utils.py:447] 2022-11-24 18:02:58,992 >> Configuration saved in logs/output_1/checkpoint-8340/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-24 18:02:59,372 >> Model weights saved in logs/output_1/checkpoint-8340/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-24 18:02:59,373 >> tokenizer config file saved in logs/output_1/checkpoint-8340/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-24 18:02:59,373 >> Special tokens file saved in logs/output_1/checkpoint-8340/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-24 18:03:01,615 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-24 18:03:01,616 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:1852] 2022-11-24 18:03:30,576 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1946] 2022-11-24 18:03:30,576 >> Loading best model from logs/output_1/checkpoint-8340 (score: 0.6614285714285715).\n",
            "{'train_runtime': 3297.2276, 'train_samples_per_second': 15.164, 'train_steps_per_second': 2.529, 'train_loss': 0.851858697978141, 'epoch': 10.0}\n",
            "100% 8340/8340 [54:57<00:00,  2.53it/s]\n",
            "[INFO|trainer.py:726] 2022-11-24 18:03:30,723 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2022-11-24 18:03:30,725 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-11-24 18:03:30,726 >>   Num examples = 1400\n",
            "[INFO|trainer.py:2912] 2022-11-24 18:03:30,726 >>   Batch size = 6\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-24 18:03:30,758 >> Initializing global attention on CLS token...\n",
            "  0% 0/234 [00:00<?, ?it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:31,071 >> Initializing global attention on CLS token...\n",
            "  1% 2/234 [00:00<00:30,  7.55it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:31,336 >> Initializing global attention on CLS token...\n",
            "  1% 3/234 [00:00<00:42,  5.40it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:31,597 >> Initializing global attention on CLS token...\n",
            "  2% 4/234 [00:00<00:49,  4.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:31,855 >> Initializing global attention on CLS token...\n",
            "  2% 5/234 [00:01<00:52,  4.37it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:32,122 >> Initializing global attention on CLS token...\n",
            "  3% 6/234 [00:01<00:54,  4.15it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:32,382 >> Initializing global attention on CLS token...\n",
            "  3% 7/234 [00:01<00:56,  4.03it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:32,644 >> Initializing global attention on CLS token...\n",
            "  3% 8/234 [00:01<00:57,  3.95it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:32,914 >> Initializing global attention on CLS token...\n",
            "  4% 9/234 [00:02<00:57,  3.89it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:33,173 >> Initializing global attention on CLS token...\n",
            "  4% 10/234 [00:02<00:58,  3.86it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:33,437 >> Initializing global attention on CLS token...\n",
            "  5% 11/234 [00:02<00:58,  3.84it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:33,702 >> Initializing global attention on CLS token...\n",
            "  5% 12/234 [00:02<00:58,  3.81it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:33,968 >> Initializing global attention on CLS token...\n",
            "  6% 13/234 [00:03<00:58,  3.81it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:34,232 >> Initializing global attention on CLS token...\n",
            "  6% 14/234 [00:03<00:57,  3.80it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:34,496 >> Initializing global attention on CLS token...\n",
            "  6% 15/234 [00:03<00:57,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:34,761 >> Initializing global attention on CLS token...\n",
            "  7% 16/234 [00:03<00:57,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:35,036 >> Initializing global attention on CLS token...\n",
            "  7% 17/234 [00:04<00:57,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:35,301 >> Initializing global attention on CLS token...\n",
            "  8% 18/234 [00:04<00:57,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:35,568 >> Initializing global attention on CLS token...\n",
            "  8% 19/234 [00:04<00:57,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:35,842 >> Initializing global attention on CLS token...\n",
            "  9% 20/234 [00:05<00:57,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:36,105 >> Initializing global attention on CLS token...\n",
            "  9% 21/234 [00:05<00:56,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:36,373 >> Initializing global attention on CLS token...\n",
            "  9% 22/234 [00:05<00:56,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:36,640 >> Initializing global attention on CLS token...\n",
            " 10% 23/234 [00:05<00:56,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:36,905 >> Initializing global attention on CLS token...\n",
            " 10% 24/234 [00:06<00:55,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:37,169 >> Initializing global attention on CLS token...\n",
            " 11% 25/234 [00:06<00:55,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:37,434 >> Initializing global attention on CLS token...\n",
            " 11% 26/234 [00:06<00:55,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:37,694 >> Initializing global attention on CLS token...\n",
            " 12% 27/234 [00:06<00:54,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:37,960 >> Initializing global attention on CLS token...\n",
            " 12% 28/234 [00:07<00:54,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:38,222 >> Initializing global attention on CLS token...\n",
            " 12% 29/234 [00:07<00:54,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:38,494 >> Initializing global attention on CLS token...\n",
            " 13% 30/234 [00:07<00:54,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:38,777 >> Initializing global attention on CLS token...\n",
            " 13% 31/234 [00:07<00:55,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:39,042 >> Initializing global attention on CLS token...\n",
            " 14% 32/234 [00:08<00:54,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:39,308 >> Initializing global attention on CLS token...\n",
            " 14% 33/234 [00:08<00:53,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:39,575 >> Initializing global attention on CLS token...\n",
            " 15% 34/234 [00:08<00:54,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:39,845 >> Initializing global attention on CLS token...\n",
            " 15% 35/234 [00:09<00:53,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:40,112 >> Initializing global attention on CLS token...\n",
            " 15% 36/234 [00:09<00:52,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:40,385 >> Initializing global attention on CLS token...\n",
            " 16% 37/234 [00:09<00:52,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:40,647 >> Initializing global attention on CLS token...\n",
            " 16% 38/234 [00:09<00:52,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:40,925 >> Initializing global attention on CLS token...\n",
            " 17% 39/234 [00:10<00:52,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:41,192 >> Initializing global attention on CLS token...\n",
            " 17% 40/234 [00:10<00:52,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:41,455 >> Initializing global attention on CLS token...\n",
            " 18% 41/234 [00:10<00:51,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:41,723 >> Initializing global attention on CLS token...\n",
            " 18% 42/234 [00:10<00:51,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:41,987 >> Initializing global attention on CLS token...\n",
            " 18% 43/234 [00:11<00:50,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:42,254 >> Initializing global attention on CLS token...\n",
            " 19% 44/234 [00:11<00:50,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:42,534 >> Initializing global attention on CLS token...\n",
            " 19% 45/234 [00:11<00:51,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:42,802 >> Initializing global attention on CLS token...\n",
            " 20% 46/234 [00:11<00:50,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:43,065 >> Initializing global attention on CLS token...\n",
            " 20% 47/234 [00:12<00:50,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:43,328 >> Initializing global attention on CLS token...\n",
            " 21% 48/234 [00:12<00:49,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:43,597 >> Initializing global attention on CLS token...\n",
            " 21% 49/234 [00:12<00:49,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:43,863 >> Initializing global attention on CLS token...\n",
            " 21% 50/234 [00:13<00:48,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:44,126 >> Initializing global attention on CLS token...\n",
            " 22% 51/234 [00:13<00:48,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:44,387 >> Initializing global attention on CLS token...\n",
            " 22% 52/234 [00:13<00:48,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:44,663 >> Initializing global attention on CLS token...\n",
            " 23% 53/234 [00:13<00:48,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:44,945 >> Initializing global attention on CLS token...\n",
            " 23% 54/234 [00:14<00:48,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:45,207 >> Initializing global attention on CLS token...\n",
            " 24% 55/234 [00:14<00:47,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:45,472 >> Initializing global attention on CLS token...\n",
            " 24% 56/234 [00:14<00:47,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:45,738 >> Initializing global attention on CLS token...\n",
            " 24% 57/234 [00:14<00:47,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:46,002 >> Initializing global attention on CLS token...\n",
            " 25% 58/234 [00:15<00:46,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:46,267 >> Initializing global attention on CLS token...\n",
            " 25% 59/234 [00:15<00:46,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:46,533 >> Initializing global attention on CLS token...\n",
            " 26% 60/234 [00:15<00:46,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:46,801 >> Initializing global attention on CLS token...\n",
            " 26% 61/234 [00:15<00:46,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:47,067 >> Initializing global attention on CLS token...\n",
            " 26% 62/234 [00:16<00:45,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:47,338 >> Initializing global attention on CLS token...\n",
            " 27% 63/234 [00:16<00:45,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:47,607 >> Initializing global attention on CLS token...\n",
            " 27% 64/234 [00:16<00:45,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:47,877 >> Initializing global attention on CLS token...\n",
            " 28% 65/234 [00:17<00:45,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:48,136 >> Initializing global attention on CLS token...\n",
            " 28% 66/234 [00:17<00:44,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:48,414 >> Initializing global attention on CLS token...\n",
            " 29% 67/234 [00:17<00:45,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:48,696 >> Initializing global attention on CLS token...\n",
            " 29% 68/234 [00:17<00:45,  3.64it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:48,964 >> Initializing global attention on CLS token...\n",
            " 29% 69/234 [00:18<00:44,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:49,230 >> Initializing global attention on CLS token...\n",
            " 30% 70/234 [00:18<00:44,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:49,498 >> Initializing global attention on CLS token...\n",
            " 30% 71/234 [00:18<00:43,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:49,763 >> Initializing global attention on CLS token...\n",
            " 31% 72/234 [00:18<00:43,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:50,030 >> Initializing global attention on CLS token...\n",
            " 31% 73/234 [00:19<00:43,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:50,295 >> Initializing global attention on CLS token...\n",
            " 32% 74/234 [00:19<00:42,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:50,558 >> Initializing global attention on CLS token...\n",
            " 32% 75/234 [00:19<00:42,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:50,827 >> Initializing global attention on CLS token...\n",
            " 32% 76/234 [00:20<00:42,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:51,095 >> Initializing global attention on CLS token...\n",
            " 33% 77/234 [00:20<00:42,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:51,365 >> Initializing global attention on CLS token...\n",
            " 33% 78/234 [00:20<00:41,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:51,634 >> Initializing global attention on CLS token...\n",
            " 34% 79/234 [00:20<00:41,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:51,901 >> Initializing global attention on CLS token...\n",
            " 34% 80/234 [00:21<00:41,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:52,160 >> Initializing global attention on CLS token...\n",
            " 35% 81/234 [00:21<00:40,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:52,425 >> Initializing global attention on CLS token...\n",
            " 35% 82/234 [00:21<00:40,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:52,687 >> Initializing global attention on CLS token...\n",
            " 35% 83/234 [00:21<00:39,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:52,952 >> Initializing global attention on CLS token...\n",
            " 36% 84/234 [00:22<00:39,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:53,219 >> Initializing global attention on CLS token...\n",
            " 36% 85/234 [00:22<00:39,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:53,484 >> Initializing global attention on CLS token...\n",
            " 37% 86/234 [00:22<00:39,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:53,749 >> Initializing global attention on CLS token...\n",
            " 37% 87/234 [00:22<00:38,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:54,011 >> Initializing global attention on CLS token...\n",
            " 38% 88/234 [00:23<00:38,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:54,275 >> Initializing global attention on CLS token...\n",
            " 38% 89/234 [00:23<00:38,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:54,541 >> Initializing global attention on CLS token...\n",
            " 38% 90/234 [00:23<00:38,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:54,802 >> Initializing global attention on CLS token...\n",
            " 39% 91/234 [00:23<00:37,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:55,071 >> Initializing global attention on CLS token...\n",
            " 39% 92/234 [00:24<00:37,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:55,332 >> Initializing global attention on CLS token...\n",
            " 40% 93/234 [00:24<00:37,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:55,601 >> Initializing global attention on CLS token...\n",
            " 40% 94/234 [00:24<00:37,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:55,863 >> Initializing global attention on CLS token...\n",
            " 41% 95/234 [00:25<00:36,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:56,126 >> Initializing global attention on CLS token...\n",
            " 41% 96/234 [00:25<00:36,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:56,390 >> Initializing global attention on CLS token...\n",
            " 41% 97/234 [00:25<00:36,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:56,655 >> Initializing global attention on CLS token...\n",
            " 42% 98/234 [00:25<00:35,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:56,917 >> Initializing global attention on CLS token...\n",
            " 42% 99/234 [00:26<00:35,  3.80it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:57,183 >> Initializing global attention on CLS token...\n",
            " 43% 100/234 [00:26<00:35,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:57,446 >> Initializing global attention on CLS token...\n",
            " 43% 101/234 [00:26<00:35,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:57,710 >> Initializing global attention on CLS token...\n",
            " 44% 102/234 [00:26<00:34,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:57,981 >> Initializing global attention on CLS token...\n",
            " 44% 103/234 [00:27<00:35,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:58,251 >> Initializing global attention on CLS token...\n",
            " 44% 104/234 [00:27<00:34,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:58,517 >> Initializing global attention on CLS token...\n",
            " 45% 105/234 [00:27<00:34,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:58,786 >> Initializing global attention on CLS token...\n",
            " 45% 106/234 [00:27<00:34,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:59,050 >> Initializing global attention on CLS token...\n",
            " 46% 107/234 [00:28<00:33,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:59,321 >> Initializing global attention on CLS token...\n",
            " 46% 108/234 [00:28<00:33,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:59,594 >> Initializing global attention on CLS token...\n",
            " 47% 109/234 [00:28<00:33,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:03:59,867 >> Initializing global attention on CLS token...\n",
            " 47% 110/234 [00:29<00:33,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:00,132 >> Initializing global attention on CLS token...\n",
            " 47% 111/234 [00:29<00:33,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:00,401 >> Initializing global attention on CLS token...\n",
            " 48% 112/234 [00:29<00:32,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:00,675 >> Initializing global attention on CLS token...\n",
            " 48% 113/234 [00:29<00:32,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:00,940 >> Initializing global attention on CLS token...\n",
            " 49% 114/234 [00:30<00:32,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:01,215 >> Initializing global attention on CLS token...\n",
            " 49% 115/234 [00:30<00:32,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:01,490 >> Initializing global attention on CLS token...\n",
            " 50% 116/234 [00:30<00:31,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:01,756 >> Initializing global attention on CLS token...\n",
            " 50% 117/234 [00:30<00:31,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:02,025 >> Initializing global attention on CLS token...\n",
            " 50% 118/234 [00:31<00:31,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:02,285 >> Initializing global attention on CLS token...\n",
            " 51% 119/234 [00:31<00:30,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:02,549 >> Initializing global attention on CLS token...\n",
            " 51% 120/234 [00:31<00:30,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:02,818 >> Initializing global attention on CLS token...\n",
            " 52% 121/234 [00:32<00:30,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:03,080 >> Initializing global attention on CLS token...\n",
            " 52% 122/234 [00:32<00:29,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:03,343 >> Initializing global attention on CLS token...\n",
            " 53% 123/234 [00:32<00:29,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:03,604 >> Initializing global attention on CLS token...\n",
            " 53% 124/234 [00:32<00:28,  3.80it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:03,865 >> Initializing global attention on CLS token...\n",
            " 53% 125/234 [00:33<00:28,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:04,131 >> Initializing global attention on CLS token...\n",
            " 54% 126/234 [00:33<00:28,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:04,396 >> Initializing global attention on CLS token...\n",
            " 54% 127/234 [00:33<00:28,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:04,663 >> Initializing global attention on CLS token...\n",
            " 55% 128/234 [00:33<00:28,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:04,928 >> Initializing global attention on CLS token...\n",
            " 55% 129/234 [00:34<00:27,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:05,192 >> Initializing global attention on CLS token...\n",
            " 56% 130/234 [00:34<00:27,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:05,466 >> Initializing global attention on CLS token...\n",
            " 56% 131/234 [00:34<00:27,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:05,736 >> Initializing global attention on CLS token...\n",
            " 56% 132/234 [00:34<00:27,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:06,000 >> Initializing global attention on CLS token...\n",
            " 57% 133/234 [00:35<00:26,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:06,270 >> Initializing global attention on CLS token...\n",
            " 57% 134/234 [00:35<00:26,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:06,534 >> Initializing global attention on CLS token...\n",
            " 58% 135/234 [00:35<00:26,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:06,799 >> Initializing global attention on CLS token...\n",
            " 58% 136/234 [00:35<00:26,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:07,064 >> Initializing global attention on CLS token...\n",
            " 59% 137/234 [00:36<00:25,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:07,327 >> Initializing global attention on CLS token...\n",
            " 59% 138/234 [00:36<00:25,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:07,591 >> Initializing global attention on CLS token...\n",
            " 59% 139/234 [00:36<00:25,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:07,853 >> Initializing global attention on CLS token...\n",
            " 60% 140/234 [00:37<00:24,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:08,117 >> Initializing global attention on CLS token...\n",
            " 60% 141/234 [00:37<00:24,  3.79it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:08,391 >> Initializing global attention on CLS token...\n",
            " 61% 142/234 [00:37<00:24,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:08,659 >> Initializing global attention on CLS token...\n",
            " 61% 143/234 [00:37<00:24,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:08,936 >> Initializing global attention on CLS token...\n",
            " 62% 144/234 [00:38<00:24,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:09,199 >> Initializing global attention on CLS token...\n",
            " 62% 145/234 [00:38<00:23,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:09,461 >> Initializing global attention on CLS token...\n",
            " 62% 146/234 [00:38<00:23,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:09,724 >> Initializing global attention on CLS token...\n",
            " 63% 147/234 [00:38<00:23,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:09,987 >> Initializing global attention on CLS token...\n",
            " 63% 148/234 [00:39<00:22,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:10,251 >> Initializing global attention on CLS token...\n",
            " 64% 149/234 [00:39<00:22,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:10,533 >> Initializing global attention on CLS token...\n",
            " 64% 150/234 [00:39<00:22,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:10,808 >> Initializing global attention on CLS token...\n",
            " 65% 151/234 [00:40<00:22,  3.67it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:11,077 >> Initializing global attention on CLS token...\n",
            " 65% 152/234 [00:40<00:22,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:11,353 >> Initializing global attention on CLS token...\n",
            " 65% 153/234 [00:40<00:21,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:11,618 >> Initializing global attention on CLS token...\n",
            " 66% 154/234 [00:40<00:21,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:11,884 >> Initializing global attention on CLS token...\n",
            " 66% 155/234 [00:41<00:21,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:12,149 >> Initializing global attention on CLS token...\n",
            " 67% 156/234 [00:41<00:20,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:12,421 >> Initializing global attention on CLS token...\n",
            " 67% 157/234 [00:41<00:20,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:12,689 >> Initializing global attention on CLS token...\n",
            " 68% 158/234 [00:41<00:20,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:12,954 >> Initializing global attention on CLS token...\n",
            " 68% 159/234 [00:42<00:20,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:13,219 >> Initializing global attention on CLS token...\n",
            " 68% 160/234 [00:42<00:19,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:13,482 >> Initializing global attention on CLS token...\n",
            " 69% 161/234 [00:42<00:19,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:13,743 >> Initializing global attention on CLS token...\n",
            " 69% 162/234 [00:42<00:19,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:14,007 >> Initializing global attention on CLS token...\n",
            " 70% 163/234 [00:43<00:18,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:14,278 >> Initializing global attention on CLS token...\n",
            " 70% 164/234 [00:43<00:18,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:14,544 >> Initializing global attention on CLS token...\n",
            " 71% 165/234 [00:43<00:18,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:14,810 >> Initializing global attention on CLS token...\n",
            " 71% 166/234 [00:44<00:18,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:15,091 >> Initializing global attention on CLS token...\n",
            " 71% 167/234 [00:44<00:18,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:15,358 >> Initializing global attention on CLS token...\n",
            " 72% 168/234 [00:44<00:17,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:15,628 >> Initializing global attention on CLS token...\n",
            " 72% 169/234 [00:44<00:17,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:15,898 >> Initializing global attention on CLS token...\n",
            " 73% 170/234 [00:45<00:17,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:16,176 >> Initializing global attention on CLS token...\n",
            " 73% 171/234 [00:45<00:17,  3.67it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:16,444 >> Initializing global attention on CLS token...\n",
            " 74% 172/234 [00:45<00:16,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:16,710 >> Initializing global attention on CLS token...\n",
            " 74% 173/234 [00:45<00:16,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:16,977 >> Initializing global attention on CLS token...\n",
            " 74% 174/234 [00:46<00:16,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:17,241 >> Initializing global attention on CLS token...\n",
            " 75% 175/234 [00:46<00:15,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:17,506 >> Initializing global attention on CLS token...\n",
            " 75% 176/234 [00:46<00:15,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:17,778 >> Initializing global attention on CLS token...\n",
            " 76% 177/234 [00:46<00:15,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:18,048 >> Initializing global attention on CLS token...\n",
            " 76% 178/234 [00:47<00:15,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:18,316 >> Initializing global attention on CLS token...\n",
            " 76% 179/234 [00:47<00:14,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:18,584 >> Initializing global attention on CLS token...\n",
            " 77% 180/234 [00:47<00:14,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:18,847 >> Initializing global attention on CLS token...\n",
            " 77% 181/234 [00:48<00:14,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:19,112 >> Initializing global attention on CLS token...\n",
            " 78% 182/234 [00:48<00:13,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:19,375 >> Initializing global attention on CLS token...\n",
            " 78% 183/234 [00:48<00:13,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:19,645 >> Initializing global attention on CLS token...\n",
            " 79% 184/234 [00:48<00:13,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:19,919 >> Initializing global attention on CLS token...\n",
            " 79% 185/234 [00:49<00:13,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:20,184 >> Initializing global attention on CLS token...\n",
            " 79% 186/234 [00:49<00:12,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:20,449 >> Initializing global attention on CLS token...\n",
            " 80% 187/234 [00:49<00:12,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:20,715 >> Initializing global attention on CLS token...\n",
            " 80% 188/234 [00:49<00:12,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:20,983 >> Initializing global attention on CLS token...\n",
            " 81% 189/234 [00:50<00:12,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:21,255 >> Initializing global attention on CLS token...\n",
            " 81% 190/234 [00:50<00:11,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:21,527 >> Initializing global attention on CLS token...\n",
            " 82% 191/234 [00:50<00:11,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:21,789 >> Initializing global attention on CLS token...\n",
            " 82% 192/234 [00:50<00:11,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:22,063 >> Initializing global attention on CLS token...\n",
            " 82% 193/234 [00:51<00:11,  3.67it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:22,338 >> Initializing global attention on CLS token...\n",
            " 83% 194/234 [00:51<00:10,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:22,602 >> Initializing global attention on CLS token...\n",
            " 83% 195/234 [00:51<00:10,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:22,865 >> Initializing global attention on CLS token...\n",
            " 84% 196/234 [00:52<00:10,  3.75it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:23,133 >> Initializing global attention on CLS token...\n",
            " 84% 197/234 [00:52<00:09,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:23,397 >> Initializing global attention on CLS token...\n",
            " 85% 198/234 [00:52<00:09,  3.76it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:23,661 >> Initializing global attention on CLS token...\n",
            " 85% 199/234 [00:52<00:09,  3.77it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:23,923 >> Initializing global attention on CLS token...\n",
            " 85% 200/234 [00:53<00:08,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:24,189 >> Initializing global attention on CLS token...\n",
            " 86% 201/234 [00:53<00:08,  3.78it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:24,466 >> Initializing global attention on CLS token...\n",
            " 86% 202/234 [00:53<00:08,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:24,735 >> Initializing global attention on CLS token...\n",
            " 87% 203/234 [00:53<00:08,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:25,001 >> Initializing global attention on CLS token...\n",
            " 87% 204/234 [00:54<00:08,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:25,266 >> Initializing global attention on CLS token...\n",
            " 88% 205/234 [00:54<00:07,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:25,535 >> Initializing global attention on CLS token...\n",
            " 88% 206/234 [00:54<00:07,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:25,806 >> Initializing global attention on CLS token...\n",
            " 88% 207/234 [00:55<00:07,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:26,082 >> Initializing global attention on CLS token...\n",
            " 89% 208/234 [00:55<00:07,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:26,351 >> Initializing global attention on CLS token...\n",
            " 89% 209/234 [00:55<00:06,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:26,614 >> Initializing global attention on CLS token...\n",
            " 90% 210/234 [00:55<00:06,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:26,893 >> Initializing global attention on CLS token...\n",
            " 90% 211/234 [00:56<00:06,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:27,161 >> Initializing global attention on CLS token...\n",
            " 91% 212/234 [00:56<00:05,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:27,437 >> Initializing global attention on CLS token...\n",
            " 91% 213/234 [00:56<00:05,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:27,702 >> Initializing global attention on CLS token...\n",
            " 91% 214/234 [00:56<00:05,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:27,965 >> Initializing global attention on CLS token...\n",
            " 92% 215/234 [00:57<00:05,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:28,231 >> Initializing global attention on CLS token...\n",
            " 92% 216/234 [00:57<00:04,  3.74it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:28,498 >> Initializing global attention on CLS token...\n",
            " 93% 217/234 [00:57<00:04,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:28,770 >> Initializing global attention on CLS token...\n",
            " 93% 218/234 [00:57<00:04,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:29,042 >> Initializing global attention on CLS token...\n",
            " 94% 219/234 [00:58<00:04,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:29,309 >> Initializing global attention on CLS token...\n",
            " 94% 220/234 [00:58<00:03,  3.72it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:29,575 >> Initializing global attention on CLS token...\n",
            " 94% 221/234 [00:58<00:03,  3.73it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:29,842 >> Initializing global attention on CLS token...\n",
            " 95% 222/234 [00:59<00:03,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:30,124 >> Initializing global attention on CLS token...\n",
            " 95% 223/234 [00:59<00:02,  3.67it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:30,393 >> Initializing global attention on CLS token...\n",
            " 96% 224/234 [00:59<00:02,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:30,665 >> Initializing global attention on CLS token...\n",
            " 96% 225/234 [00:59<00:02,  3.70it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:30,939 >> Initializing global attention on CLS token...\n",
            " 97% 226/234 [01:00<00:02,  3.66it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:31,212 >> Initializing global attention on CLS token...\n",
            " 97% 227/234 [01:00<00:01,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:31,480 >> Initializing global attention on CLS token...\n",
            " 97% 228/234 [01:00<00:01,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:31,749 >> Initializing global attention on CLS token...\n",
            " 98% 229/234 [01:00<00:01,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:32,022 >> Initializing global attention on CLS token...\n",
            " 98% 230/234 [01:01<00:01,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:32,291 >> Initializing global attention on CLS token...\n",
            " 99% 231/234 [01:01<00:00,  3.71it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:32,563 >> Initializing global attention on CLS token...\n",
            " 99% 232/234 [01:01<00:00,  3.68it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:32,839 >> Initializing global attention on CLS token...\n",
            "100% 233/234 [01:02<00:00,  3.69it/s][INFO|modeling_longformer.py:1932] 2022-11-24 18:04:33,083 >> Initializing global attention on CLS token...\n",
            "100% 234/234 [01:02<00:00,  3.71it/s]\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 12942]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 12942]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 12942]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 12942]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 12942]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 12942]].\n"
          ]
        }
      ]
    }
  ]
}