{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/Longformer/experiments/scotus_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naqIWr56jSUf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/danielsaggau/IR_LDC.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rw6QC9ojY5t",
        "outputId": "65200c9b-3402-4070-e68f-baad0d4fc70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/IR_LDC\n"
          ]
        }
      ],
      "source": [
        "%cd IR_LDC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipRGPbTR45u1",
        "outputId": "2d052a7d-98f1-4b02-8680-5777ebe7f046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed GitPython-3.1.29 datasets-2.3.2 dill-0.3.5.1 docker-pycreds-0.4.0 evaluate-0.2.2 gitdb-4.0.10 huggingface-hub-0.11.1 multiprocess-0.70.13 pathtools-0.1.2 responses-0.18.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 sentry-sdk-1.9.0 setfit-0.4.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 tokenizers-0.13.2 transformers-4.23.1 urllib3-1.25.11 wandb-0.13.5 xxhash-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOCuFbIKgDss"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "# !wandb login XXXX\n",
        "%env WANDB_PROJECT=IR_LDC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQYL6fodppBA",
        "outputId": "ed0f4544-73a5-41c5-877f-e025266dfc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=logs/output_1/runs/Dec06_17-34-28_f09c3a086694,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=micro-f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=logs/output_1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=6,\n",
            "per_device_train_batch_size=6,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=logs/output_1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Reusing dataset lex_glue (/root/.cache/huggingface/datasets/lex_glue/ecthr_b/1.0.0/c3c0bd7433b636dc39ae49a84dc401190c73156617efc415b04e9835a93a7043)\n",
            "100% 3/3 [00:00<00:00, 750.90it/s]\n",
            "INFO:__main__:load ecthr_b regular model\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 17:34:31,360 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 17:34:31,360 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 17:34:31,360 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 17:34:31,360 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 17:34:31,360 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:653] 2022-12-06 17:34:32,310 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/config.json\n",
            "[INFO|configuration_utils.py:705] 2022-12-06 17:34:32,313 >> Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_ecthrb_k_10_ep1\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"multi_label_classification\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2156] 2022-12-06 17:34:32,337 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2606] 2022-12-06 17:34:32,895 >> All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:2608] 2022-12-06 17:34:32,895 >> Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at danielsaggau/bregman_ecthrb_k_10_ep1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f17234035e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Tokenizing the entire dataset: 100% 9/9 [00:33<00:00,  3.76s/ba]\n",
            "Tokenizing the entire dataset: 100% 1/1 [00:03<00:00,  3.91s/ba]\n",
            "Tokenizing the entire dataset: 100% 1/1 [00:03<00:00,  3.79s/ba]\n",
            "100% 9000/9000 [01:10<00:00, 128.04ex/s]\n",
            "100% 1000/1000 [00:07<00:00, 127.47ex/s]\n",
            "100% 1000/1000 [00:07<00:00, 129.02ex/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/content/IR_LDC/logs/output_1 is already a clone of https://huggingface.co/danielsaggau/output_1. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/IR_LDC/logs/output_1 is already a clone of https://huggingface.co/danielsaggau/output_1. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:557] 2022-12-06 17:36:48,729 >> Using cuda_amp half precision backend\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielsaggau\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/IR_LDC/wandb/run-20221206_173650-g8q5ex0p\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mECTHR_bregman_mean\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/danielsaggau/IR_LDC\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/danielsaggau/IR_LDC/runs/g8q5ex0p\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1607] 2022-12-06 17:36:51,009 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2022-12-06 17:36:51,009 >>   Num examples = 9000\n",
            "[INFO|trainer.py:1609] 2022-12-06 17:36:51,009 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1610] 2022-12-06 17:36:51,009 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:1611] 2022-12-06 17:36:51,009 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "[INFO|trainer.py:1612] 2022-12-06 17:36:51,010 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2022-12-06 17:36:51,010 >>   Total optimization steps = 15000\n",
            "[INFO|integrations.py:680] 2022-12-06 17:36:51,011 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "  0% 0/15000 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-12-06 17:36:51,022 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.2591, 'learning_rate': 2.9e-05, 'epoch': 0.33}\n",
            "{'loss': 0.1704, 'learning_rate': 2.8e-05, 'epoch': 0.67}\n",
            "{'loss': 0.1578, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.0}\n",
            " 10% 1500/15000 [20:45<3:13:56,  1.16it/s][INFO|trainer.py:2907] 2022-12-06 17:57:36,542 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-12-06 17:57:36,542 >>   Num examples = 1000\n",
            "[INFO|trainer.py:2912] 2022-12-06 17:57:36,543 >>   Batch size = 6\n",
            "\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/167 [00:00<00:19,  8.26it/s]\u001b[A\n",
            "  2% 3/167 [00:00<00:28,  5.83it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:32,  5.00it/s]\u001b[A\n",
            "  3% 5/167 [00:00<00:34,  4.68it/s]\u001b[A\n",
            "  4% 6/167 [00:01<00:35,  4.49it/s]\u001b[A\n",
            "  4% 7/167 [00:01<00:36,  4.38it/s]\u001b[A\n",
            "  5% 8/167 [00:01<00:36,  4.34it/s]\u001b[A\n",
            "  5% 9/167 [00:01<00:36,  4.30it/s]\u001b[A\n",
            "  6% 10/167 [00:02<00:36,  4.25it/s]\u001b[A\n",
            "  7% 11/167 [00:02<00:36,  4.22it/s]\u001b[A\n",
            "  7% 12/167 [00:02<00:36,  4.22it/s]\u001b[A\n",
            "  8% 13/167 [00:02<00:36,  4.20it/s]\u001b[A\n",
            "  8% 14/167 [00:03<00:36,  4.18it/s]\u001b[A\n",
            "  9% 15/167 [00:03<00:36,  4.14it/s]\u001b[A\n",
            " 10% 16/167 [00:03<00:36,  4.16it/s]\u001b[A\n",
            " 10% 17/167 [00:03<00:35,  4.19it/s]\u001b[A\n",
            " 11% 18/167 [00:04<00:35,  4.21it/s]\u001b[A\n",
            " 11% 19/167 [00:04<00:35,  4.19it/s]\u001b[A\n",
            " 12% 20/167 [00:04<00:35,  4.16it/s]\u001b[A\n",
            " 13% 21/167 [00:04<00:35,  4.13it/s]\u001b[A\n",
            " 13% 22/167 [00:05<00:35,  4.13it/s]\u001b[A\n",
            " 14% 23/167 [00:05<00:34,  4.13it/s]\u001b[A\n",
            " 14% 24/167 [00:05<00:34,  4.12it/s]\u001b[A\n",
            " 15% 25/167 [00:05<00:34,  4.12it/s]\u001b[A\n",
            " 16% 26/167 [00:06<00:34,  4.13it/s]\u001b[A\n",
            " 16% 27/167 [00:06<00:33,  4.16it/s]\u001b[A\n",
            " 17% 28/167 [00:06<00:33,  4.17it/s]\u001b[A\n",
            " 17% 29/167 [00:06<00:33,  4.16it/s]\u001b[A\n",
            " 18% 30/167 [00:06<00:32,  4.16it/s]\u001b[A\n",
            " 19% 31/167 [00:07<00:32,  4.16it/s]\u001b[A\n",
            " 19% 32/167 [00:07<00:32,  4.18it/s]\u001b[A\n",
            " 20% 33/167 [00:07<00:31,  4.19it/s]\u001b[A\n",
            " 20% 34/167 [00:07<00:31,  4.17it/s]\u001b[A\n",
            " 21% 35/167 [00:08<00:31,  4.18it/s]\u001b[A\n",
            " 22% 36/167 [00:08<00:31,  4.20it/s]\u001b[A\n",
            " 22% 37/167 [00:08<00:31,  4.18it/s]\u001b[A\n",
            " 23% 38/167 [00:08<00:31,  4.16it/s]\u001b[A\n",
            " 23% 39/167 [00:09<00:30,  4.14it/s]\u001b[A\n",
            " 24% 40/167 [00:09<00:30,  4.16it/s]\u001b[A\n",
            " 25% 41/167 [00:09<00:30,  4.18it/s]\u001b[A\n",
            " 25% 42/167 [00:09<00:29,  4.19it/s]\u001b[A\n",
            " 26% 43/167 [00:10<00:29,  4.17it/s]\u001b[A\n",
            " 26% 44/167 [00:10<00:29,  4.16it/s]\u001b[A\n",
            " 27% 45/167 [00:10<00:29,  4.14it/s]\u001b[A\n",
            " 28% 46/167 [00:10<00:29,  4.16it/s]\u001b[A\n",
            " 28% 47/167 [00:11<00:28,  4.19it/s]\u001b[A\n",
            " 29% 48/167 [00:11<00:28,  4.20it/s]\u001b[A\n",
            " 29% 49/167 [00:11<00:28,  4.19it/s]\u001b[A\n",
            " 30% 50/167 [00:11<00:27,  4.19it/s]\u001b[A\n",
            " 31% 51/167 [00:12<00:27,  4.19it/s]\u001b[A\n",
            " 31% 52/167 [00:12<00:27,  4.20it/s]\u001b[A\n",
            " 32% 53/167 [00:12<00:27,  4.18it/s]\u001b[A\n",
            " 32% 54/167 [00:12<00:26,  4.21it/s]\u001b[A\n",
            " 33% 55/167 [00:12<00:26,  4.21it/s]\u001b[A\n",
            " 34% 56/167 [00:13<00:26,  4.21it/s]\u001b[A\n",
            " 34% 57/167 [00:13<00:26,  4.21it/s]\u001b[A\n",
            " 35% 58/167 [00:13<00:25,  4.19it/s]\u001b[A\n",
            " 35% 59/167 [00:13<00:25,  4.19it/s]\u001b[A\n",
            " 36% 60/167 [00:14<00:25,  4.16it/s]\u001b[A\n",
            " 37% 61/167 [00:14<00:25,  4.16it/s]\u001b[A\n",
            " 37% 62/167 [00:14<00:25,  4.19it/s]\u001b[A\n",
            " 38% 63/167 [00:14<00:24,  4.20it/s]\u001b[A\n",
            " 38% 64/167 [00:15<00:24,  4.23it/s]\u001b[A\n",
            " 39% 65/167 [00:15<00:24,  4.22it/s]\u001b[A\n",
            " 40% 66/167 [00:15<00:23,  4.21it/s]\u001b[A\n",
            " 40% 67/167 [00:15<00:23,  4.20it/s]\u001b[A\n",
            " 41% 68/167 [00:16<00:23,  4.21it/s]\u001b[A\n",
            " 41% 69/167 [00:16<00:23,  4.20it/s]\u001b[A\n",
            " 42% 70/167 [00:16<00:23,  4.21it/s]\u001b[A\n",
            " 43% 71/167 [00:16<00:22,  4.21it/s]\u001b[A\n",
            " 43% 72/167 [00:17<00:22,  4.20it/s]\u001b[A\n",
            " 44% 73/167 [00:17<00:22,  4.19it/s]\u001b[A\n",
            " 44% 74/167 [00:17<00:22,  4.19it/s]\u001b[A\n",
            " 45% 75/167 [00:17<00:21,  4.21it/s]\u001b[A\n",
            " 46% 76/167 [00:17<00:21,  4.21it/s]\u001b[A\n",
            " 46% 77/167 [00:18<00:21,  4.21it/s]\u001b[A\n",
            " 47% 78/167 [00:18<00:21,  4.20it/s]\u001b[A\n",
            " 47% 79/167 [00:18<00:20,  4.20it/s]\u001b[A\n",
            " 48% 80/167 [00:18<00:20,  4.21it/s]\u001b[A\n",
            " 49% 81/167 [00:19<00:20,  4.21it/s]\u001b[A\n",
            " 49% 82/167 [00:19<00:20,  4.18it/s]\u001b[A\n",
            " 50% 83/167 [00:19<00:20,  4.15it/s]\u001b[A\n",
            " 50% 84/167 [00:19<00:20,  4.15it/s]\u001b[A\n",
            " 51% 85/167 [00:20<00:19,  4.17it/s]\u001b[A\n",
            " 51% 86/167 [00:20<00:19,  4.20it/s]\u001b[A\n",
            " 52% 87/167 [00:20<00:19,  4.20it/s]\u001b[A\n",
            " 53% 88/167 [00:20<00:18,  4.19it/s]\u001b[A\n",
            " 53% 89/167 [00:21<00:18,  4.21it/s]\u001b[A\n",
            " 54% 90/167 [00:21<00:18,  4.18it/s]\u001b[A\n",
            " 54% 91/167 [00:21<00:18,  4.17it/s]\u001b[A\n",
            " 55% 92/167 [00:21<00:17,  4.18it/s]\u001b[A\n",
            " 56% 93/167 [00:22<00:17,  4.20it/s]\u001b[A\n",
            " 56% 94/167 [00:22<00:17,  4.18it/s]\u001b[A\n",
            " 57% 95/167 [00:22<00:17,  4.16it/s]\u001b[A\n",
            " 57% 96/167 [00:22<00:17,  4.15it/s]\u001b[A\n",
            " 58% 97/167 [00:22<00:16,  4.14it/s]\u001b[A\n",
            " 59% 98/167 [00:23<00:16,  4.15it/s]\u001b[A\n",
            " 59% 99/167 [00:23<00:16,  4.18it/s]\u001b[A\n",
            " 60% 100/167 [00:23<00:16,  4.17it/s]\u001b[A\n",
            " 60% 101/167 [00:23<00:15,  4.15it/s]\u001b[A\n",
            " 61% 102/167 [00:24<00:15,  4.15it/s]\u001b[A\n",
            " 62% 103/167 [00:24<00:15,  4.18it/s]\u001b[A\n",
            " 62% 104/167 [00:24<00:15,  4.18it/s]\u001b[A\n",
            " 63% 105/167 [00:24<00:14,  4.18it/s]\u001b[A\n",
            " 63% 106/167 [00:25<00:14,  4.19it/s]\u001b[A\n",
            " 64% 107/167 [00:25<00:14,  4.21it/s]\u001b[A\n",
            " 65% 108/167 [00:25<00:14,  4.21it/s]\u001b[A\n",
            " 65% 109/167 [00:25<00:13,  4.17it/s]\u001b[A\n",
            " 66% 110/167 [00:26<00:13,  4.17it/s]\u001b[A\n",
            " 66% 111/167 [00:26<00:13,  4.19it/s]\u001b[A\n",
            " 67% 112/167 [00:26<00:13,  4.19it/s]\u001b[A\n",
            " 68% 113/167 [00:26<00:12,  4.18it/s]\u001b[A\n",
            " 68% 114/167 [00:27<00:12,  4.18it/s]\u001b[A\n",
            " 69% 115/167 [00:27<00:12,  4.20it/s]\u001b[A\n",
            " 69% 116/167 [00:27<00:12,  4.17it/s]\u001b[A\n",
            " 70% 117/167 [00:27<00:12,  4.16it/s]\u001b[A\n",
            " 71% 118/167 [00:28<00:11,  4.15it/s]\u001b[A\n",
            " 71% 119/167 [00:28<00:11,  4.16it/s]\u001b[A\n",
            " 72% 120/167 [00:28<00:11,  4.18it/s]\u001b[A\n",
            " 72% 121/167 [00:28<00:10,  4.19it/s]\u001b[A\n",
            " 73% 122/167 [00:28<00:10,  4.17it/s]\u001b[A\n",
            " 74% 123/167 [00:29<00:10,  4.18it/s]\u001b[A\n",
            " 74% 124/167 [00:29<00:10,  4.19it/s]\u001b[A\n",
            " 75% 125/167 [00:29<00:10,  4.19it/s]\u001b[A\n",
            " 75% 126/167 [00:29<00:09,  4.19it/s]\u001b[A\n",
            " 76% 127/167 [00:30<00:09,  4.20it/s]\u001b[A\n",
            " 77% 128/167 [00:30<00:09,  4.20it/s]\u001b[A\n",
            " 77% 129/167 [00:30<00:09,  4.18it/s]\u001b[A\n",
            " 78% 130/167 [00:30<00:08,  4.18it/s]\u001b[A\n",
            " 78% 131/167 [00:31<00:08,  4.20it/s]\u001b[A\n",
            " 79% 132/167 [00:31<00:08,  4.22it/s]\u001b[A\n",
            " 80% 133/167 [00:31<00:08,  4.21it/s]\u001b[A\n",
            " 80% 134/167 [00:31<00:07,  4.22it/s]\u001b[A\n",
            " 81% 135/167 [00:32<00:07,  4.21it/s]\u001b[A\n",
            " 81% 136/167 [00:32<00:07,  4.20it/s]\u001b[A\n",
            " 82% 137/167 [00:32<00:07,  4.20it/s]\u001b[A\n",
            " 83% 138/167 [00:32<00:06,  4.21it/s]\u001b[A\n",
            " 83% 139/167 [00:33<00:06,  4.21it/s]\u001b[A\n",
            " 84% 140/167 [00:33<00:06,  4.23it/s]\u001b[A\n",
            " 84% 141/167 [00:33<00:06,  4.22it/s]\u001b[A\n",
            " 85% 142/167 [00:33<00:05,  4.23it/s]\u001b[A\n",
            " 86% 143/167 [00:33<00:05,  4.23it/s]\u001b[A\n",
            " 86% 144/167 [00:34<00:05,  4.20it/s]\u001b[A\n",
            " 87% 145/167 [00:34<00:05,  4.19it/s]\u001b[A\n",
            " 87% 146/167 [00:34<00:05,  4.18it/s]\u001b[A\n",
            " 88% 147/167 [00:34<00:04,  4.20it/s]\u001b[A\n",
            " 89% 148/167 [00:35<00:04,  4.19it/s]\u001b[A\n",
            " 89% 149/167 [00:35<00:04,  4.17it/s]\u001b[A\n",
            " 90% 150/167 [00:35<00:04,  4.17it/s]\u001b[A\n",
            " 90% 151/167 [00:35<00:03,  4.19it/s]\u001b[A\n",
            " 91% 152/167 [00:36<00:03,  4.20it/s]\u001b[A\n",
            " 92% 153/167 [00:36<00:03,  4.20it/s]\u001b[A\n",
            " 92% 154/167 [00:36<00:03,  4.21it/s]\u001b[A\n",
            " 93% 155/167 [00:36<00:02,  4.20it/s]\u001b[A\n",
            " 93% 156/167 [00:37<00:02,  4.20it/s]\u001b[A\n",
            " 94% 157/167 [00:37<00:02,  4.20it/s]\u001b[A\n",
            " 95% 158/167 [00:37<00:02,  4.21it/s]\u001b[A\n",
            " 95% 159/167 [00:37<00:01,  4.20it/s]\u001b[A\n",
            " 96% 160/167 [00:38<00:01,  4.18it/s]\u001b[A\n",
            " 96% 161/167 [00:38<00:01,  4.17it/s]\u001b[A\n",
            " 97% 162/167 [00:38<00:01,  4.15it/s]\u001b[A\n",
            " 98% 163/167 [00:38<00:00,  4.17it/s]\u001b[A\n",
            " 98% 164/167 [00:38<00:00,  4.20it/s]\u001b[A\n",
            " 99% 165/167 [00:39<00:00,  4.19it/s]\u001b[A\n",
            " 99% 166/167 [00:39<00:00,  4.20it/s]\u001b[A\n",
            "100% 167/167 [00:39<00:00,  4.66it/s]\u001b[A\n",
            "{'eval_loss': 0.1795271337032318, 'eval_macro-f1': 0.5895551502241334, 'eval_micro-f1': 0.7411052233156699, 'eval_runtime': 39.855, 'eval_samples_per_second': 25.091, 'eval_steps_per_second': 4.19, 'epoch': 1.0}\n",
            "\n",
            " 10% 1500/15000 [21:25<3:13:56,  1.16it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-12-06 17:58:16,400 >> Saving model checkpoint to logs/output_1/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2022-12-06 17:58:16,401 >> Configuration saved in logs/output_1/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-12-06 17:58:16,820 >> Model weights saved in logs/output_1/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-12-06 17:58:16,821 >> tokenizer config file saved in logs/output_1/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-12-06 17:58:16,822 >> Special tokens file saved in logs/output_1/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-12-06 17:58:18,399 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-12-06 17:58:18,400 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{'loss': 0.1316, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.33}\n",
            "{'loss': 0.1285, 'learning_rate': 2.5002000000000002e-05, 'epoch': 1.67}\n",
            "{'loss': 0.1279, 'learning_rate': 2.4002e-05, 'epoch': 2.0}\n",
            " 20% 3000/15000 [42:19<2:53:36,  1.15it/s][INFO|trainer.py:2907] 2022-12-06 18:19:11,001 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-12-06 18:19:11,002 >>   Num examples = 1000\n",
            "[INFO|trainer.py:2912] 2022-12-06 18:19:11,002 >>   Batch size = 6\n",
            "\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/167 [00:00<00:20,  8.18it/s]\u001b[A\n",
            "  2% 3/167 [00:00<00:28,  5.83it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:32,  5.02it/s]\u001b[A\n",
            "  3% 5/167 [00:00<00:34,  4.69it/s]\u001b[A\n",
            "  4% 6/167 [00:01<00:35,  4.52it/s]\u001b[A\n",
            "  4% 7/167 [00:01<00:36,  4.40it/s]\u001b[A\n",
            "  5% 8/167 [00:01<00:36,  4.30it/s]\u001b[A\n",
            "  5% 9/167 [00:01<00:37,  4.26it/s]\u001b[A\n",
            "  6% 10/167 [00:02<00:37,  4.22it/s]\u001b[A\n",
            "  7% 11/167 [00:02<00:37,  4.19it/s]\u001b[A\n",
            "  7% 12/167 [00:02<00:37,  4.19it/s]\u001b[A\n",
            "  8% 13/167 [00:02<00:36,  4.18it/s]\u001b[A\n",
            "  8% 14/167 [00:03<00:36,  4.18it/s]\u001b[A\n",
            "  9% 15/167 [00:03<00:36,  4.15it/s]\u001b[A\n",
            " 10% 16/167 [00:03<00:36,  4.15it/s]\u001b[A\n",
            " 10% 17/167 [00:03<00:36,  4.16it/s]\u001b[A\n",
            " 11% 18/167 [00:04<00:35,  4.17it/s]\u001b[A\n",
            " 11% 19/167 [00:04<00:35,  4.19it/s]\u001b[A\n",
            " 12% 20/167 [00:04<00:35,  4.16it/s]\u001b[A\n",
            " 13% 21/167 [00:04<00:35,  4.14it/s]\u001b[A\n",
            " 13% 22/167 [00:05<00:35,  4.13it/s]\u001b[A\n",
            " 14% 23/167 [00:05<00:34,  4.13it/s]\u001b[A\n",
            " 14% 24/167 [00:05<00:34,  4.13it/s]\u001b[A\n",
            " 15% 25/167 [00:05<00:34,  4.12it/s]\u001b[A\n",
            " 16% 26/167 [00:06<00:34,  4.12it/s]\u001b[A\n",
            " 16% 27/167 [00:06<00:33,  4.12it/s]\u001b[A\n",
            " 17% 28/167 [00:06<00:33,  4.12it/s]\u001b[A\n",
            " 17% 29/167 [00:06<00:33,  4.15it/s]\u001b[A\n",
            " 18% 30/167 [00:06<00:32,  4.17it/s]\u001b[A\n",
            " 19% 31/167 [00:07<00:32,  4.16it/s]\u001b[A\n",
            " 19% 32/167 [00:07<00:32,  4.14it/s]\u001b[A\n",
            " 20% 33/167 [00:07<00:32,  4.14it/s]\u001b[A\n",
            " 20% 34/167 [00:07<00:32,  4.15it/s]\u001b[A\n",
            " 21% 35/167 [00:08<00:31,  4.18it/s]\u001b[A\n",
            " 22% 36/167 [00:08<00:31,  4.18it/s]\u001b[A\n",
            " 22% 37/167 [00:08<00:31,  4.14it/s]\u001b[A\n",
            " 23% 38/167 [00:08<00:31,  4.13it/s]\u001b[A\n",
            " 23% 39/167 [00:09<00:30,  4.13it/s]\u001b[A\n",
            " 24% 40/167 [00:09<00:30,  4.14it/s]\u001b[A\n",
            " 25% 41/167 [00:09<00:30,  4.16it/s]\u001b[A\n",
            " 25% 42/167 [00:09<00:29,  4.17it/s]\u001b[A\n",
            " 26% 43/167 [00:10<00:29,  4.16it/s]\u001b[A\n",
            " 26% 44/167 [00:10<00:29,  4.15it/s]\u001b[A\n",
            " 27% 45/167 [00:10<00:29,  4.12it/s]\u001b[A\n",
            " 28% 46/167 [00:10<00:29,  4.13it/s]\u001b[A\n",
            " 28% 47/167 [00:11<00:28,  4.15it/s]\u001b[A\n",
            " 29% 48/167 [00:11<00:28,  4.17it/s]\u001b[A\n",
            " 29% 49/167 [00:11<00:28,  4.17it/s]\u001b[A\n",
            " 30% 50/167 [00:11<00:28,  4.16it/s]\u001b[A\n",
            " 31% 51/167 [00:12<00:28,  4.13it/s]\u001b[A\n",
            " 31% 52/167 [00:12<00:27,  4.14it/s]\u001b[A\n",
            " 32% 53/167 [00:12<00:27,  4.13it/s]\u001b[A\n",
            " 32% 54/167 [00:12<00:27,  4.15it/s]\u001b[A\n",
            " 33% 55/167 [00:13<00:26,  4.18it/s]\u001b[A\n",
            " 34% 56/167 [00:13<00:26,  4.18it/s]\u001b[A\n",
            " 34% 57/167 [00:13<00:26,  4.16it/s]\u001b[A\n",
            " 35% 58/167 [00:13<00:26,  4.16it/s]\u001b[A\n",
            " 35% 59/167 [00:13<00:25,  4.18it/s]\u001b[A\n",
            " 36% 60/167 [00:14<00:25,  4.17it/s]\u001b[A\n",
            " 37% 61/167 [00:14<00:25,  4.16it/s]\u001b[A\n",
            " 37% 62/167 [00:14<00:25,  4.15it/s]\u001b[A\n",
            " 38% 63/167 [00:14<00:24,  4.17it/s]\u001b[A\n",
            " 38% 64/167 [00:15<00:24,  4.20it/s]\u001b[A\n",
            " 39% 65/167 [00:15<00:24,  4.20it/s]\u001b[A\n",
            " 40% 66/167 [00:15<00:24,  4.20it/s]\u001b[A\n",
            " 40% 67/167 [00:15<00:23,  4.20it/s]\u001b[A\n",
            " 41% 68/167 [00:16<00:23,  4.19it/s]\u001b[A\n",
            " 41% 69/167 [00:16<00:23,  4.19it/s]\u001b[A\n",
            " 42% 70/167 [00:16<00:23,  4.20it/s]\u001b[A\n",
            " 43% 71/167 [00:16<00:22,  4.19it/s]\u001b[A\n",
            " 43% 72/167 [00:17<00:22,  4.18it/s]\u001b[A\n",
            " 44% 73/167 [00:17<00:22,  4.18it/s]\u001b[A\n",
            " 44% 74/167 [00:17<00:22,  4.19it/s]\u001b[A\n",
            " 45% 75/167 [00:17<00:21,  4.19it/s]\u001b[A\n",
            " 46% 76/167 [00:18<00:21,  4.18it/s]\u001b[A\n",
            " 46% 77/167 [00:18<00:21,  4.18it/s]\u001b[A\n",
            " 47% 78/167 [00:18<00:21,  4.20it/s]\u001b[A\n",
            " 47% 79/167 [00:18<00:20,  4.20it/s]\u001b[A\n",
            " 48% 80/167 [00:18<00:20,  4.19it/s]\u001b[A\n",
            " 49% 81/167 [00:19<00:20,  4.17it/s]\u001b[A\n",
            " 49% 82/167 [00:19<00:20,  4.16it/s]\u001b[A\n",
            " 50% 83/167 [00:19<00:20,  4.17it/s]\u001b[A\n",
            " 50% 84/167 [00:19<00:19,  4.17it/s]\u001b[A\n",
            " 51% 85/167 [00:20<00:19,  4.16it/s]\u001b[A\n",
            " 51% 86/167 [00:20<00:19,  4.16it/s]\u001b[A\n",
            " 52% 87/167 [00:20<00:19,  4.18it/s]\u001b[A\n",
            " 53% 88/167 [00:20<00:18,  4.19it/s]\u001b[A\n",
            " 53% 89/167 [00:21<00:18,  4.17it/s]\u001b[A\n",
            " 54% 90/167 [00:21<00:18,  4.14it/s]\u001b[A\n",
            " 54% 91/167 [00:21<00:18,  4.16it/s]\u001b[A\n",
            " 55% 92/167 [00:21<00:17,  4.19it/s]\u001b[A\n",
            " 56% 93/167 [00:22<00:17,  4.18it/s]\u001b[A\n",
            " 56% 94/167 [00:22<00:17,  4.14it/s]\u001b[A\n",
            " 57% 95/167 [00:22<00:17,  4.12it/s]\u001b[A\n",
            " 57% 96/167 [00:22<00:17,  4.13it/s]\u001b[A\n",
            " 58% 97/167 [00:23<00:16,  4.12it/s]\u001b[A\n",
            " 59% 98/167 [00:23<00:16,  4.12it/s]\u001b[A\n",
            " 59% 99/167 [00:23<00:16,  4.12it/s]\u001b[A\n",
            " 60% 100/167 [00:23<00:16,  4.14it/s]\u001b[A\n",
            " 60% 101/167 [00:24<00:15,  4.14it/s]\u001b[A\n",
            " 61% 102/167 [00:24<00:15,  4.16it/s]\u001b[A\n",
            " 62% 103/167 [00:24<00:15,  4.17it/s]\u001b[A\n",
            " 62% 104/167 [00:24<00:15,  4.14it/s]\u001b[A\n",
            " 63% 105/167 [00:25<00:14,  4.13it/s]\u001b[A\n",
            " 63% 106/167 [00:25<00:14,  4.14it/s]\u001b[A\n",
            " 64% 107/167 [00:25<00:14,  4.17it/s]\u001b[A\n",
            " 65% 108/167 [00:25<00:14,  4.20it/s]\u001b[A\n",
            " 65% 109/167 [00:25<00:13,  4.16it/s]\u001b[A\n",
            " 66% 110/167 [00:26<00:13,  4.15it/s]\u001b[A\n",
            " 66% 111/167 [00:26<00:13,  4.14it/s]\u001b[A\n",
            " 67% 112/167 [00:26<00:13,  4.14it/s]\u001b[A\n",
            " 68% 113/167 [00:26<00:13,  4.15it/s]\u001b[A\n",
            " 68% 114/167 [00:27<00:12,  4.16it/s]\u001b[A\n",
            " 69% 115/167 [00:27<00:12,  4.17it/s]\u001b[A\n",
            " 69% 116/167 [00:27<00:12,  4.16it/s]\u001b[A\n",
            " 70% 117/167 [00:27<00:12,  4.15it/s]\u001b[A\n",
            " 71% 118/167 [00:28<00:11,  4.14it/s]\u001b[A\n",
            " 71% 119/167 [00:28<00:11,  4.14it/s]\u001b[A\n",
            " 72% 120/167 [00:28<00:11,  4.15it/s]\u001b[A\n",
            " 72% 121/167 [00:28<00:11,  4.16it/s]\u001b[A\n",
            " 73% 122/167 [00:29<00:10,  4.17it/s]\u001b[A\n",
            " 74% 123/167 [00:29<00:10,  4.17it/s]\u001b[A\n",
            " 74% 124/167 [00:29<00:10,  4.14it/s]\u001b[A\n",
            " 75% 125/167 [00:29<00:10,  4.14it/s]\u001b[A\n",
            " 75% 126/167 [00:30<00:09,  4.15it/s]\u001b[A\n",
            " 76% 127/167 [00:30<00:09,  4.15it/s]\u001b[A\n",
            " 77% 128/167 [00:30<00:09,  4.17it/s]\u001b[A\n",
            " 77% 129/167 [00:30<00:09,  4.17it/s]\u001b[A\n",
            " 78% 130/167 [00:31<00:08,  4.15it/s]\u001b[A\n",
            " 78% 131/167 [00:31<00:08,  4.15it/s]\u001b[A\n",
            " 79% 132/167 [00:31<00:08,  4.18it/s]\u001b[A\n",
            " 80% 133/167 [00:31<00:08,  4.20it/s]\u001b[A\n",
            " 80% 134/167 [00:31<00:07,  4.18it/s]\u001b[A\n",
            " 81% 135/167 [00:32<00:07,  4.18it/s]\u001b[A\n",
            " 81% 136/167 [00:32<00:07,  4.17it/s]\u001b[A\n",
            " 82% 137/167 [00:32<00:07,  4.18it/s]\u001b[A\n",
            " 83% 138/167 [00:32<00:06,  4.19it/s]\u001b[A\n",
            " 83% 139/167 [00:33<00:06,  4.19it/s]\u001b[A\n",
            " 84% 140/167 [00:33<00:06,  4.20it/s]\u001b[A\n",
            " 84% 141/167 [00:33<00:06,  4.20it/s]\u001b[A\n",
            " 85% 142/167 [00:33<00:05,  4.19it/s]\u001b[A\n",
            " 86% 143/167 [00:34<00:05,  4.18it/s]\u001b[A\n",
            " 86% 144/167 [00:34<00:05,  4.16it/s]\u001b[A\n",
            " 87% 145/167 [00:34<00:05,  4.17it/s]\u001b[A\n",
            " 87% 146/167 [00:34<00:05,  4.18it/s]\u001b[A\n",
            " 88% 147/167 [00:35<00:04,  4.18it/s]\u001b[A\n",
            " 89% 148/167 [00:35<00:04,  4.15it/s]\u001b[A\n",
            " 89% 149/167 [00:35<00:04,  4.14it/s]\u001b[A\n",
            " 90% 150/167 [00:35<00:04,  4.15it/s]\u001b[A\n",
            " 90% 151/167 [00:36<00:03,  4.16it/s]\u001b[A\n",
            " 91% 152/167 [00:36<00:03,  4.19it/s]\u001b[A\n",
            " 92% 153/167 [00:36<00:03,  4.19it/s]\u001b[A\n",
            " 92% 154/167 [00:36<00:03,  4.18it/s]\u001b[A\n",
            " 93% 155/167 [00:37<00:02,  4.16it/s]\u001b[A\n",
            " 93% 156/167 [00:37<00:02,  4.18it/s]\u001b[A\n",
            " 94% 157/167 [00:37<00:02,  4.19it/s]\u001b[A\n",
            " 95% 158/167 [00:37<00:02,  4.17it/s]\u001b[A\n",
            " 95% 159/167 [00:37<00:01,  4.17it/s]\u001b[A\n",
            " 96% 160/167 [00:38<00:01,  4.16it/s]\u001b[A\n",
            " 96% 161/167 [00:38<00:01,  4.18it/s]\u001b[A\n",
            " 97% 162/167 [00:38<00:01,  4.17it/s]\u001b[A\n",
            " 98% 163/167 [00:38<00:00,  4.16it/s]\u001b[A\n",
            " 98% 164/167 [00:39<00:00,  4.16it/s]\u001b[A\n",
            " 99% 165/167 [00:39<00:00,  4.16it/s]\u001b[A\n",
            " 99% 166/167 [00:39<00:00,  4.19it/s]\u001b[A\n",
            "\n",
            "\u001b[A{'eval_loss': 0.1520998328924179, 'eval_macro-f1': 0.6920623293222232, 'eval_micro-f1': 0.7829341317365269, 'eval_runtime': 40.0625, 'eval_samples_per_second': 24.961, 'eval_steps_per_second': 4.168, 'epoch': 2.0}\n",
            " 20% 3000/15000 [43:00<2:53:36,  1.15it/s]\n",
            "100% 167/167 [00:39<00:00,  4.63it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-12-06 18:19:51,066 >> Saving model checkpoint to logs/output_1/checkpoint-3000\n",
            "[INFO|configuration_utils.py:447] 2022-12-06 18:19:51,068 >> Configuration saved in logs/output_1/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-12-06 18:19:51,364 >> Model weights saved in logs/output_1/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-12-06 18:19:51,365 >> tokenizer config file saved in logs/output_1/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-12-06 18:19:51,365 >> Special tokens file saved in logs/output_1/checkpoint-3000/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-12-06 18:19:54,252 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-12-06 18:19:54,253 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{'loss': 0.1028, 'learning_rate': 2.3002e-05, 'epoch': 2.33}\n",
            "{'loss': 0.1065, 'learning_rate': 2.2002e-05, 'epoch': 2.67}\n",
            "{'loss': 0.1039, 'learning_rate': 2.1002e-05, 'epoch': 3.0}\n",
            " 30% 4500/15000 [1:04:02<2:31:48,  1.15it/s][INFO|trainer.py:2907] 2022-12-06 18:40:53,043 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-12-06 18:40:53,044 >>   Num examples = 1000\n",
            "[INFO|trainer.py:2912] 2022-12-06 18:40:53,044 >>   Batch size = 6\n",
            "\n",
            "  0% 0/167 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/167 [00:00<00:20,  8.10it/s]\u001b[A\n",
            "  2% 3/167 [00:00<00:28,  5.81it/s]\u001b[A\n",
            "  2% 4/167 [00:00<00:32,  5.04it/s]\u001b[A\n",
            "  3% 5/167 [00:00<00:34,  4.73it/s]\u001b[A\n",
            "  4% 6/167 [00:01<00:35,  4.52it/s]\u001b[A\n",
            "  4% 7/167 [00:01<00:36,  4.38it/s]\u001b[A\n",
            "  5% 8/167 [00:01<00:36,  4.30it/s]\u001b[A\n",
            "  5% 9/167 [00:01<00:36,  4.27it/s]\u001b[A\n",
            "  6% 10/167 [00:02<00:36,  4.25it/s]\u001b[A\n",
            "  7% 11/167 [00:02<00:37,  4.21it/s]\u001b[A\n",
            "  7% 12/167 [00:02<00:37,  4.18it/s]\u001b[A\n",
            "  8% 13/167 [00:02<00:37,  4.15it/s]\u001b[A\n",
            "  8% 14/167 [00:03<00:36,  4.14it/s]\u001b[A\n",
            "  9% 15/167 [00:03<00:36,  4.13it/s]\u001b[A\n",
            " 10% 16/167 [00:03<00:36,  4.15it/s]\u001b[A\n",
            " 10% 17/167 [00:03<00:35,  4.18it/s]\u001b[A\n",
            " 11% 18/167 [00:04<00:35,  4.19it/s]\u001b[A\n",
            " 11% 19/167 [00:04<00:35,  4.18it/s]\u001b[A\n",
            " 12% 20/167 [00:04<00:35,  4.15it/s]\u001b[A\n",
            " 13% 21/167 [00:04<00:35,  4.12it/s]\u001b[A\n",
            " 13% 22/167 [00:05<00:35,  4.12it/s]\u001b[A\n",
            " 14% 23/167 [00:05<00:34,  4.12it/s]\u001b[A\n",
            " 14% 24/167 [00:05<00:34,  4.11it/s]\u001b[A\n",
            " 15% 25/167 [00:05<00:34,  4.12it/s]\u001b[A\n",
            " 16% 26/167 [00:06<00:34,  4.14it/s]\u001b[A\n",
            " 16% 27/167 [00:06<00:33,  4.17it/s]\u001b[A\n",
            " 17% 28/167 [00:06<00:33,  4.15it/s]\u001b[A\n",
            " 17% 29/167 [00:06<00:33,  4.15it/s]\u001b[A\n",
            " 18% 30/167 [00:07<00:33,  4.15it/s]\u001b[A\n",
            " 19% 31/167 [00:07<00:32,  4.15it/s]\u001b[A\n",
            " 19% 32/167 [00:07<00:32,  4.16it/s]\u001b[A\n",
            " 20% 33/167 [00:07<00:32,  4.18it/s]\u001b[A\n",
            " 20% 34/167 [00:07<00:31,  4.16it/s]\u001b[A\n",
            " 21% 35/167 [00:08<00:31,  4.18it/s]\u001b[A\n",
            " 22% 36/167 [00:08<00:31,  4.17it/s]\u001b[A\n",
            " 22% 37/167 [00:08<00:31,  4.16it/s]\u001b[A\n",
            " 23% 38/167 [00:08<00:31,  4.15it/s]\u001b[A\n",
            " 23% 39/167 [00:09<00:30,  4.14it/s]\u001b[A\n",
            " 24% 40/167 [00:09<00:30,  4.15it/s]\u001b[A\n",
            " 25% 41/167 [00:09<00:30,  4.17it/s]\u001b[A\n",
            " 25% 42/167 [00:09<00:29,  4.18it/s]\u001b[A\n",
            " 26% 43/167 [00:10<00:29,  4.16it/s]\u001b[A\n",
            " 26% 44/167 [00:10<00:29,  4.15it/s]\u001b[A\n",
            " 27% 45/167 [00:10<00:29,  4.13it/s]\u001b[A\n",
            " 28% 46/167 [00:10<00:29,  4.14it/s]\u001b[A\n",
            " 28% 47/167 [00:11<00:28,  4.15it/s]\u001b[A\n",
            " 29% 48/167 [00:11<00:28,  4.17it/s]\u001b[A\n",
            " 29% 49/167 [00:11<00:28,  4.18it/s]\u001b[A\n",
            " 30% 50/167 [00:11<00:28,  4.16it/s]\u001b[A\n",
            " 31% 51/167 [00:12<00:28,  4.14it/s]\u001b[A\n",
            " 31% 52/167 [00:12<00:27,  4.15it/s]\u001b[A\n",
            " 32% 53/167 [00:12<00:27,  4.15it/s]\u001b[A\n",
            " 32% 54/167 [00:12<00:27,  4.18it/s]\u001b[A\n",
            " 33% 55/167 [00:13<00:26,  4.19it/s]\u001b[A\n",
            " 34% 56/167 [00:13<00:26,  4.18it/s]\u001b[A\n",
            " 34% 57/167 [00:13<00:26,  4.15it/s]\u001b[A\n",
            " 35% 58/167 [00:13<00:26,  4.14it/s]\u001b[A\n",
            " 35% 59/167 [00:13<00:26,  4.14it/s]\u001b[A\n",
            " 36% 60/167 [00:14<00:25,  4.12it/s]\u001b[A\n",
            " 37% 61/167 [00:14<00:25,  4.12it/s]\u001b[A\n",
            " 37% 62/167 [00:14<00:25,  4.14it/s]\u001b[A\n",
            " 38% 63/167 [00:14<00:24,  4.17it/s]\u001b[A\n",
            " 38% 64/167 [00:15<00:24,  4.19it/s]\u001b[A\n",
            " 39% 65/167 [00:15<00:24,  4.19it/s]\u001b[A\n",
            " 40% 66/167 [00:15<00:24,  4.20it/s]\u001b[A\n",
            " 40% 67/167 [00:15<00:23,  4.19it/s]\u001b[A\n",
            " 41% 68/167 [00:16<00:23,  4.19it/s]\u001b[A\n",
            " 41% 69/167 [00:16<00:23,  4.19it/s]\u001b[A\n",
            " 42% 70/167 [00:16<00:23,  4.21it/s]\u001b[A\n",
            " 43% 71/167 [00:16<00:22,  4.20it/s]\u001b[A\n",
            " 43% 72/167 [00:17<00:22,  4.19it/s]\u001b[A\n",
            " 44% 73/167 [00:17<00:22,  4.19it/s]\u001b[A\n",
            " 44% 74/167 [00:17<00:22,  4.21it/s]\u001b[A\n",
            " 45% 75/167 [00:17<00:21,  4.19it/s]\u001b[A\n",
            " 46% 76/167 [00:18<00:21,  4.18it/s]\u001b[A\n",
            " 46% 77/167 [00:18<00:21,  4.18it/s]\u001b[A\n",
            " 47% 78/167 [00:18<00:21,  4.20it/s]\u001b[A\n",
            " 47% 79/167 [00:18<00:20,  4.20it/s]\u001b[A\n",
            " 48% 80/167 [00:18<00:20,  4.20it/s]\u001b[A\n",
            " 49% 81/167 [00:19<00:20,  4.20it/s]\u001b[A\n",
            " 49% 82/167 [00:19<00:20,  4.20it/s]\u001b[A\n",
            " 50% 83/167 [00:19<00:20,  4.17it/s]\u001b[A\n",
            " 50% 84/167 [00:19<00:19,  4.16it/s]\u001b[A\n",
            " 51% 85/167 [00:20<00:19,  4.14it/s]\u001b[A\n",
            " 51% 86/167 [00:20<00:19,  4.16it/s]\u001b[A\n",
            " 52% 87/167 [00:20<00:19,  4.19it/s]\u001b[A\n",
            " 53% 88/167 [00:20<00:18,  4.18it/s]\u001b[A\n",
            " 53% 89/167 [00:21<00:18,  4.16it/s]\u001b[A\n",
            " 54% 90/167 [00:21<00:18,  4.14it/s]\u001b[A\n",
            " 54% 91/167 [00:21<00:18,  4.14it/s]\u001b[A\n",
            " 55% 92/167 [00:21<00:18,  4.16it/s]\u001b[A\n",
            " 56% 93/167 [00:22<00:17,  4.17it/s]\u001b[A\n",
            " 56% 94/167 [00:22<00:17,  4.17it/s]\u001b[A\n",
            " 57% 95/167 [00:22<00:17,  4.15it/s]\u001b[A\n",
            " 57% 96/167 [00:22<00:17,  4.14it/s]\u001b[A\n",
            " 58% 97/167 [00:23<00:16,  4.14it/s]\u001b[A\n",
            " 59% 98/167 [00:23<00:16,  4.13it/s]\u001b[A\n",
            " 59% 99/167 [00:23<00:16,  4.14it/s]\u001b[A\n",
            " 60% 100/167 [00:23<00:16,  4.16it/s]\u001b[A\n",
            " 60% 101/167 [00:24<00:15,  4.17it/s]\u001b[A\n",
            " 61% 102/167 [00:24<00:15,  4.16it/s]\u001b[A\n",
            " 62% 103/167 [00:24<00:15,  4.17it/s]\u001b[A\n",
            " 62% 104/167 [00:24<00:15,  4.16it/s]\u001b[A\n",
            " 63% 105/167 [00:24<00:14,  4.17it/s]\u001b[A\n",
            " 63% 106/167 [00:25<00:14,  4.19it/s]\u001b[A\n",
            " 64% 107/167 [00:25<00:14,  4.21it/s]\u001b[A\n",
            " 65% 108/167 [00:25<00:13,  4.22it/s]\u001b[A\n",
            " 65% 109/167 [00:25<00:13,  4.19it/s]\u001b[A\n",
            " 66% 110/167 [00:26<00:13,  4.18it/s]\u001b[A\n",
            " 66% 111/167 [00:26<00:13,  4.16it/s]\u001b[A\n",
            " 67% 112/167 [00:26<00:13,  4.17it/s]\u001b[A\n",
            " 68% 113/167 [00:26<00:12,  4.19it/s]\u001b[A\n",
            " 68% 114/167 [00:27<00:12,  4.19it/s]\u001b[A\n",
            " 69% 115/167 [00:27<00:12,  4.17it/s]\u001b[A\n",
            " 69% 116/167 [00:27<00:12,  4.14it/s]\u001b[A\n",
            " 70% 117/167 [00:27<00:12,  4.14it/s]\u001b[A\n",
            " 71% 118/167 [00:28<00:11,  4.14it/s]\u001b[A\n",
            " 71% 119/167 [00:28<00:11,  4.13it/s]\u001b[A\n",
            " 72% 120/167 [00:28<00:11,  4.13it/s]\u001b[A\n",
            " 72% 121/167 [00:28<00:11,  4.14it/s]\u001b[A\n",
            " 73% 122/167 [00:29<00:10,  4.15it/s]\u001b[A\n",
            " 74% 123/167 [00:29<00:10,  4.16it/s]\u001b[A\n",
            " 74% 124/167 [00:29<00:10,  4.14it/s]\u001b[A\n",
            " 75% 125/167 [00:29<00:10,  4.14it/s]\u001b[A\n",
            " 75% 126/167 [00:30<00:09,  4.13it/s]\u001b[A\n",
            " 76% 127/167 [00:30<00:09,  4.12it/s]\u001b[A\n",
            " 77% 128/167 [00:30<00:09,  4.12it/s]\u001b[A\n",
            " 77% 129/167 [00:30<00:09,  4.12it/s]\u001b[A\n",
            " 78% 130/167 [00:31<00:09,  4.11it/s]\u001b[A\n",
            " 78% 131/167 [00:31<00:08,  4.12it/s]\u001b[A\n",
            " 79% 132/167 [00:31<00:08,  4.16it/s]\u001b[A\n",
            " 80% 133/167 [00:31<00:08,  4.18it/s]\u001b[A\n",
            " 80% 134/167 [00:31<00:07,  4.17it/s]\u001b[A\n",
            " 81% 135/167 [00:32<00:07,  4.17it/s]\u001b[A\n",
            " 81% 136/167 [00:32<00:07,  4.17it/s]\u001b[A\n",
            " 82% 137/167 [00:32<00:07,  4.18it/s]\u001b[A\n",
            " 83% 138/167 [00:32<00:06,  4.18it/s]\u001b[A\n",
            " 83% 139/167 [00:33<00:06,  4.18it/s]\u001b[A\n",
            " 84% 140/167 [00:33<00:06,  4.20it/s]\u001b[A\n",
            " 84% 141/167 [00:33<00:06,  4.21it/s]\u001b[A\n",
            " 85% 142/167 [00:33<00:05,  4.20it/s]\u001b[A\n",
            " 86% 143/167 [00:34<00:05,  4.19it/s]\u001b[A\n",
            " 86% 144/167 [00:34<00:05,  4.18it/s]\u001b[A\n",
            " 87% 145/167 [00:34<00:05,  4.19it/s]\u001b[A\n",
            " 87% 146/167 [00:34<00:05,  4.17it/s]\u001b[A\n",
            " 88% 147/167 [00:35<00:04,  4.17it/s]\u001b[A\n",
            " 89% 148/167 [00:35<00:04,  4.15it/s]\u001b[A\n",
            " 89% 149/167 [00:35<00:04,  4.14it/s]\u001b[A\n",
            " 90% 150/167 [00:35<00:04,  4.17it/s]\u001b[A\n",
            " 90% 151/167 [00:36<00:03,  4.19it/s]\u001b[A\n",
            " 91% 152/167 [00:36<00:03,  4.17it/s]\u001b[A\n",
            " 92% 153/167 [00:36<00:03,  4.18it/s]\u001b[A\n",
            " 92% 154/167 [00:36<00:03,  4.20it/s]\u001b[A\n",
            " 93% 155/167 [00:36<00:02,  4.19it/s]\u001b[A\n",
            " 93% 156/167 [00:37<00:02,  4.18it/s]\u001b[A\n",
            " 94% 157/167 [00:37<00:02,  4.17it/s]\u001b[A\n",
            " 95% 158/167 [00:37<00:02,  4.16it/s]\u001b[A\n",
            " 95% 159/167 [00:37<00:01,  4.19it/s]\u001b[A\n",
            " 96% 160/167 [00:38<00:01,  4.19it/s]\u001b[A\n",
            " 96% 161/167 [00:38<00:01,  4.17it/s]\u001b[A\n",
            " 97% 162/167 [00:38<00:01,  4.13it/s]\u001b[A\n",
            " 98% 163/167 [00:38<00:00,  4.14it/s]\u001b[A\n",
            " 98% 164/167 [00:39<00:00,  4.17it/s]\u001b[A\n",
            " 99% 165/167 [00:39<00:00,  4.18it/s]\u001b[A\n",
            " 99% 166/167 [00:39<00:00,  4.19it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.1552126705646515, 'eval_macro-f1': 0.7007263841510057, 'eval_micro-f1': 0.7933765298776098, 'eval_runtime': 40.0462, 'eval_samples_per_second': 24.971, 'eval_steps_per_second': 4.17, 'epoch': 3.0}\n",
            " 30% 4500/15000 [1:04:42<2:31:48,  1.15it/s]\n",
            "100% 167/167 [00:39<00:00,  4.65it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-12-06 18:41:33,092 >> Saving model checkpoint to logs/output_1/checkpoint-4500\n",
            "[INFO|configuration_utils.py:447] 2022-12-06 18:41:33,093 >> Configuration saved in logs/output_1/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-12-06 18:41:33,389 >> Model weights saved in logs/output_1/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-12-06 18:41:33,390 >> tokenizer config file saved in logs/output_1/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-12-06 18:41:33,390 >> Special tokens file saved in logs/output_1/checkpoint-4500/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-12-06 18:41:36,269 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-12-06 18:41:36,270 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{'loss': 0.0814, 'learning_rate': 2.0002e-05, 'epoch': 3.33}\n",
            " 36% 5445/15000 [1:18:03<2:12:14,  1.20it/s]"
          ]
        }
      ],
      "source": [
        "!python /content/IR_LDC/model/ECTHR/bregman_ecthr.py \\\n",
        "    --output_dir logs/output_1 \\\n",
        "    --model_name 'danielsaggau/bregman_ecthrb_k_10_ep1' \\\n",
        "    --load_best_model_at_end \\\n",
        "    --model_type 'mean' \\\n",
        "    --overwrite_output_dir \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --per_device_eval_batch_size 6 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --push_to_hub \\\n",
        "    --fp16 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --metric_for_best_model \"micro-f1\" \\\n",
        "    --greater_is_better 1 \\\n",
        "    --report_to 'wandb' \\\n",
        "    --model_type 'ecthr_breg_test'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMjDoUs38GxyyJRD4YqzhYT",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
