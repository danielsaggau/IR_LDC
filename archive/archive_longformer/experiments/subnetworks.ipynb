{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnbULuDGHSLF5Zfsi9zZ8m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/subnetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Hp_z1k_1D2R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaVCPb2BDmRm"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 base_model=\"\",\n",
        "                 fc_dim=512,\n",
        "                 k_subs=10,\n",
        "                 layer_sizes=[64, 1], # todo fix\n",
        "                 ):\n",
        "        super(Model, self).__init__()\n",
        "        self.model = \"\"\n",
        "        dim_mlp = self.model.fc.input\n",
        "        self.model.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.model.fc)\n",
        "        \n",
        "        # k subnetworks for bregman\n",
        "        self.subnets = nn.ModuleList()\n",
        "        \n",
        "        for k_idx in range(k_subs):\n",
        "            fc = nn.Sequential()\n",
        "            \n",
        "            for i, (in_size, out_size) in enumerate(zip([fc_dim] + layer_sizes[:-1], layer_sizes)):\n",
        "                if i + 1 < len(layer_sizes):\n",
        "                    fc.add_module(\n",
        "                        name=\"fc_{:d}_{:d}\".format(k_idx, i),\n",
        "                        module=nn.Linear(in_size, out_size))\n",
        "                        \n",
        "                    fc.add_module(\n",
        "                        name=\"relu_{:d}_{:d}\".format(k_idx, i),\n",
        "                        module=nn.ReLU())\n",
        "                    \n",
        "                    fc.add_module(\n",
        "                        name=\"dp_{:d}_{:d}\".format(k_idx, i),\n",
        "                        module=nn.Dropout(p=dr_rate))\n",
        "\n",
        "                else:\n",
        "                    fc.add_module(\n",
        "                        name=\"output_{:d}\".format(k_idx),\n",
        "                        module=nn.Linear(in_size, out_size))\n",
        "                    \n",
        "                    #fc.add_module(\n",
        "                    #    name=\"output_A_{:d}\".format(k_idx),\n",
        "                    #    module=nn.Sigmoid())\n",
        "                \n",
        "            self.subnets.append(fc)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        fc_out = self.model(x)\n",
        "        \n",
        "        out = []\n",
        "        for subnet in self.subnets:\n",
        "            out.append(subnet(fc_out))\n",
        "        \n",
        "        out = torch.cat(out, -1)\n",
        "        #F.normalize(feature, dim=-1)\n",
        "        return fc_out, out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit"
      ],
      "metadata": {
        "id": "PTSFWTCjGXKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}