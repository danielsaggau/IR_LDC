{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHdEQEF8dkzntLCnJrwYAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/experiment_contrastive_bregman_simple_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEHzywItFHCc"
      },
      "outputs": [],
      "source": [
        "# contrastive loss\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Costum dataloader for torch \n",
        "class Dataloader(self): \n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n"
      ],
      "metadata": {
        "id": "soaMxvkLTGr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from typing import Iterable, Dict\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
        "\n",
        "class DistanceMetric(Enum):\n",
        "    \"\"\"\n",
        "    The metric for the contrastive loss\n",
        "    \"\"\"\n",
        "    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n",
        "    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)\n",
        "    COSINE_DISTANCE = lambda x, y: 1-F.cosine_similarity(x, y)\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss. Expects as input two texts and a label of either 0 or 1. If the label == 1, then the distance between the\n",
        "    two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased.\n",
        "    Further information: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    :param model: SentenceTransformer model\n",
        "    :param distance_metric: Function that returns a distance between two emeddings. The class SiameseDistanceMetric contains pre-defined metrices that can be used\n",
        "    :param margin: Negative samples (label == 0) should have a distance of at least the margin value.\n",
        "    :param size_average: Average by the size of the mini-batch.\n",
        "    Example::\n",
        "        from sentence_transformers import SentenceTransformer, LoggingHandler, losses, InputExample\n",
        "        from torch.utils.data import DataLoader\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        train_examples = [\n",
        "            InputExample(texts=['This is a positive pair', 'Where the distance will be minimized'], label=1),\n",
        "            InputExample(texts=['This is a negative pair', 'Their distance will be increased'], label=0)]\n",
        "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
        "        train_loss = losses.ContrastiveLoss(model=model)\n",
        "        model.fit([(train_dataloader, train_loss)], show_progress_bar=True)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: SentenceTransformer, distance_metric=DistanceMetric.COSINE_DISTANCE, margin: float = 0.5, size_average:bool = True):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.distance_metric = distance_metric\n",
        "        self.margin = margin\n",
        "        self.model = model\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def get_config_dict(self):\n",
        "        distance_metric_name = self.distance_metric.__name__\n",
        "        for name, value in vars(DistanceMetric).items():\n",
        "            if value == self.distance_metric:\n",
        "                distance_metric_name = \"DistanceMetric.{}\".format(name)\n",
        "                break\n",
        "\n",
        "        return {'distance_metric': distance_metric_name, 'margin': self.margin, 'size_average': self.size_average}\n",
        "\n",
        "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
        "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
        "        assert len(reps) == 2\n",
        "        rep_anchor, rep_other = reps\n",
        "        distances = self.distance_metric(rep_anchor, rep_other)\n",
        "        losses = 0.5 * (labels.float() * distances.pow(2) + (1 - labels).float() * F.relu(self.margin - distances).pow(2))\n",
        "        return losses.mean() if self.size_average else losses.sum()"
      ],
      "metadata": {
        "id": "lgE5IQjfHZkQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "train_examples = [\n",
        " InputExample(texts=['This is a positive pair', 'Where the distance will be minimized'], label=1),\n",
        " InputExample(texts=['This is a negative pair', 'Their distance will be increased'], label=0)]\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
        "train_loss = ContrastiveLoss(model=model)\n",
        "model.fit([(train_dataloader, train_loss)], show_progress_bar=True)"
      ],
      "metadata": {
        "id": "nTbZzIrJH5uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BregmanLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The Bregman loss. Expect as Input \n",
        "\n",
        "    \"\"\" \n",
        "\n",
        "    def __init__(self, batch_size, model: SentenceTransformer,sigma, temperature, margin: float = 0.5, size_average:bool = True):\n",
        "        super(BregmanLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.sigma = sigma\n",
        "        self.batch_size = batch_size\n",
        "        self.size_average = size_average\n",
        "        self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "    \n",
        "    def b_sim(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):): # double check the issue here \n",
        "        \n",
        "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
        "        assert len(reps) == 2\n",
        "        rep_anchor, rep_other = reps\n",
        "        \n",
        "        mm = torch.max(reps, dim=1) # was features instead of reps \n",
        "        indx_max_features = mm[1]\n",
        "        max_features = mm[0].reshape(-1, 1)\n",
        "        \n",
        "        # Compute the number of active subnets in one batch\n",
        "        eye = torch.eye(features.shape[1])\n",
        "        one = eye[indx_max_features]\n",
        "        num_max = torch.sum(one, dim=0)\n",
        "        \n",
        "        dist_matrix = max_features - features[:, indx_max_features]\n",
        "        \n",
        "        case = 2\n",
        "        if case == 0:\n",
        "            m2 = torch.divide(dist_matrix, torch.max(dist_matrix))\n",
        "            sim_matrix = torch.divide(torch.tensor([1]).to(features.device), m2 + 1)\n",
        "            \n",
        "        if case == 1:\n",
        "            gamma = torch.tensor([1]).to(features.device)\n",
        "            sim_matrix = torch.exp(torch.mul(-dist_matrix, gamma))\n",
        "            \n",
        "        if case == 2:\n",
        "            sigma = torch.tensor([self.sigma]).to(features.device)\n",
        "            sig2 = 2 * torch.pow(sigma, 2)\n",
        "            sim_matrix = torch.exp(torch.div(-dist_matrix, sig2))\n",
        "        \n",
        "        if case == 3:\n",
        "            sim_matrix = 1 - dist_matrix\n",
        "            \n",
        "        return sim_matrix, num_max\n",
        "\n",
        "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
        "        \n",
        "        N = 2 * self.batch_size\n",
        "        ###################################################\n",
        "        ### Computing Similarity Matrix ###################\n",
        "        sim_matrix, num_max = self.b_sim(features) # todo features vs reps\n",
        "        sim_matrix = sim_matrix / self.temperature\n",
        "        ##################################################\n",
        "\n",
        "        pos_ab = torch.diag(sim_matrix, self.batch_size)\n",
        "        pos_ba = torch.diag(sim_matrix, -self.batch_size)\n",
        "\n",
        "        positives = torch.cat((pos_ab, pos_ba), dim=0).reshape(N, 1)\n",
        "        negatives = sim_matrix[self.mask].reshape(N, -1)\n",
        "\n",
        "        labels = torch.zeros(N, dtype=torch.long).to(features.device)\n",
        "        logits = torch.cat((positives, negatives), dim=1)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        loss /= N\n",
        "        return loss, num_max"
      ],
      "metadata": {
        "id": "HKIOxC3tISJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "train_loss = BregmanLoss(model = model, sigma = 1, temperature =0.2, batch_size = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv6uLWSzKN46",
        "outputId": "82ca0cd9-18be-4ae7-82be-119a6d1c4419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([(train_dataloader, train_loss)], show_progress_bar=True)"
      ],
      "metadata": {
        "id": "iaOMpLY7NTlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUhw9o6gdncM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triplet implementation \n"
      ],
      "metadata": {
        "id": "IN9V-FnSKOFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import csv\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "import tqdm\n",
        "from ipywidgets import IntProgress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWT_VjJBKOUj",
        "outputId": "d8e62c5b-d505-484b-9e92-8ab06cf077d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _pairwise_divergences(embed):\n",
        "\n",
        "    max_out = tf.math.argmax(embed, 1, output_type=tf.dtypes.int32)\n",
        "    one_to_n = tf.range(tf.shape(embed)[0], dtype=tf.dtypes.int32)\n",
        "    max_indices = tf.transpose(tf.stack([one_to_n, max_out]))\n",
        "    max_values = tf.gather_nd(embed, max_indices)\n",
        "    max_values_repeated = tf.transpose(tf.reshape(tf.tile(max_values, [tf.shape(embed)[0]]), [tf.shape(embed)[0], tf.shape(embed)[0]]))\n",
        "    repeated_max_out = tf.tile(max_out, [tf.shape(embed)[0]])\n",
        "    repeated_one_to_n = tf.tile(one_to_n, [tf.shape(embed)[0]])\n",
        "    mat_rotn = tf.reshape(tf.transpose(tf.reshape(repeated_one_to_n, [tf.shape(embed)[0], tf.shape(embed)[0]])), [-1])\n",
        "    new_max_indices = tf.transpose(tf.stack([mat_rotn, repeated_max_out]))\n",
        "    new_max_values = tf.gather_nd(embed, new_max_indices)\n",
        "    reshaped_new_max_values = tf.reshape(new_max_values, [tf.shape(embed)[0], tf.shape(embed)[0]])\n",
        "    div_matrix = tf.maximum(tf.subtract(max_values_repeated, reshaped_new_max_values), 0.0)  \n",
        "    \n",
        "#    #for differentiability, this version uses softmax instead of argmax\n",
        "#    sftmx = tf.nn.softmax(tf.multiply(1.0, embed))\n",
        "#    ES = tf.linalg.matmul(embed, sftmx, transpose_b=True)\n",
        "#    one_vec = tf.reshape(tf.ones([tf.shape(embed)[0]]), [1, tf.shape(embed)[0]])\n",
        "#    diag_ES = tf.reshape(tf.linalg.diag_part(ES), [1, tf.shape(embed)[0]])\n",
        "#    max_outputs = tf.linalg.matmul(diag_ES, one_vec, transpose_a=True)\n",
        "#    div_matrix = tf.maximum(tf.subtract(max_outputs, ES), 0.0)\n",
        "    \n",
        "    return div_matrix"
      ],
      "metadata": {
        "id": "8W0yzq4bKlhz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_anchor_positive_triplet_mask(labels):\n",
        "\n",
        "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
        "    indices_not_equal = tf.logical_not(indices_equal)\n",
        "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "SzqILQGiKrO8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_anchor_negative_triplet_mask(labels):\n",
        "\n",
        "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "\n",
        "    mask = tf.logical_not(labels_equal)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def _get_triplet_mask(labels):\n",
        "\n",
        "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
        "    indices_not_equal = tf.logical_not(indices_equal)\n",
        "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
        "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
        "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
        "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
        "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
        "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
        "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
        "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "v2mlyCdpLlSA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_all_triplet_loss(labels, embeddings, margin, squared=False, breg=False):\n",
        "\n",
        "    if breg:\n",
        "        pairwise_dist = _pairwise_divergences(embeddings)\n",
        "    else:\n",
        "        pairwise_dist = _pairwise_distances(embeddings, squared=True)  \n",
        "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
        "    assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
        "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
        "    assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
        "    \n",
        "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
        "    mask = _get_triplet_mask(labels)\n",
        "    mask = tf.to_float(mask)\n",
        "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
        "\n",
        "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
        "    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n",
        "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
        "    num_valid_triplets = tf.reduce_sum(mask)\n",
        "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
        "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
        "\n",
        "    return triplet_loss, fraction_positive_triplets"
      ],
      "metadata": {
        "id": "oh5zEHhrLn-P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3vqdsqbMX1m",
        "outputId": "53ecfe57-a189-4c5e-9b4c-e2468fcf633e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<sentence_transformers.readers.InputExample.InputExample at 0x7fb6df669350>,\n",
              " <sentence_transformers.readers.InputExample.InputExample at 0x7fb6df6693d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PB3cujcJLfs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
