{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rezaei_Bregman.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNv4mthROEgMt5BLPGuwxC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/Rezaei_Bregman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FZ0EGv2nkV6n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BregmanLoss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature, sigma):\n",
        "        super(BregmanLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.sigma = sigma\n",
        "\n",
        "        self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "        \n",
        "        #self.similarity_f = nn.CosineSimilarity(dim=2)\n",
        "\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "    \n",
        "    def b_sim(self, features):\n",
        "        mm = torch.max(features, dim=1)\n",
        "        indx_max_features = mm[1]\n",
        "        max_features = mm[0].reshape(-1, 1)\n",
        "        \n",
        "        # Compute the number of active subnets in one batch\n",
        "        eye = torch.eye(features.shape[1])\n",
        "        one = eye[indx_max_features]\n",
        "        num_max = torch.sum(one, dim=0)\n",
        "        \n",
        "        dist_matrix = max_features - features[:, indx_max_features]\n",
        "        \n",
        "        case = 2\n",
        "        if case == 0:\n",
        "            m2 = torch.divide(dist_matrix, torch.max(dist_matrix))\n",
        "            sim_matrix = torch.divide(torch.tensor([1]).to(features.device), m2 + 1)\n",
        "            \n",
        "        if case == 1:\n",
        "            gamma = torch.tensor([1]).to(features.device)\n",
        "            sim_matrix = torch.exp(torch.mul(-dist_matrix, gamma))\n",
        "            \n",
        "        if case == 2:\n",
        "            sigma = torch.tensor([self.sigma]).to(features.device)\n",
        "            sig2 = 2 * torch.pow(sigma, 2)\n",
        "            sim_matrix = torch.exp(torch.div(-dist_matrix, sig2))\n",
        "        \n",
        "        if case == 3:\n",
        "            sim_matrix = 1 - dist_matrix\n",
        "            \n",
        "        return sim_matrix, num_max\n",
        "\n",
        "    def forward(self, out_a, out_b):\n",
        "        \n",
        "        N = 2 * self.batch_size\n",
        "\n",
        "        features = torch.cat((out_a, out_b), dim=0)\n",
        "        \n",
        "        ###################################################\n",
        "        ### Computing Similarity Matrix ###################\n",
        "        sim_matrix, num_max = self.b_sim(features)\n",
        "        sim_matrix = sim_matrix / self.temperature\n",
        "        ###################################################\n",
        "        #sim_matrix = self.similarity_f(out.unsqueeze(1), out.unsqueeze(0)) / self.temperature\n",
        "\n",
        "        pos_ab = torch.diag(sim_matrix, self.batch_size)\n",
        "        pos_ba = torch.diag(sim_matrix, -self.batch_size)\n",
        "\n",
        "        positives = torch.cat((pos_ab, pos_ba), dim=0).reshape(N, 1)\n",
        "        negatives = sim_matrix[self.mask].reshape(N, -1)\n",
        "\n",
        "        labels = torch.zeros(N, dtype=torch.long).to(features.device)\n",
        "        logits = torch.cat((positives, negatives), dim=1)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        loss /= N\n",
        "        return loss, num_max"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class NT_Xent(nn.Module):\n",
        "    def __init__(self, batch_size, temperature):\n",
        "        super(NT_Xent, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "\n",
        "        self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
        "\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "    \n",
        "    def forward(self, out_a, out_b):\n",
        "        \n",
        "        N = 2 * self.batch_size\n",
        "\n",
        "        out = torch.cat((out_a, out_b), dim=0)\n",
        "        \n",
        "        ###################################################\n",
        "        ### Computing Similarity Matrix ###################\n",
        "        sim_matrix = self.similarity_f(out.unsqueeze(1), out.unsqueeze(0)) / self.temperature\n",
        "        ###################################################\n",
        "        \n",
        "\n",
        "        pos_ab = torch.diag(sim_matrix, self.batch_size)\n",
        "        pos_ba = torch.diag(sim_matrix, -self.batch_size)\n",
        "\n",
        "        positives = torch.cat((pos_ab, pos_ba), dim=0).reshape(N, 1)\n",
        "        negatives = sim_matrix[self.mask].reshape(N, -1)\n",
        "        \n",
        "        #######################################################\n",
        "        ### New loss\n",
        "        #negatives = negatives.reshape(-1, 1)\n",
        "        #negatives, negatives_indices = negatives.topk(k=(N-10)*N, largest=False, dim=0)\n",
        "        #negatives = negatives.reshape(N, -1)\n",
        "        #######################################################\n",
        "\n",
        "        labels = torch.zeros(N, dtype=torch.long).to(out.device)\n",
        "        logits = torch.cat((positives, negatives), dim=1)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        loss /= N\n",
        "        return loss"
      ],
      "metadata": {
        "id": "cPNVN990pq-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "#from loss.breg_loss import BregmanLoss\n",
        "from loss.nt_xent import NT_Xent\n",
        "#from loss.breg_margin_loss import BregMarginLoss\n",
        "\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKCYAN = '\\033[96m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model,\n",
        "                 optimizer,\n",
        "                 scheduler,\n",
        "                 temperature,\n",
        "                 num_cls,\n",
        "                 epochs,\n",
        "                 sigma,\n",
        "                 lmbda,\n",
        "                 device):\n",
        "        super(Trainer, self).__init__()\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.temperature = temperature\n",
        "        self.num_cls = num_cls\n",
        "        self.epochs = epochs\n",
        "        self.sigma = sigma\n",
        "        self.lmbda = lmbda\n",
        "        self.device = device\n",
        "        self.mixed_loss = True\n",
        "\n",
        "    # train for one epoch to learn unique features\n",
        "    def train(self, data_loader, epoch):\n",
        "        self.model.train()\n",
        "        batch_size = data_loader.batch_size\n",
        "        #bloss = BregMarginLoss(batch_size)\n",
        "        bloss = BregmanLoss(batch_size, self.temperature, self.sigma)\n",
        "        nt_xent = NT_Xent(batch_size, self.temperature)\n",
        "        \n",
        "        total_loss, total_num, tot_max, train_bar = 0.0, 0, 0, tqdm(data_loader)\n",
        "        tot_bloss, tot_nt_xent = 0.0, 0.0\n",
        "        num_max = torch.tensor([0])\n",
        "        for [aug_1, aug_2], target in train_bar:\n",
        "            aug_1, aug_2 = aug_1.to(self.device), aug_2.to(self.device)\n",
        "            feature_1, out_1 = self.model(aug_1)\n",
        "            feature_2, out_2 = self.model(aug_2)\n",
        "\n",
        "            # compute loss\n",
        "            loss, num_max = bloss(out_1, out_2)\n",
        "            tot_bloss += loss.item() * batch_size\n",
        "            if self.mixed_loss:\n",
        "                loss1 = nt_xent(feature_1, feature_2)\n",
        "                tot_nt_xent += loss1.item() * batch_size\n",
        "                loss = loss + self.lmbda * loss1\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            tot_max += num_max\n",
        "            total_num += batch_size\n",
        "            total_loss += loss.item() * batch_size\n",
        "            train_bar.set_description(\n",
        "                '{}Train{} {}Epoch:{} [{}/{}] {}Loss:{}  {:.4f} {}Active Subs:{} [{}/{}]'\n",
        "                .format(\n",
        "                    bcolors.OKCYAN, bcolors.ENDC,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    epoch,\n",
        "                    self.epochs,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    total_loss / total_num,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    len(torch.where(tot_max>10)[0]),\n",
        "                    tot_max.shape[0]))\n",
        "            \n",
        "        # warmup with nt_xent loss for the first 50 epochs\n",
        "        #if epoch >= 100:\n",
        "        self.scheduler.step()\n",
        "\n",
        "        return (total_loss/total_num,\n",
        "                tot_bloss/total_num,\n",
        "                tot_nt_xent/total_num,\n",
        "                self.scheduler.get_last_lr()[0])\n",
        "    \n",
        "    \n",
        "    def bregman_sim(self, feature, feature_bank):\n",
        "        # [B, 1]\n",
        "        mf = torch.max(feature, dim=1)\n",
        "        # [N, 1]\n",
        "        mfb = torch.max(feature_bank, dim=1)\n",
        "        indx_max_feature_bank = mfb[1]\n",
        "        max_feature = mf[0].reshape(-1, 1)\n",
        "        # [B, N]\n",
        "        dist_matrix = max_feature - feature[:, indx_max_feature_bank]\n",
        "        # Computing Similarity from Bregman distance\n",
        "        sigma = torch.tensor([1.]).to(self.device)\n",
        "        sigma = 2 * torch.pow(sigma, 2)\n",
        "        sim_matrix = torch.exp(torch.div(-dist_matrix, sigma))\n",
        "        \n",
        "        return sim_matrix\n",
        "        \n",
        "    # test for one epoch, use weighted knn to find the most similar images' label to assign the test image\n",
        "    def test(self, memory_data_loader, test_data_loader, k_nn, epoch):\n",
        "        self.model.eval()\n",
        "        total_top1, total_top5, total_num, feature_bank, feature_labels = 0.0, 0.0, 0, [], []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # generate feature bank\n",
        "            \n",
        "            for [data, _], target in tqdm(memory_data_loader,\n",
        "                                        desc=f'{bcolors.OKBLUE}Feature extracting{bcolors.ENDC}'):\n",
        "                feature, out = self.model(data.to(self.device))\n",
        "                feature_bank.append(out)\n",
        "                feature_labels.append(target)\n",
        "            # [N, D]\n",
        "            feature_bank = torch.cat(feature_bank, dim=0)\n",
        "            feature_labels = torch.cat(feature_labels, dim=0).long().to(self.device)\n",
        "            # [N]\n",
        "            \n",
        "            #feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=self.device)\n",
        "            # loop test data to predict the label by weighted knn search\n",
        "            test_bar = tqdm(test_data_loader)\n",
        "            for [data, _], target in test_bar:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                feature, out = self.model(data)\n",
        "\n",
        "                total_num += data.size(0)\n",
        "                # compute bregman similarity between each feature vector and feature bank ---> [B, N]\n",
        "                sim_matrix = self.bregman_sim(out, feature_bank)\n",
        "                # [B, K]\n",
        "                sim_weight, sim_indices = sim_matrix.topk(k=k_nn, dim=-1)\n",
        "                # [B, K]\n",
        "                sim_labels = torch.gather(feature_labels.expand(data.size(0), -1),\n",
        "                                          dim=-1,\n",
        "                                          index=sim_indices)\n",
        "                sim_weight = (sim_weight / self.temperature).exp()\n",
        "\n",
        "                # counts for each class\n",
        "                one_hot_label = torch.zeros(data.size(0) * k_nn, self.num_cls, device=self.device)\n",
        "                # [B*K, C]\n",
        "                one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
        "                # weighted score ---> [B, C]\n",
        "                pred_scores = torch.sum(one_hot_label.view(\n",
        "                    data.size(0), -1, self.num_cls) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
        "\n",
        "                pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "                total_top1 += torch.sum(\n",
        "                    (pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "                total_top5 += torch.sum(\n",
        "                    (pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "                \n",
        "                test_bar.set_description(\n",
        "                    '{}Test{}  {}Epoch:{} [{}/{}] {}Acc@1: {}{:.2f}% {}Acc@5: {}{:.2f}%'.format(\n",
        "                    bcolors.OKCYAN, bcolors.ENDC,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    epoch,\n",
        "                    self.epochs,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    (total_top1 / total_num) * 100,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    (total_top5 / total_num) * 100))\n",
        "\n",
        "        return (total_top1 / total_num) * 100, (total_top5 / total_num) * 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpZ5_6e1nbbq",
        "outputId": "e52d37cb-1101-4d60-9f42-7e80dbd48680"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.BregmanLoss"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.resnet import resnet18, resnet50\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 base_model=\"resnet18\",\n",
        "                 fc_dim=128,\n",
        "                 k_subs=10,\n",
        "                 layer_sizes=[64, 1],\n",
        "                 use_bn=False,\n",
        "                 dr_rate=0.2):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        imagenet = True\n",
        "        resnet_dict = {\"resnet18\": resnet18(num_classes=fc_dim),\n",
        "                       \"resnet50\": resnet50(num_classes=fc_dim)}\n",
        "        self.model = resnet_dict[base_model]\n",
        "        dim_mlp = self.model.fc.in_features\n",
        "        self.model.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.model.fc)\n",
        "\n",
        "        \n",
        "        # k subnetworks for bregman\n",
        "        self.subnets = nn.ModuleList()\n",
        "        \n",
        "        for k_idx in range(k_subs):\n",
        "            fc = nn.Sequential()\n",
        "            \n",
        "            for i, (in_size, out_size) in enumerate(zip([fc_dim] + layer_sizes[:-1], layer_sizes)):\n",
        "                if i + 1 < len(layer_sizes):\n",
        "                    fc.add_module(\n",
        "                        name=\"fc_{:d}_{:d}\".format(k_idx, i),\n",
        "                        module=nn.Linear(in_size, out_size))\n",
        "                    \n",
        "                    if use_bn:\n",
        "                        fc.add_module(\n",
        "                            name=\"bn_{:d}_{:d}\".format(k_idx, i),\n",
        "                            module=nn.BatchNorm1d(out_size))\n",
        "                        \n",
        "                    fc.add_module(\n",
        "                        name=\"relu_{:d}_{:d}\".format(k_idx, i),\n",
        "                        module=nn.ReLU())\n",
        "                    \n",
        "                    fc.add_module(\n",
        "                        name=\"dp_{:d}_{:d}\".format(k_idx, i),\n",
        "                        module=nn.Dropout(p=dr_rate))\n",
        "\n",
        "                else:\n",
        "                    fc.add_module(\n",
        "                        name=\"output_{:d}\".format(k_idx),\n",
        "                        module=nn.Linear(in_size, out_size))\n",
        "                    \n",
        "                    #fc.add_module(\n",
        "                    #    name=\"output_A_{:d}\".format(k_idx),\n",
        "                    #    module=nn.Sigmoid())\n",
        "                \n",
        "            self.subnets.append(fc)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        fc_out = self.model(x)\n",
        "        \n",
        "        out = []\n",
        "        for subnet in self.subnets:\n",
        "            out.append(subnet(fc_out))\n",
        "        \n",
        "        out = torch.cat(out, -1)\n",
        "        #F.normalize(feature, dim=-1)\n",
        "        return fc_out, out\n"
      ],
      "metadata": {
        "id": "7ALSyXMKpX-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "\n",
        "from data_aug.data_loader import CustomDataLoader\n",
        "from model import Model\n",
        "from trainer import Trainer\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
        "\n",
        "\n",
        "def save_config_file(model_checkpoints_folder, args):\n",
        "    if not os.path.exists(model_checkpoints_folder):\n",
        "        os.makedirs(model_checkpoints_folder)\n",
        "    with open(os.path.join(model_checkpoints_folder, 'config.yml'), 'w') as outfile:\n",
        "        yaml.dump(args, outfile, default_flow_style=False)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
        "    parser.add_argument('--fc_dim', default=128, type=int, help='Feature dim for latent vector')\n",
        "    parser.add_argument('--temperature', default=0.1, type=float, help='Temperature used in softmax')\n",
        "    parser.add_argument('--lmbda', default=5, type=float, help='ratio of contrastive to divergence loss')\n",
        "    parser.add_argument('--sigma', default=1.5, type=float, help='sigma in gaussian kernel')\n",
        "    #parser.add_argument('--k_nn', default=200, type=int, help='k in knn')\n",
        "    parser.add_argument('--batch_size', default=256, type=int, help='batch size')\n",
        "    parser.add_argument('--epochs', default=400, type=int, help='epochs')\n",
        "    parser.add_argument('--k_subs', default=500, type=int, help='k subnets')\n",
        "    parser.add_argument('--layer_size', default=[32, 1], type=int,\n",
        "                        help='subnetworks layers size (defaut: [64, 1])')\n",
        "    parser.add_argument('--lr', default=3e-3, type=float,help='initial learning rate')\n",
        "    parser.add_argument('--wd', default=1e-4, type=float, help='weight decay (default: 1e-4)')\n",
        "    parser.add_argument('--seed', default=10, type=int, help='seed for initializing training.')\n",
        "    parser.add_argument('--workers', default=16, type=int, help='number of data loading workers')\n",
        "    parser.add_argument('--base_model',\n",
        "                        default='resnet50',\n",
        "                        help='dataset name',\n",
        "                        choices=[\"resnet18\", \"resnet50\"])\n",
        "    \n",
        "    parser.add_argument('-dataset-name', default='imagenet', help='dataset name')\n",
        "    parser.add_argument('--stepwise', action='store_true', help='use stepwise lr schedule')\n",
        "    # args parse\n",
        "    args = parser.parse_args()\n",
        "    base_model = args.base_model\n",
        "    dataset_name = args.dataset_name\n",
        "    lr, wd = args.lr, args.wd\n",
        "    fc_dim, temperature, k_nn = args.fc_dim, args.temperature, args.k_nn\n",
        "    batch_size, epochs = args.batch_size, args.epochs\n",
        "    workers = args.workers\n",
        "\n",
        "    # model setup and optimizer config\n",
        "    if torch.cuda.is_available():\n",
        "        args.device = torch.device('cuda')\n",
        "        #cudnn.deterministic = False\n",
        "        #cudnn.benchmark = True\n",
        "    else:\n",
        "        args.device = torch.device('cpu')\n",
        "        \n",
        "    # create a tensorboard writer\n",
        "    writer = SummaryWriter()\n",
        "    # save config file\n",
        "    save_config_file(writer.log_dir, args)\n",
        "############################################################]\n",
        "### Load Datasets and Dataloaders\n",
        "    dl = CustomDataLoader()\n",
        "    train_loader, memory_loader, test_loader = dl.get_loader(dataset_name, batch_size, workers)\n",
        "    \n",
        "    num_cls = len(test_loader.dataset.classes)\n",
        "    model = Model(base_model=base_model,\n",
        "                  fc_dim=fc_dim,\n",
        "                  k_subs=args.k_subs,\n",
        "                  layer_sizes=args.layer_size,\n",
        "                  use_bn=True,\n",
        "                  dr_rate=0.2).to(args.device)\n",
        "    print(model)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "            print(\"We have available\", torch.cuda.device_count(), \"GPUs!\")\n",
        "            model = nn.DataParallel(model, device_ids=[0,1,2,3,4,5,6,7])\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    \n",
        "    if args.stepwise:\n",
        "        scheduler = MultiStepLR(optimizer, milestones=[120,240], gamma=0.1)\n",
        "    else:\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=epochs,\n",
        "            eta_min=0, \n",
        "            last_epoch=-1)\n",
        "    \n",
        "    trainer = Trainer(model,\n",
        "                      optimizer,\n",
        "                      scheduler,\n",
        "                      temperature,\n",
        "                      num_cls,\n",
        "                      epochs,\n",
        "                      sigma,\n",
        "                      lmbda,\n",
        "                      args.device)\n",
        "    \n",
        "    # training loop\n",
        "    results = {'train_loss': [],\n",
        "               'bloss_loss': [],\n",
        "               'NTXent_loss': [],\n",
        "               #'test_acc@1': [],\n",
        "               #'test_acc@5': []\n",
        "              }\n",
        "    save_name_pre = '{}_K{}_{}_{}_{}_{}_{}_{}_{}'.format(\n",
        "        dataset_name, args.k_subs,\n",
        "        base_model, lr,\n",
        "        fc_dim, temperature,\n",
        "        k_nn, batch_size, epochs)\n",
        "    csv_dir = os.path.join(writer.log_dir, '{}_stats.csv'.format(save_name_pre))\n",
        "    model_dir = os.path.join(writer.log_dir, '{}_model.pth'.format(save_name_pre))\n",
        "    fig_dir = os.path.join(writer.log_dir, '{}_loss_acc.png'.format(save_name_pre))\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, bloss, NTXent = trainer.train(train_loader, epoch)\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['bloss_loss'].append(bloss)\n",
        "        results['NTXent_loss'].append(NTXent)\n",
        "        writer.add_scalar('loss/train', results['train_loss'][-1], epoch)\n",
        "        \n",
        "        #test_acc_1, test_acc_5 = trainer.test(memory_loader, test_loader, k_nn, epoch)\n",
        "        #results['test_acc@1'].append(test_acc_1)\n",
        "        #results['test_acc@5'].append(test_acc_5)\n",
        "        #writer.add_scalar('acc@1/test', results['test_acc@1'][-1], epoch)\n",
        "        #writer.add_scalar('acc@5/test', results['test_acc@5'][-1], epoch)\n",
        "        \n",
        "        # save statistics\n",
        "        data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
        "        data_frame.to_csv(csv_dir, index_label='epoch')\n",
        "        \n",
        "        if isinstance(model, nn.DataParallel):\n",
        "            state_dict = model.module.state_dict()\n",
        "        else:\n",
        "            state_dict = model.state_dict()\n",
        "        torch.save(state_dict, model_dir)\n",
        "        \n",
        "        #if test_acc_1 > best_acc:\n",
        "        #    best_acc = test_acc_1\n",
        "        #    if isinstance(model, nn.DataParallel):\n",
        "        #        state_dict = model.module.state_dict()\n",
        "        #    else:\n",
        "        #        state_dict = model.state_dict()\n",
        "        #    torch.save(state_dict, model_dir)\n",
        "    \n",
        "    # plotting loss and accuracies\n",
        "    #df = pd.read_csv(csv_dir)\n",
        "    #fig, axes = plt.subplots(1, 3, sharex=True, figsize=(20,5))\n",
        "    #axes[0].set_title('Loss/Train')\n",
        "    #axes[1].set_title('acc@1/test')\n",
        "    #axes[2].set_title('acc@5/test')\n",
        "    #sns.lineplot(ax=axes[0], x=\"epoch\", y=\"train_loss\", data=df)\n",
        "    #sns.lineplot(ax=axes[1], x=\"epoch\", y=\"test_acc@1\", data=df)\n",
        "    #sns.lineplot(ax=axes[2], x=\"epoch\", y=\"test_acc@5\", data=df)\n",
        "    \n",
        "    #fig.savefig(fig_dir)\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "sng3-0aRpGqP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
