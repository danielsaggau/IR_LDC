{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_hbert_replica.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN41X9E2ItKTBW+e3HhRXVb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model_hbert_replica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p36vrovP7A6v",
        "outputId": "3f7b0098-9db1-4c4e-8aa0-e0d2d3929569"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 19 15:25:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "YfxzsK0G1j_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.environ.get(\"COLAB_GPU\", False):"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "E6Qlwv5f6SRX",
        "outputId": "abb39e14-ce9f-42aa-80d0-afedcea8f3ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f5ed5888d078>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    if os.environ.get(\"COLAB_GPU\", False):\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b_1Ucvti1B8d"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from transformers.file_utils import ModelOutput"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class SimpleOutput(ModelOutput):\n",
        "    last_hidden_state: torch.FloatTensor = None\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None"
      ],
      "metadata": {
        "id": "tWyHhHdF1Emi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sinusoidal_init(num_embeddings: int, embedding_dim: int):\n",
        "    # keep dim 0 for padding token position encoding zero vector\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2 * i / embedding_dim) for i in range(embedding_dim)]\n",
        "        if pos != 0 else np.zeros(embedding_dim) for pos in range(num_embeddings)])\n",
        "\n",
        "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n",
        "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n",
        "    return torch.from_numpy(position_enc).type(torch.FloatTensor)"
      ],
      "metadata": {
        "id": "de0XiAa91JvD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalBert(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, max_segments=64, max_segment_length=128):\n",
        "        super(HierarchicalBert, self).__init__()\n",
        "        supported_models = ['bert', 'roberta', 'deberta']\n",
        "        assert encoder.config.model_type in supported_models  # other model types are not supported so far\n",
        "        # Pre-trained segment (token-wise) encoder, e.g., BERT\n",
        "        self.encoder = encoder\n",
        "        # Specs for the segment-wise encoder\n",
        "        self.hidden_size = encoder.config.hidden_size\n",
        "        self.max_segments = max_segments\n",
        "        self.max_segment_length = max_segment_length\n",
        "        # Init sinusoidal positional embeddings\n",
        "        self.seg_pos_embeddings = nn.Embedding(max_segments + 1, encoder.config.hidden_size,\n",
        "                                               padding_idx=0,\n",
        "                                               _weight=sinusoidal_init(max_segments + 1, encoder.config.hidden_size))\n",
        "        # Init segment-wise transformer-based encoder\n",
        "        self.seg_encoder = nn.Transformer(d_model=encoder.config.hidden_size,\n",
        "                                          nhead=encoder.config.num_attention_heads,\n",
        "                                          batch_first=True, dim_feedforward=encoder.config.intermediate_size,\n",
        "                                          activation=encoder.config.hidden_act,\n",
        "                                          dropout=encoder.config.hidden_dropout_prob,\n",
        "                                          layer_norm_eps=encoder.config.layer_norm_eps,\n",
        "                                          num_encoder_layers=2, num_decoder_layers=0).encoder\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids=None,\n",
        "                attention_mask=None,\n",
        "                token_type_ids=None,\n",
        "                position_ids=None,\n",
        "                head_mask=None,\n",
        "                inputs_embeds=None,\n",
        "                labels=None,\n",
        "                output_attentions=None,\n",
        "                output_hidden_states=None,\n",
        "                return_dict=None,\n",
        "                ):\n",
        "        # Hypothetical Example\n",
        "        # Batch of 4 documents: (batch_size, n_segments, max_segment_length) --> (4, 64, 128)\n",
        "        # BERT-BASE encoder: 768 hidden units\n",
        "\n",
        "        # Squash samples and segments into a single axis (batch_size * n_segments, max_segment_length) --> (256, 128)\n",
        "        input_ids_reshape = input_ids.contiguous().view(-1, input_ids.size(-1))\n",
        "        attention_mask_reshape = attention_mask.contiguous().view(-1, attention_mask.size(-1))\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids_reshape = token_type_ids.contiguous().view(-1, token_type_ids.size(-1))\n",
        "        else:\n",
        "            token_type_ids_reshape = None\n",
        "\n",
        "        # Encode segments with BERT --> (256, 128, 768)\n",
        "        encoder_outputs = self.encoder(input_ids=input_ids_reshape,\n",
        "                                       attention_mask=attention_mask_reshape,\n",
        "                                       token_type_ids=token_type_ids_reshape)[0]\n",
        "\n",
        "        # Reshape back to (batch_size, n_segments, max_segment_length, output_size) --> (4, 64, 128, 768)\n",
        "        encoder_outputs = encoder_outputs.contiguous().view(input_ids.size(0), self.max_segments,\n",
        "                                                            self.max_segment_length,\n",
        "                                                            self.hidden_size)\n",
        "\n",
        "        # Gather CLS outputs per segment --> (4, 64, 768)\n",
        "        encoder_outputs = encoder_outputs[:, :, 0]\n",
        "\n",
        "        # Infer real segments, i.e., mask paddings\n",
        "        seg_mask = (torch.sum(input_ids, 2) != 0).to(input_ids.dtype)\n",
        "        # Infer and collect segment positional embeddings\n",
        "        seg_positions = torch.arange(1, self.max_segments + 1).to(input_ids.device) * seg_mask\n",
        "        # Add segment positional embeddings to segment inputs\n",
        "        encoder_outputs += self.seg_pos_embeddings(seg_positions)\n",
        "\n",
        "        # Encode segments with segment-wise transformer\n",
        "        seg_encoder_outputs = self.seg_encoder(encoder_outputs)\n",
        "\n",
        "        # Collect document representation\n",
        "        outputs, _ = torch.max(seg_encoder_outputs, 1)\n",
        "\n",
        "        return SimpleOutput(last_hidden_state=outputs, hidden_states=outputs)"
      ],
      "metadata": {
        "id": "-UDZOor11W5R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Use as a stand-alone encoder\n",
        "    bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "    model = HierarchicalBert(encoder=bert, max_segments=64, max_segment_length=128)\n",
        "\n",
        "    fake_inputs = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n",
        "    for i in range(2): # was 4 not 2\n",
        "        # Tokenize segment\n",
        "        temp_inputs = tokenizer(['dog ' * 126] * 64)\n",
        "        fake_inputs['input_ids'].append(temp_inputs['input_ids'])\n",
        "        fake_inputs['attention_mask'].append(temp_inputs['attention_mask'])\n",
        "        fake_inputs['token_type_ids'].append(temp_inputs['token_type_ids'])\n",
        "\n",
        "    fake_inputs['input_ids'] = torch.as_tensor(fake_inputs['input_ids'])\n",
        "    fake_inputs['attention_mask'] = torch.as_tensor(fake_inputs['attention_mask'])\n",
        "    fake_inputs['token_type_ids'] = torch.as_tensor(fake_inputs['token_type_ids'])\n",
        "\n",
        "    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n",
        "\n",
        "    # 4 document representations of 768 features are expected\n",
        "    assert output[0].shape == torch.Size([2, 768]) # was for 4 not 2\n",
        "\n",
        "    # Use with HuggingFace AutoModelForSequenceClassification and Trainer API\n",
        "\n",
        "    # Init Classifier\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)\n",
        "    # Replace flat BERT encoder with hierarchical BERT encoder\n",
        "    model.bert = HierarchicalBert(encoder=model.bert, max_segments=64, max_segment_length=128)\n",
        "    output = model(fake_inputs['input_ids'], fake_inputs['attention_mask'], fake_inputs['token_type_ids'])\n",
        "\n",
        "    # 4 document outputs with 10 (num_labels) logits are expected\n",
        "    assert output.logits.shape == torch.Size([2, 10]) # was 4 not 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4z0Eq2n1bsi",
        "outputId": "921583fb-e63c-4e85-f05d-1e2b13ff79ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    }
  ]
}