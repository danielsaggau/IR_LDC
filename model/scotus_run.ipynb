{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOcEn9o625Y8LvpILCPeDBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/scotus_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naqIWr56jSUf"
      },
      "outputs": [],
      "source": [
        "!git clone https://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@github.com/danielsaggau/IR_LDC.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd IR_LDC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rw6QC9ojY5t",
        "outputId": "adc4dbeb-c0af-497f-eb6f-cddf92aa5015"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IR_LDC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ipRGPbTR45u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login fd6f7deb3126d40be9abf77ee753bf45f00e2a9a\n",
        "%env WANDB_PROJECT=IR_LDC"
      ],
      "metadata": {
        "id": "XOCuFbIKgDss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IR_LDC/model/SCOTUS/scotus_clean.py \\\n",
        "    --output_dir logs/output_1 \\\n",
        "    --load_best_model_at_end \\\n",
        "    --overwrite_output_dir \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --per_device_eval_batch_size 6 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --push_to_hub \\\n",
        "    --fp16 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --metric_for_best_model \"f1-micro\" \\\n",
        "    --greater_is_better 1 \\\n",
        "    --report_to 'wandb' \\\n",
        "    --model_type 'max'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQYL6fodppBA",
        "outputId": "d3a45536-6084-49fd-e11f-2c813a7ea33d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mDie letzten 5000Â Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n",
            " 44% 3653/8340 [57:38<1:10:35,  1.11it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:10,074 >> Initializing global attention on CLS token...\n",
            " 44% 3654/8340 [57:39<1:09:37,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:10,936 >> Initializing global attention on CLS token...\n",
            " 44% 3655/8340 [57:40<1:08:47,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:11,795 >> Initializing global attention on CLS token...\n",
            " 44% 3656/8340 [57:41<1:08:18,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:12,656 >> Initializing global attention on CLS token...\n",
            " 44% 3657/8340 [57:42<1:07:41,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:13,504 >> Initializing global attention on CLS token...\n",
            " 44% 3658/8340 [57:43<1:07:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:14,370 >> Initializing global attention on CLS token...\n",
            " 44% 3659/8340 [57:43<1:07:25,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:15,223 >> Initializing global attention on CLS token...\n",
            " 44% 3660/8340 [57:44<1:06:59,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:16,070 >> Initializing global attention on CLS token...\n",
            " 44% 3661/8340 [57:45<1:07:08,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:16,951 >> Initializing global attention on CLS token...\n",
            " 44% 3662/8340 [57:46<1:07:20,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:17,808 >> Initializing global attention on CLS token...\n",
            " 44% 3663/8340 [57:47<1:07:06,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:18,660 >> Initializing global attention on CLS token...\n",
            " 44% 3664/8340 [57:48<1:06:54,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:19,514 >> Initializing global attention on CLS token...\n",
            " 44% 3665/8340 [57:49<1:06:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:20,369 >> Initializing global attention on CLS token...\n",
            " 44% 3666/8340 [57:49<1:06:53,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:21,242 >> Initializing global attention on CLS token...\n",
            " 44% 3667/8340 [57:50<1:07:11,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:22,103 >> Initializing global attention on CLS token...\n",
            " 44% 3668/8340 [57:51<1:06:55,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:22,960 >> Initializing global attention on CLS token...\n",
            " 44% 3669/8340 [57:52<1:07:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:23,821 >> Initializing global attention on CLS token...\n",
            " 44% 3670/8340 [57:53<1:07:04,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:24,682 >> Initializing global attention on CLS token...\n",
            " 44% 3671/8340 [57:54<1:07:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:25,566 >> Initializing global attention on CLS token...\n",
            " 44% 3672/8340 [57:55<1:07:27,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:26,424 >> Initializing global attention on CLS token...\n",
            " 44% 3673/8340 [57:56<1:07:34,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:27,321 >> Initializing global attention on CLS token...\n",
            " 44% 3674/8340 [57:56<1:08:01,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:28,210 >> Initializing global attention on CLS token...\n",
            " 44% 3675/8340 [57:57<1:08:18,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:29,097 >> Initializing global attention on CLS token...\n",
            " 44% 3676/8340 [57:58<1:08:26,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:29,957 >> Initializing global attention on CLS token...\n",
            " 44% 3677/8340 [57:59<1:07:44,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:30,807 >> Initializing global attention on CLS token...\n",
            " 44% 3678/8340 [58:00<1:07:22,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:31,665 >> Initializing global attention on CLS token...\n",
            " 44% 3679/8340 [58:01<1:06:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:32,515 >> Initializing global attention on CLS token...\n",
            " 44% 3680/8340 [58:02<1:06:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:33,369 >> Initializing global attention on CLS token...\n",
            " 44% 3681/8340 [58:02<1:06:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:34,223 >> Initializing global attention on CLS token...\n",
            " 44% 3682/8340 [58:03<1:06:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:35,103 >> Initializing global attention on CLS token...\n",
            " 44% 3683/8340 [58:04<1:07:07,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:35,971 >> Initializing global attention on CLS token...\n",
            " 44% 3684/8340 [58:05<1:07:12,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:36,856 >> Initializing global attention on CLS token...\n",
            " 44% 3685/8340 [58:06<1:07:51,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:37,725 >> Initializing global attention on CLS token...\n",
            " 44% 3686/8340 [58:07<1:07:29,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:38,585 >> Initializing global attention on CLS token...\n",
            " 44% 3687/8340 [58:08<1:07:21,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:39,476 >> Initializing global attention on CLS token...\n",
            " 44% 3688/8340 [58:09<1:07:54,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:40,359 >> Initializing global attention on CLS token...\n",
            " 44% 3689/8340 [58:09<1:08:02,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:41,236 >> Initializing global attention on CLS token...\n",
            " 44% 3690/8340 [58:10<1:07:50,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:42,112 >> Initializing global attention on CLS token...\n",
            " 44% 3691/8340 [58:11<1:07:39,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:42,963 >> Initializing global attention on CLS token...\n",
            " 44% 3692/8340 [58:12<1:07:16,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:43,822 >> Initializing global attention on CLS token...\n",
            " 44% 3693/8340 [58:13<1:06:51,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:44,673 >> Initializing global attention on CLS token...\n",
            " 44% 3694/8340 [58:14<1:06:40,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:45,530 >> Initializing global attention on CLS token...\n",
            " 44% 3695/8340 [58:15<1:06:29,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:46,382 >> Initializing global attention on CLS token...\n",
            " 44% 3696/8340 [58:15<1:06:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:47,242 >> Initializing global attention on CLS token...\n",
            " 44% 3697/8340 [58:16<1:06:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:48,096 >> Initializing global attention on CLS token...\n",
            " 44% 3698/8340 [58:17<1:06:38,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:48,990 >> Initializing global attention on CLS token...\n",
            " 44% 3699/8340 [58:18<1:07:15,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:49,877 >> Initializing global attention on CLS token...\n",
            " 44% 3700/8340 [58:19<1:07:49,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:50,776 >> Initializing global attention on CLS token...\n",
            " 44% 3701/8340 [58:20<1:07:59,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:51,633 >> Initializing global attention on CLS token...\n",
            " 44% 3702/8340 [58:21<1:07:23,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:52,489 >> Initializing global attention on CLS token...\n",
            " 44% 3703/8340 [58:22<1:06:51,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:53,357 >> Initializing global attention on CLS token...\n",
            " 44% 3704/8340 [58:22<1:07:12,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:54,256 >> Initializing global attention on CLS token...\n",
            " 44% 3705/8340 [58:23<1:07:43,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:55,154 >> Initializing global attention on CLS token...\n",
            " 44% 3706/8340 [58:24<1:08:02,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:56,001 >> Initializing global attention on CLS token...\n",
            " 44% 3707/8340 [58:25<1:07:29,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:56,860 >> Initializing global attention on CLS token...\n",
            " 44% 3708/8340 [58:26<1:07:10,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:57,722 >> Initializing global attention on CLS token...\n",
            " 44% 3709/8340 [58:27<1:06:59,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:58,583 >> Initializing global attention on CLS token...\n",
            " 44% 3710/8340 [58:28<1:06:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:56:59,456 >> Initializing global attention on CLS token...\n",
            " 44% 3711/8340 [58:29<1:06:50,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:00,320 >> Initializing global attention on CLS token...\n",
            " 45% 3712/8340 [58:29<1:06:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:01,175 >> Initializing global attention on CLS token...\n",
            " 45% 3713/8340 [58:30<1:06:29,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:02,044 >> Initializing global attention on CLS token...\n",
            " 45% 3714/8340 [58:31<1:06:39,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:02,905 >> Initializing global attention on CLS token...\n",
            " 45% 3715/8340 [58:32<1:06:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:03,769 >> Initializing global attention on CLS token...\n",
            " 45% 3716/8340 [58:33<1:06:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:04,634 >> Initializing global attention on CLS token...\n",
            " 45% 3717/8340 [58:34<1:06:38,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:05,504 >> Initializing global attention on CLS token...\n",
            " 45% 3718/8340 [58:35<1:06:31,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:06,407 >> Initializing global attention on CLS token...\n",
            " 45% 3719/8340 [58:36<1:07:32,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:07,266 >> Initializing global attention on CLS token...\n",
            " 45% 3720/8340 [58:36<1:07:18,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:08,137 >> Initializing global attention on CLS token...\n",
            " 45% 3721/8340 [58:37<1:07:16,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:09,027 >> Initializing global attention on CLS token...\n",
            " 45% 3722/8340 [58:38<1:09:36,  1.11it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:10,026 >> Initializing global attention on CLS token...\n",
            " 45% 3723/8340 [58:39<1:09:54,  1.10it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:10,903 >> Initializing global attention on CLS token...\n",
            " 45% 3724/8340 [58:40<1:08:54,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:11,765 >> Initializing global attention on CLS token...\n",
            " 45% 3725/8340 [58:41<1:08:04,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:12,624 >> Initializing global attention on CLS token...\n",
            " 45% 3726/8340 [58:42<1:07:28,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:13,485 >> Initializing global attention on CLS token...\n",
            " 45% 3727/8340 [58:43<1:06:58,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:14,342 >> Initializing global attention on CLS token...\n",
            " 45% 3728/8340 [58:43<1:06:42,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:15,218 >> Initializing global attention on CLS token...\n",
            " 45% 3729/8340 [58:44<1:06:45,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:16,072 >> Initializing global attention on CLS token...\n",
            " 45% 3730/8340 [58:45<1:06:32,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:16,933 >> Initializing global attention on CLS token...\n",
            " 45% 3731/8340 [58:46<1:06:29,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:17,806 >> Initializing global attention on CLS token...\n",
            " 45% 3732/8340 [58:47<1:06:42,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:18,672 >> Initializing global attention on CLS token...\n",
            " 45% 3733/8340 [58:48<1:06:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:19,528 >> Initializing global attention on CLS token...\n",
            " 45% 3734/8340 [58:49<1:06:07,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:20,379 >> Initializing global attention on CLS token...\n",
            " 45% 3735/8340 [58:49<1:05:59,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:21,237 >> Initializing global attention on CLS token...\n",
            " 45% 3736/8340 [58:50<1:05:52,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:22,091 >> Initializing global attention on CLS token...\n",
            " 45% 3737/8340 [58:51<1:05:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:22,945 >> Initializing global attention on CLS token...\n",
            " 45% 3738/8340 [58:52<1:05:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:23,797 >> Initializing global attention on CLS token...\n",
            " 45% 3739/8340 [58:53<1:05:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:24,671 >> Initializing global attention on CLS token...\n",
            " 45% 3740/8340 [58:54<1:06:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:25,531 >> Initializing global attention on CLS token...\n",
            " 45% 3741/8340 [58:55<1:05:51,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:26,388 >> Initializing global attention on CLS token...\n",
            " 45% 3742/8340 [58:55<1:05:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:27,239 >> Initializing global attention on CLS token...\n",
            " 45% 3743/8340 [58:56<1:05:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:28,094 >> Initializing global attention on CLS token...\n",
            " 45% 3744/8340 [58:57<1:05:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:28,959 >> Initializing global attention on CLS token...\n",
            " 45% 3745/8340 [58:58<1:05:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:29,805 >> Initializing global attention on CLS token...\n",
            " 45% 3746/8340 [58:59<1:05:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:30,665 >> Initializing global attention on CLS token...\n",
            " 45% 3747/8340 [59:00<1:05:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:31,518 >> Initializing global attention on CLS token...\n",
            " 45% 3748/8340 [59:01<1:05:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:32,377 >> Initializing global attention on CLS token...\n",
            " 45% 3749/8340 [59:01<1:05:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:33,235 >> Initializing global attention on CLS token...\n",
            " 45% 3750/8340 [59:02<1:05:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:34,098 >> Initializing global attention on CLS token...\n",
            " 45% 3751/8340 [59:03<1:05:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:34,963 >> Initializing global attention on CLS token...\n",
            " 45% 3752/8340 [59:04<1:05:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:35,821 >> Initializing global attention on CLS token...\n",
            " 45% 3753/8340 [59:05<1:05:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:36,681 >> Initializing global attention on CLS token...\n",
            " 45% 3754/8340 [59:06<1:05:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:37,544 >> Initializing global attention on CLS token...\n",
            " 45% 3755/8340 [59:07<1:05:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:38,407 >> Initializing global attention on CLS token...\n",
            " 45% 3756/8340 [59:08<1:06:00,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:39,275 >> Initializing global attention on CLS token...\n",
            " 45% 3757/8340 [59:08<1:05:50,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:40,130 >> Initializing global attention on CLS token...\n",
            " 45% 3758/8340 [59:09<1:05:48,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:40,995 >> Initializing global attention on CLS token...\n",
            " 45% 3759/8340 [59:10<1:05:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:41,877 >> Initializing global attention on CLS token...\n",
            " 45% 3760/8340 [59:11<1:06:21,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:42,763 >> Initializing global attention on CLS token...\n",
            " 45% 3761/8340 [59:12<1:06:50,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:43,655 >> Initializing global attention on CLS token...\n",
            " 45% 3762/8340 [59:13<1:07:00,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:44,521 >> Initializing global attention on CLS token...\n",
            " 45% 3763/8340 [59:14<1:06:49,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:45,401 >> Initializing global attention on CLS token...\n",
            " 45% 3764/8340 [59:15<1:06:50,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:46,263 >> Initializing global attention on CLS token...\n",
            " 45% 3765/8340 [59:15<1:06:32,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:47,155 >> Initializing global attention on CLS token...\n",
            " 45% 3766/8340 [59:16<1:07:00,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:48,024 >> Initializing global attention on CLS token...\n",
            " 45% 3767/8340 [59:17<1:06:29,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:48,884 >> Initializing global attention on CLS token...\n",
            " 45% 3768/8340 [59:18<1:06:30,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:49,773 >> Initializing global attention on CLS token...\n",
            " 45% 3769/8340 [59:19<1:06:51,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:50,663 >> Initializing global attention on CLS token...\n",
            " 45% 3770/8340 [59:20<1:07:03,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:51,594 >> Initializing global attention on CLS token...\n",
            " 45% 3771/8340 [59:21<1:08:07,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:52,480 >> Initializing global attention on CLS token...\n",
            " 45% 3772/8340 [59:22<1:07:56,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:53,344 >> Initializing global attention on CLS token...\n",
            " 45% 3773/8340 [59:22<1:07:07,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:54,202 >> Initializing global attention on CLS token...\n",
            " 45% 3774/8340 [59:23<1:06:28,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:55,053 >> Initializing global attention on CLS token...\n",
            " 45% 3775/8340 [59:24<1:06:03,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:55,957 >> Initializing global attention on CLS token...\n",
            " 45% 3776/8340 [59:25<1:06:54,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:56,817 >> Initializing global attention on CLS token...\n",
            " 45% 3777/8340 [59:26<1:06:37,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:57,707 >> Initializing global attention on CLS token...\n",
            " 45% 3778/8340 [59:27<1:06:36,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:58,568 >> Initializing global attention on CLS token...\n",
            " 45% 3779/8340 [59:28<1:06:21,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:57:59,426 >> Initializing global attention on CLS token...\n",
            " 45% 3780/8340 [59:29<1:05:54,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:00,279 >> Initializing global attention on CLS token...\n",
            " 45% 3781/8340 [59:29<1:05:29,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:01,130 >> Initializing global attention on CLS token...\n",
            " 45% 3782/8340 [59:30<1:05:30,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:01,994 >> Initializing global attention on CLS token...\n",
            " 45% 3783/8340 [59:31<1:05:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:02,848 >> Initializing global attention on CLS token...\n",
            " 45% 3784/8340 [59:32<1:05:17,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:03,708 >> Initializing global attention on CLS token...\n",
            " 45% 3785/8340 [59:33<1:05:23,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:04,573 >> Initializing global attention on CLS token...\n",
            " 45% 3786/8340 [59:34<1:05:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:05,429 >> Initializing global attention on CLS token...\n",
            " 45% 3787/8340 [59:35<1:05:13,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:06,286 >> Initializing global attention on CLS token...\n",
            " 45% 3788/8340 [59:35<1:05:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:07,149 >> Initializing global attention on CLS token...\n",
            " 45% 3789/8340 [59:36<1:05:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:08,048 >> Initializing global attention on CLS token...\n",
            " 45% 3790/8340 [59:37<1:05:53,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:08,897 >> Initializing global attention on CLS token...\n",
            " 45% 3791/8340 [59:38<1:05:35,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:09,771 >> Initializing global attention on CLS token...\n",
            " 45% 3792/8340 [59:39<1:05:54,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:10,631 >> Initializing global attention on CLS token...\n",
            " 45% 3793/8340 [59:40<1:05:24,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:11,480 >> Initializing global attention on CLS token...\n",
            " 45% 3794/8340 [59:41<1:05:09,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:12,335 >> Initializing global attention on CLS token...\n",
            " 46% 3795/8340 [59:41<1:04:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:13,183 >> Initializing global attention on CLS token...\n",
            " 46% 3796/8340 [59:42<1:04:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:14,072 >> Initializing global attention on CLS token...\n",
            " 46% 3797/8340 [59:43<1:05:35,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:14,927 >> Initializing global attention on CLS token...\n",
            " 46% 3798/8340 [59:44<1:05:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:15,782 >> Initializing global attention on CLS token...\n",
            " 46% 3799/8340 [59:45<1:05:15,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:16,644 >> Initializing global attention on CLS token...\n",
            " 46% 3800/8340 [59:46<1:05:03,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:17,497 >> Initializing global attention on CLS token...\n",
            " 46% 3801/8340 [59:47<1:04:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:18,349 >> Initializing global attention on CLS token...\n",
            " 46% 3802/8340 [59:47<1:04:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:19,203 >> Initializing global attention on CLS token...\n",
            " 46% 3803/8340 [59:48<1:04:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:20,056 >> Initializing global attention on CLS token...\n",
            " 46% 3804/8340 [59:49<1:04:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:20,911 >> Initializing global attention on CLS token...\n",
            " 46% 3805/8340 [59:50<1:04:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:21,771 >> Initializing global attention on CLS token...\n",
            " 46% 3806/8340 [59:51<1:04:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:22,624 >> Initializing global attention on CLS token...\n",
            " 46% 3807/8340 [59:52<1:04:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:23,477 >> Initializing global attention on CLS token...\n",
            " 46% 3808/8340 [59:53<1:04:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:24,332 >> Initializing global attention on CLS token...\n",
            " 46% 3809/8340 [59:53<1:04:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:25,229 >> Initializing global attention on CLS token...\n",
            " 46% 3810/8340 [59:54<1:05:27,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:26,081 >> Initializing global attention on CLS token...\n",
            " 46% 3811/8340 [59:55<1:05:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:26,940 >> Initializing global attention on CLS token...\n",
            " 46% 3812/8340 [59:56<1:04:58,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:27,794 >> Initializing global attention on CLS token...\n",
            " 46% 3813/8340 [59:57<1:04:51,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:28,649 >> Initializing global attention on CLS token...\n",
            " 46% 3814/8340 [59:58<1:04:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:29,504 >> Initializing global attention on CLS token...\n",
            " 46% 3815/8340 [59:59<1:04:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:30,357 >> Initializing global attention on CLS token...\n",
            " 46% 3816/8340 [59:59<1:04:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:31,212 >> Initializing global attention on CLS token...\n",
            " 46% 3817/8340 [1:00:00<1:04:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:32,072 >> Initializing global attention on CLS token...\n",
            " 46% 3818/8340 [1:00:01<1:04:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:32,924 >> Initializing global attention on CLS token...\n",
            " 46% 3819/8340 [1:00:02<1:04:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:33,780 >> Initializing global attention on CLS token...\n",
            " 46% 3820/8340 [1:00:03<1:04:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:34,638 >> Initializing global attention on CLS token...\n",
            " 46% 3821/8340 [1:00:04<1:04:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:35,491 >> Initializing global attention on CLS token...\n",
            " 46% 3822/8340 [1:00:05<1:04:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:36,345 >> Initializing global attention on CLS token...\n",
            " 46% 3823/8340 [1:00:05<1:04:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:37,199 >> Initializing global attention on CLS token...\n",
            " 46% 3824/8340 [1:00:06<1:04:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:38,054 >> Initializing global attention on CLS token...\n",
            " 46% 3825/8340 [1:00:07<1:04:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:38,913 >> Initializing global attention on CLS token...\n",
            " 46% 3826/8340 [1:00:08<1:04:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:39,766 >> Initializing global attention on CLS token...\n",
            " 46% 3827/8340 [1:00:09<1:04:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:40,624 >> Initializing global attention on CLS token...\n",
            " 46% 3828/8340 [1:00:10<1:04:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:41,480 >> Initializing global attention on CLS token...\n",
            " 46% 3829/8340 [1:00:11<1:04:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:42,331 >> Initializing global attention on CLS token...\n",
            " 46% 3830/8340 [1:00:11<1:04:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:43,180 >> Initializing global attention on CLS token...\n",
            " 46% 3831/8340 [1:00:12<1:04:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:44,033 >> Initializing global attention on CLS token...\n",
            " 46% 3832/8340 [1:00:13<1:04:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:44,887 >> Initializing global attention on CLS token...\n",
            " 46% 3833/8340 [1:00:14<1:04:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:45,742 >> Initializing global attention on CLS token...\n",
            " 46% 3834/8340 [1:00:15<1:04:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:46,598 >> Initializing global attention on CLS token...\n",
            " 46% 3835/8340 [1:00:16<1:04:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:47,453 >> Initializing global attention on CLS token...\n",
            " 46% 3836/8340 [1:00:17<1:04:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:48,300 >> Initializing global attention on CLS token...\n",
            " 46% 3837/8340 [1:00:17<1:04:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:49,155 >> Initializing global attention on CLS token...\n",
            " 46% 3838/8340 [1:00:18<1:04:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:50,007 >> Initializing global attention on CLS token...\n",
            " 46% 3839/8340 [1:00:19<1:04:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:50,867 >> Initializing global attention on CLS token...\n",
            " 46% 3840/8340 [1:00:20<1:04:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:51,717 >> Initializing global attention on CLS token...\n",
            " 46% 3841/8340 [1:00:21<1:03:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:52,570 >> Initializing global attention on CLS token...\n",
            " 46% 3842/8340 [1:00:22<1:03:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:53,455 >> Initializing global attention on CLS token...\n",
            " 46% 3843/8340 [1:00:23<1:06:41,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:54,399 >> Initializing global attention on CLS token...\n",
            " 46% 3844/8340 [1:00:23<1:05:50,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:55,251 >> Initializing global attention on CLS token...\n",
            " 46% 3845/8340 [1:00:24<1:05:13,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:56,102 >> Initializing global attention on CLS token...\n",
            " 46% 3846/8340 [1:00:25<1:04:52,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:56,959 >> Initializing global attention on CLS token...\n",
            " 46% 3847/8340 [1:00:26<1:04:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:57,811 >> Initializing global attention on CLS token...\n",
            " 46% 3848/8340 [1:00:27<1:04:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:58,665 >> Initializing global attention on CLS token...\n",
            " 46% 3849/8340 [1:00:28<1:04:15,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:58:59,522 >> Initializing global attention on CLS token...\n",
            " 46% 3850/8340 [1:00:29<1:04:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:00,373 >> Initializing global attention on CLS token...\n",
            " 46% 3851/8340 [1:00:29<1:04:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:01,228 >> Initializing global attention on CLS token...\n",
            " 46% 3852/8340 [1:00:30<1:04:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:02,084 >> Initializing global attention on CLS token...\n",
            " 46% 3853/8340 [1:00:31<1:03:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:02,981 >> Initializing global attention on CLS token...\n",
            " 46% 3854/8340 [1:00:32<1:04:52,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:03,837 >> Initializing global attention on CLS token...\n",
            " 46% 3855/8340 [1:00:33<1:04:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:04,687 >> Initializing global attention on CLS token...\n",
            " 46% 3856/8340 [1:00:34<1:04:22,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:05,562 >> Initializing global attention on CLS token...\n",
            " 46% 3857/8340 [1:00:35<1:04:34,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:06,417 >> Initializing global attention on CLS token...\n",
            " 46% 3858/8340 [1:00:36<1:04:19,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:07,274 >> Initializing global attention on CLS token...\n",
            " 46% 3859/8340 [1:00:36<1:04:14,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:08,133 >> Initializing global attention on CLS token...\n",
            " 46% 3860/8340 [1:00:37<1:04:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:08,991 >> Initializing global attention on CLS token...\n",
            " 46% 3861/8340 [1:00:38<1:04:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:09,844 >> Initializing global attention on CLS token...\n",
            " 46% 3862/8340 [1:00:39<1:04:06,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:10,702 >> Initializing global attention on CLS token...\n",
            " 46% 3863/8340 [1:00:40<1:04:04,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:11,563 >> Initializing global attention on CLS token...\n",
            " 46% 3864/8340 [1:00:41<1:03:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:12,460 >> Initializing global attention on CLS token...\n",
            " 46% 3865/8340 [1:00:42<1:04:47,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:13,309 >> Initializing global attention on CLS token...\n",
            " 46% 3866/8340 [1:00:42<1:04:26,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:14,163 >> Initializing global attention on CLS token...\n",
            " 46% 3867/8340 [1:00:43<1:04:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:15,017 >> Initializing global attention on CLS token...\n",
            " 46% 3868/8340 [1:00:44<1:04:00,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:15,870 >> Initializing global attention on CLS token...\n",
            " 46% 3869/8340 [1:00:45<1:03:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:16,726 >> Initializing global attention on CLS token...\n",
            " 46% 3870/8340 [1:00:46<1:03:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:17,580 >> Initializing global attention on CLS token...\n",
            " 46% 3871/8340 [1:00:47<1:03:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:18,436 >> Initializing global attention on CLS token...\n",
            " 46% 3872/8340 [1:00:48<1:03:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:19,293 >> Initializing global attention on CLS token...\n",
            " 46% 3873/8340 [1:00:48<1:03:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:20,141 >> Initializing global attention on CLS token...\n",
            " 46% 3874/8340 [1:00:49<1:03:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:20,995 >> Initializing global attention on CLS token...\n",
            " 46% 3875/8340 [1:00:50<1:03:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:21,848 >> Initializing global attention on CLS token...\n",
            " 46% 3876/8340 [1:00:51<1:03:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:22,704 >> Initializing global attention on CLS token...\n",
            " 46% 3877/8340 [1:00:52<1:03:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:23,558 >> Initializing global attention on CLS token...\n",
            " 46% 3878/8340 [1:00:53<1:03:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:24,410 >> Initializing global attention on CLS token...\n",
            " 47% 3879/8340 [1:00:54<1:03:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:25,262 >> Initializing global attention on CLS token...\n",
            " 47% 3880/8340 [1:00:54<1:03:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:26,115 >> Initializing global attention on CLS token...\n",
            " 47% 3881/8340 [1:00:55<1:03:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:26,972 >> Initializing global attention on CLS token...\n",
            " 47% 3882/8340 [1:00:56<1:03:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:27,825 >> Initializing global attention on CLS token...\n",
            " 47% 3883/8340 [1:00:57<1:03:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:28,680 >> Initializing global attention on CLS token...\n",
            " 47% 3884/8340 [1:00:58<1:03:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:29,549 >> Initializing global attention on CLS token...\n",
            " 47% 3885/8340 [1:00:59<1:03:46,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:30,414 >> Initializing global attention on CLS token...\n",
            " 47% 3886/8340 [1:01:00<1:03:43,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:31,267 >> Initializing global attention on CLS token...\n",
            " 47% 3887/8340 [1:01:00<1:03:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:32,117 >> Initializing global attention on CLS token...\n",
            " 47% 3888/8340 [1:01:01<1:03:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:32,975 >> Initializing global attention on CLS token...\n",
            " 47% 3889/8340 [1:01:02<1:03:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:33,831 >> Initializing global attention on CLS token...\n",
            " 47% 3890/8340 [1:01:03<1:03:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:34,687 >> Initializing global attention on CLS token...\n",
            " 47% 3891/8340 [1:01:04<1:03:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:35,537 >> Initializing global attention on CLS token...\n",
            " 47% 3892/8340 [1:01:05<1:03:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:36,400 >> Initializing global attention on CLS token...\n",
            " 47% 3893/8340 [1:01:05<1:03:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:37,248 >> Initializing global attention on CLS token...\n",
            " 47% 3894/8340 [1:01:06<1:03:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:38,105 >> Initializing global attention on CLS token...\n",
            " 47% 3895/8340 [1:01:07<1:03:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:38,970 >> Initializing global attention on CLS token...\n",
            " 47% 3896/8340 [1:01:08<1:03:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:39,822 >> Initializing global attention on CLS token...\n",
            " 47% 3897/8340 [1:01:09<1:03:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:40,669 >> Initializing global attention on CLS token...\n",
            " 47% 3898/8340 [1:01:10<1:03:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:41,528 >> Initializing global attention on CLS token...\n",
            " 47% 3899/8340 [1:01:11<1:03:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:42,386 >> Initializing global attention on CLS token...\n",
            " 47% 3900/8340 [1:01:11<1:03:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:43,236 >> Initializing global attention on CLS token...\n",
            " 47% 3901/8340 [1:01:12<1:03:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:44,092 >> Initializing global attention on CLS token...\n",
            " 47% 3902/8340 [1:01:13<1:03:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:44,980 >> Initializing global attention on CLS token...\n",
            " 47% 3903/8340 [1:01:14<1:03:55,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:45,834 >> Initializing global attention on CLS token...\n",
            " 47% 3904/8340 [1:01:15<1:03:40,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:46,687 >> Initializing global attention on CLS token...\n",
            " 47% 3905/8340 [1:01:16<1:03:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:47,533 >> Initializing global attention on CLS token...\n",
            " 47% 3906/8340 [1:01:17<1:03:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:48,386 >> Initializing global attention on CLS token...\n",
            " 47% 3907/8340 [1:01:17<1:03:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:49,240 >> Initializing global attention on CLS token...\n",
            " 47% 3908/8340 [1:01:18<1:03:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:50,093 >> Initializing global attention on CLS token...\n",
            " 47% 3909/8340 [1:01:19<1:03:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:50,946 >> Initializing global attention on CLS token...\n",
            " 47% 3910/8340 [1:01:20<1:03:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:51,800 >> Initializing global attention on CLS token...\n",
            " 47% 3911/8340 [1:01:21<1:03:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:52,654 >> Initializing global attention on CLS token...\n",
            " 47% 3912/8340 [1:01:22<1:03:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:53,512 >> Initializing global attention on CLS token...\n",
            " 47% 3913/8340 [1:01:23<1:03:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:54,366 >> Initializing global attention on CLS token...\n",
            " 47% 3914/8340 [1:01:23<1:02:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:55,216 >> Initializing global attention on CLS token...\n",
            " 47% 3915/8340 [1:01:24<1:03:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:56,074 >> Initializing global attention on CLS token...\n",
            " 47% 3916/8340 [1:01:25<1:02:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:56,923 >> Initializing global attention on CLS token...\n",
            " 47% 3917/8340 [1:01:26<1:02:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:57,775 >> Initializing global attention on CLS token...\n",
            " 47% 3918/8340 [1:01:27<1:02:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:58,629 >> Initializing global attention on CLS token...\n",
            " 47% 3919/8340 [1:01:28<1:02:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 16:59:59,485 >> Initializing global attention on CLS token...\n",
            " 47% 3920/8340 [1:01:29<1:03:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:00,347 >> Initializing global attention on CLS token...\n",
            " 47% 3921/8340 [1:01:29<1:02:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:01,197 >> Initializing global attention on CLS token...\n",
            " 47% 3922/8340 [1:01:30<1:02:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:02,051 >> Initializing global attention on CLS token...\n",
            " 47% 3923/8340 [1:01:31<1:02:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:02,902 >> Initializing global attention on CLS token...\n",
            " 47% 3924/8340 [1:01:32<1:02:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:03,759 >> Initializing global attention on CLS token...\n",
            " 47% 3925/8340 [1:01:33<1:02:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:04,608 >> Initializing global attention on CLS token...\n",
            " 47% 3926/8340 [1:01:34<1:02:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:05,463 >> Initializing global attention on CLS token...\n",
            " 47% 3927/8340 [1:01:35<1:02:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:06,316 >> Initializing global attention on CLS token...\n",
            " 47% 3928/8340 [1:01:35<1:02:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:07,172 >> Initializing global attention on CLS token...\n",
            " 47% 3929/8340 [1:01:36<1:02:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:08,023 >> Initializing global attention on CLS token...\n",
            " 47% 3930/8340 [1:01:37<1:02:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:08,882 >> Initializing global attention on CLS token...\n",
            " 47% 3931/8340 [1:01:38<1:02:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:09,738 >> Initializing global attention on CLS token...\n",
            " 47% 3932/8340 [1:01:39<1:02:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:10,586 >> Initializing global attention on CLS token...\n",
            " 47% 3933/8340 [1:01:40<1:02:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:11,444 >> Initializing global attention on CLS token...\n",
            " 47% 3934/8340 [1:01:41<1:02:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:12,301 >> Initializing global attention on CLS token...\n",
            " 47% 3935/8340 [1:01:41<1:02:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:13,164 >> Initializing global attention on CLS token...\n",
            " 47% 3936/8340 [1:01:42<1:03:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:14,028 >> Initializing global attention on CLS token...\n",
            " 47% 3937/8340 [1:01:43<1:02:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:14,882 >> Initializing global attention on CLS token...\n",
            " 47% 3938/8340 [1:01:44<1:02:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:15,729 >> Initializing global attention on CLS token...\n",
            " 47% 3939/8340 [1:01:45<1:02:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:16,585 >> Initializing global attention on CLS token...\n",
            " 47% 3940/8340 [1:01:46<1:02:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:17,442 >> Initializing global attention on CLS token...\n",
            " 47% 3941/8340 [1:01:47<1:02:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:18,295 >> Initializing global attention on CLS token...\n",
            " 47% 3942/8340 [1:01:47<1:02:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:19,144 >> Initializing global attention on CLS token...\n",
            " 47% 3943/8340 [1:01:48<1:02:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:19,998 >> Initializing global attention on CLS token...\n",
            " 47% 3944/8340 [1:01:49<1:02:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:20,850 >> Initializing global attention on CLS token...\n",
            " 47% 3945/8340 [1:01:50<1:02:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:21,705 >> Initializing global attention on CLS token...\n",
            " 47% 3946/8340 [1:01:51<1:02:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:22,559 >> Initializing global attention on CLS token...\n",
            " 47% 3947/8340 [1:01:52<1:02:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:23,413 >> Initializing global attention on CLS token...\n",
            " 47% 3948/8340 [1:01:53<1:02:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:24,266 >> Initializing global attention on CLS token...\n",
            " 47% 3949/8340 [1:01:53<1:02:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:25,121 >> Initializing global attention on CLS token...\n",
            " 47% 3950/8340 [1:01:54<1:02:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:25,977 >> Initializing global attention on CLS token...\n",
            " 47% 3951/8340 [1:01:55<1:02:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:26,826 >> Initializing global attention on CLS token...\n",
            " 47% 3952/8340 [1:01:56<1:02:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:27,679 >> Initializing global attention on CLS token...\n",
            " 47% 3953/8340 [1:01:57<1:02:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:28,535 >> Initializing global attention on CLS token...\n",
            " 47% 3954/8340 [1:01:58<1:02:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:29,387 >> Initializing global attention on CLS token...\n",
            " 47% 3955/8340 [1:01:58<1:02:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:30,240 >> Initializing global attention on CLS token...\n",
            " 47% 3956/8340 [1:01:59<1:02:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:31,096 >> Initializing global attention on CLS token...\n",
            " 47% 3957/8340 [1:02:00<1:02:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:31,951 >> Initializing global attention on CLS token...\n",
            " 47% 3958/8340 [1:02:01<1:02:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:32,803 >> Initializing global attention on CLS token...\n",
            " 47% 3959/8340 [1:02:02<1:02:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:33,655 >> Initializing global attention on CLS token...\n",
            " 47% 3960/8340 [1:02:03<1:02:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:34,511 >> Initializing global attention on CLS token...\n",
            " 47% 3961/8340 [1:02:04<1:02:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:35,371 >> Initializing global attention on CLS token...\n",
            " 48% 3962/8340 [1:02:04<1:02:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:36,221 >> Initializing global attention on CLS token...\n",
            " 48% 3963/8340 [1:02:05<1:02:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:37,075 >> Initializing global attention on CLS token...\n",
            " 48% 3964/8340 [1:02:06<1:02:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:37,931 >> Initializing global attention on CLS token...\n",
            " 48% 3965/8340 [1:02:07<1:02:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:38,782 >> Initializing global attention on CLS token...\n",
            " 48% 3966/8340 [1:02:08<1:02:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:39,637 >> Initializing global attention on CLS token...\n",
            " 48% 3967/8340 [1:02:09<1:02:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:40,493 >> Initializing global attention on CLS token...\n",
            " 48% 3968/8340 [1:02:10<1:02:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:41,354 >> Initializing global attention on CLS token...\n",
            " 48% 3969/8340 [1:02:10<1:02:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:42,209 >> Initializing global attention on CLS token...\n",
            " 48% 3970/8340 [1:02:11<1:02:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:43,060 >> Initializing global attention on CLS token...\n",
            " 48% 3971/8340 [1:02:12<1:02:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:43,913 >> Initializing global attention on CLS token...\n",
            " 48% 3972/8340 [1:02:13<1:02:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:44,770 >> Initializing global attention on CLS token...\n",
            " 48% 3973/8340 [1:02:14<1:02:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:45,627 >> Initializing global attention on CLS token...\n",
            " 48% 3974/8340 [1:02:15<1:02:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:46,483 >> Initializing global attention on CLS token...\n",
            " 48% 3975/8340 [1:02:16<1:02:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:47,337 >> Initializing global attention on CLS token...\n",
            " 48% 3976/8340 [1:02:16<1:02:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:48,194 >> Initializing global attention on CLS token...\n",
            " 48% 3977/8340 [1:02:17<1:02:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:49,049 >> Initializing global attention on CLS token...\n",
            " 48% 3978/8340 [1:02:18<1:02:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:49,905 >> Initializing global attention on CLS token...\n",
            " 48% 3979/8340 [1:02:19<1:02:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:50,759 >> Initializing global attention on CLS token...\n",
            " 48% 3980/8340 [1:02:20<1:02:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:51,614 >> Initializing global attention on CLS token...\n",
            " 48% 3981/8340 [1:02:21<1:02:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:52,470 >> Initializing global attention on CLS token...\n",
            " 48% 3982/8340 [1:02:22<1:02:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:53,325 >> Initializing global attention on CLS token...\n",
            " 48% 3983/8340 [1:02:22<1:02:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:54,182 >> Initializing global attention on CLS token...\n",
            " 48% 3984/8340 [1:02:23<1:02:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:55,038 >> Initializing global attention on CLS token...\n",
            " 48% 3985/8340 [1:02:24<1:02:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:55,893 >> Initializing global attention on CLS token...\n",
            " 48% 3986/8340 [1:02:25<1:02:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:56,747 >> Initializing global attention on CLS token...\n",
            " 48% 3987/8340 [1:02:26<1:01:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:57,600 >> Initializing global attention on CLS token...\n",
            " 48% 3988/8340 [1:02:27<1:01:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:58,452 >> Initializing global attention on CLS token...\n",
            " 48% 3989/8340 [1:02:28<1:01:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:00:59,310 >> Initializing global attention on CLS token...\n",
            " 48% 3990/8340 [1:02:28<1:01:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:00,162 >> Initializing global attention on CLS token...\n",
            " 48% 3991/8340 [1:02:29<1:01:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:01,021 >> Initializing global attention on CLS token...\n",
            " 48% 3992/8340 [1:02:30<1:01:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:01,870 >> Initializing global attention on CLS token...\n",
            " 48% 3993/8340 [1:02:31<1:01:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:02,728 >> Initializing global attention on CLS token...\n",
            " 48% 3994/8340 [1:02:32<1:01:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:03,624 >> Initializing global attention on CLS token...\n",
            " 48% 3995/8340 [1:02:33<1:02:42,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:04,475 >> Initializing global attention on CLS token...\n",
            " 48% 3996/8340 [1:02:34<1:02:24,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:05,328 >> Initializing global attention on CLS token...\n",
            " 48% 3997/8340 [1:02:34<1:02:15,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:06,182 >> Initializing global attention on CLS token...\n",
            " 48% 3998/8340 [1:02:35<1:02:07,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:07,039 >> Initializing global attention on CLS token...\n",
            " 48% 3999/8340 [1:02:36<1:02:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:07,891 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.1793, 'learning_rate': 2.604916067146283e-05, 'epoch': 4.8}\n",
            " 48% 4000/8340 [1:02:37<1:04:47,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:08,882 >> Initializing global attention on CLS token...\n",
            " 48% 4001/8340 [1:02:38<1:04:03,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:09,739 >> Initializing global attention on CLS token...\n",
            " 48% 4002/8340 [1:02:39<1:03:15,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:10,592 >> Initializing global attention on CLS token...\n",
            " 48% 4003/8340 [1:02:40<1:02:48,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:11,447 >> Initializing global attention on CLS token...\n",
            " 48% 4004/8340 [1:02:41<1:02:30,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:12,301 >> Initializing global attention on CLS token...\n",
            " 48% 4005/8340 [1:02:41<1:02:13,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:13,152 >> Initializing global attention on CLS token...\n",
            " 48% 4006/8340 [1:02:42<1:02:06,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:14,009 >> Initializing global attention on CLS token...\n",
            " 48% 4007/8340 [1:02:43<1:01:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:14,860 >> Initializing global attention on CLS token...\n",
            " 48% 4008/8340 [1:02:44<1:02:33,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:15,749 >> Initializing global attention on CLS token...\n",
            " 48% 4009/8340 [1:02:45<1:02:22,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:16,608 >> Initializing global attention on CLS token...\n",
            " 48% 4010/8340 [1:02:46<1:02:11,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:17,468 >> Initializing global attention on CLS token...\n",
            " 48% 4011/8340 [1:02:47<1:01:59,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:18,318 >> Initializing global attention on CLS token...\n",
            " 48% 4012/8340 [1:02:47<1:01:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:19,172 >> Initializing global attention on CLS token...\n",
            " 48% 4013/8340 [1:02:48<1:01:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:20,034 >> Initializing global attention on CLS token...\n",
            " 48% 4014/8340 [1:02:49<1:01:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:20,888 >> Initializing global attention on CLS token...\n",
            " 48% 4015/8340 [1:02:50<1:01:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:21,746 >> Initializing global attention on CLS token...\n",
            " 48% 4016/8340 [1:02:51<1:01:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:22,600 >> Initializing global attention on CLS token...\n",
            " 48% 4017/8340 [1:02:52<1:01:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:23,456 >> Initializing global attention on CLS token...\n",
            " 48% 4018/8340 [1:02:53<1:01:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:24,314 >> Initializing global attention on CLS token...\n",
            " 48% 4019/8340 [1:02:53<1:01:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:25,172 >> Initializing global attention on CLS token...\n",
            " 48% 4020/8340 [1:02:54<1:01:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:26,031 >> Initializing global attention on CLS token...\n",
            " 48% 4021/8340 [1:02:55<1:01:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:26,882 >> Initializing global attention on CLS token...\n",
            " 48% 4022/8340 [1:02:56<1:01:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:27,733 >> Initializing global attention on CLS token...\n",
            " 48% 4023/8340 [1:02:57<1:01:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:28,591 >> Initializing global attention on CLS token...\n",
            " 48% 4024/8340 [1:02:58<1:01:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:29,442 >> Initializing global attention on CLS token...\n",
            " 48% 4025/8340 [1:02:59<1:01:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:30,298 >> Initializing global attention on CLS token...\n",
            " 48% 4026/8340 [1:02:59<1:01:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:31,151 >> Initializing global attention on CLS token...\n",
            " 48% 4027/8340 [1:03:00<1:01:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:32,008 >> Initializing global attention on CLS token...\n",
            " 48% 4028/8340 [1:03:01<1:01:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:32,862 >> Initializing global attention on CLS token...\n",
            " 48% 4029/8340 [1:03:02<1:01:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:33,712 >> Initializing global attention on CLS token...\n",
            " 48% 4030/8340 [1:03:03<1:01:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:34,574 >> Initializing global attention on CLS token...\n",
            " 48% 4031/8340 [1:03:04<1:01:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:35,428 >> Initializing global attention on CLS token...\n",
            " 48% 4032/8340 [1:03:05<1:01:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:36,282 >> Initializing global attention on CLS token...\n",
            " 48% 4033/8340 [1:03:05<1:01:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:37,132 >> Initializing global attention on CLS token...\n",
            " 48% 4034/8340 [1:03:06<1:01:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:37,986 >> Initializing global attention on CLS token...\n",
            " 48% 4035/8340 [1:03:07<1:01:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:38,838 >> Initializing global attention on CLS token...\n",
            " 48% 4036/8340 [1:03:08<1:01:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:39,688 >> Initializing global attention on CLS token...\n",
            " 48% 4037/8340 [1:03:09<1:01:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:40,547 >> Initializing global attention on CLS token...\n",
            " 48% 4038/8340 [1:03:10<1:01:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:41,405 >> Initializing global attention on CLS token...\n",
            " 48% 4039/8340 [1:03:10<1:01:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:42,252 >> Initializing global attention on CLS token...\n",
            " 48% 4040/8340 [1:03:11<1:01:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:43,109 >> Initializing global attention on CLS token...\n",
            " 48% 4041/8340 [1:03:12<1:01:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:43,964 >> Initializing global attention on CLS token...\n",
            " 48% 4042/8340 [1:03:13<1:01:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:44,815 >> Initializing global attention on CLS token...\n",
            " 48% 4043/8340 [1:03:14<1:01:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:45,674 >> Initializing global attention on CLS token...\n",
            " 48% 4044/8340 [1:03:15<1:01:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:46,525 >> Initializing global attention on CLS token...\n",
            " 49% 4045/8340 [1:03:16<1:01:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:47,414 >> Initializing global attention on CLS token...\n",
            " 49% 4046/8340 [1:03:17<1:01:50,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:48,267 >> Initializing global attention on CLS token...\n",
            " 49% 4047/8340 [1:03:17<1:01:26,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:49,115 >> Initializing global attention on CLS token...\n",
            " 49% 4048/8340 [1:03:18<1:01:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:49,968 >> Initializing global attention on CLS token...\n",
            " 49% 4049/8340 [1:03:19<1:01:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:50,820 >> Initializing global attention on CLS token...\n",
            " 49% 4050/8340 [1:03:20<1:01:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:51,677 >> Initializing global attention on CLS token...\n",
            " 49% 4051/8340 [1:03:21<1:01:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:52,532 >> Initializing global attention on CLS token...\n",
            " 49% 4052/8340 [1:03:22<1:01:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:53,388 >> Initializing global attention on CLS token...\n",
            " 49% 4053/8340 [1:03:22<1:01:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:54,240 >> Initializing global attention on CLS token...\n",
            " 49% 4054/8340 [1:03:23<1:01:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:55,094 >> Initializing global attention on CLS token...\n",
            " 49% 4055/8340 [1:03:24<1:01:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:55,951 >> Initializing global attention on CLS token...\n",
            " 49% 4056/8340 [1:03:25<1:01:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:56,807 >> Initializing global attention on CLS token...\n",
            " 49% 4057/8340 [1:03:26<1:01:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:57,661 >> Initializing global attention on CLS token...\n",
            " 49% 4058/8340 [1:03:27<1:01:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:58,525 >> Initializing global attention on CLS token...\n",
            " 49% 4059/8340 [1:03:28<1:01:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:01:59,377 >> Initializing global attention on CLS token...\n",
            " 49% 4060/8340 [1:03:28<1:00:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:00,228 >> Initializing global attention on CLS token...\n",
            " 49% 4061/8340 [1:03:29<1:00:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:01,081 >> Initializing global attention on CLS token...\n",
            " 49% 4062/8340 [1:03:30<1:00:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:01,934 >> Initializing global attention on CLS token...\n",
            " 49% 4063/8340 [1:03:31<1:00:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:02,792 >> Initializing global attention on CLS token...\n",
            " 49% 4064/8340 [1:03:32<1:00:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:03,683 >> Initializing global attention on CLS token...\n",
            " 49% 4065/8340 [1:03:33<1:01:35,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:04,535 >> Initializing global attention on CLS token...\n",
            " 49% 4066/8340 [1:03:34<1:01:23,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:05,391 >> Initializing global attention on CLS token...\n",
            " 49% 4067/8340 [1:03:34<1:01:11,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:06,243 >> Initializing global attention on CLS token...\n",
            " 49% 4068/8340 [1:03:35<1:01:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:07,097 >> Initializing global attention on CLS token...\n",
            " 49% 4069/8340 [1:03:36<1:01:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:07,953 >> Initializing global attention on CLS token...\n",
            " 49% 4070/8340 [1:03:37<1:00:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:08,802 >> Initializing global attention on CLS token...\n",
            " 49% 4071/8340 [1:03:38<1:00:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:09,652 >> Initializing global attention on CLS token...\n",
            " 49% 4072/8340 [1:03:39<1:00:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:10,505 >> Initializing global attention on CLS token...\n",
            " 49% 4073/8340 [1:03:40<1:00:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:11,362 >> Initializing global attention on CLS token...\n",
            " 49% 4074/8340 [1:03:40<1:00:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:12,219 >> Initializing global attention on CLS token...\n",
            " 49% 4075/8340 [1:03:41<1:00:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:13,076 >> Initializing global attention on CLS token...\n",
            " 49% 4076/8340 [1:03:42<1:00:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:13,927 >> Initializing global attention on CLS token...\n",
            " 49% 4077/8340 [1:03:43<1:00:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:14,779 >> Initializing global attention on CLS token...\n",
            " 49% 4078/8340 [1:03:44<1:00:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:15,632 >> Initializing global attention on CLS token...\n",
            " 49% 4079/8340 [1:03:45<1:00:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:16,487 >> Initializing global attention on CLS token...\n",
            " 49% 4080/8340 [1:03:46<1:00:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:17,345 >> Initializing global attention on CLS token...\n",
            " 49% 4081/8340 [1:03:46<1:00:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:18,195 >> Initializing global attention on CLS token...\n",
            " 49% 4082/8340 [1:03:47<1:00:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:19,049 >> Initializing global attention on CLS token...\n",
            " 49% 4083/8340 [1:03:48<1:00:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:19,906 >> Initializing global attention on CLS token...\n",
            " 49% 4084/8340 [1:03:49<1:00:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:20,755 >> Initializing global attention on CLS token...\n",
            " 49% 4085/8340 [1:03:50<1:00:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:21,653 >> Initializing global attention on CLS token...\n",
            " 49% 4086/8340 [1:03:51<1:01:27,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:22,508 >> Initializing global attention on CLS token...\n",
            " 49% 4087/8340 [1:03:52<1:01:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:23,362 >> Initializing global attention on CLS token...\n",
            " 49% 4088/8340 [1:03:52<1:00:58,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:24,215 >> Initializing global attention on CLS token...\n",
            " 49% 4089/8340 [1:03:53<1:00:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:25,068 >> Initializing global attention on CLS token...\n",
            " 49% 4090/8340 [1:03:54<1:00:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:25,939 >> Initializing global attention on CLS token...\n",
            " 49% 4091/8340 [1:03:55<1:01:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:26,801 >> Initializing global attention on CLS token...\n",
            " 49% 4092/8340 [1:03:56<1:00:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:27,655 >> Initializing global attention on CLS token...\n",
            " 49% 4093/8340 [1:03:57<1:00:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:28,503 >> Initializing global attention on CLS token...\n",
            " 49% 4094/8340 [1:03:58<1:00:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:29,366 >> Initializing global attention on CLS token...\n",
            " 49% 4095/8340 [1:03:58<1:00:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:30,212 >> Initializing global attention on CLS token...\n",
            " 49% 4096/8340 [1:03:59<1:00:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:31,065 >> Initializing global attention on CLS token...\n",
            " 49% 4097/8340 [1:04:00<1:00:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:31,923 >> Initializing global attention on CLS token...\n",
            " 49% 4098/8340 [1:04:01<1:00:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:32,777 >> Initializing global attention on CLS token...\n",
            " 49% 4099/8340 [1:04:02<1:00:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:33,629 >> Initializing global attention on CLS token...\n",
            " 49% 4100/8340 [1:04:03<1:00:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:34,482 >> Initializing global attention on CLS token...\n",
            " 49% 4101/8340 [1:04:04<1:00:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:35,340 >> Initializing global attention on CLS token...\n",
            " 49% 4102/8340 [1:04:04<1:00:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:36,188 >> Initializing global attention on CLS token...\n",
            " 49% 4103/8340 [1:04:05<1:00:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:37,043 >> Initializing global attention on CLS token...\n",
            " 49% 4104/8340 [1:04:06<1:00:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:37,896 >> Initializing global attention on CLS token...\n",
            " 49% 4105/8340 [1:04:07<1:00:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:38,753 >> Initializing global attention on CLS token...\n",
            " 49% 4106/8340 [1:04:08<1:00:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:39,603 >> Initializing global attention on CLS token...\n",
            " 49% 4107/8340 [1:04:09<1:00:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:40,457 >> Initializing global attention on CLS token...\n",
            " 49% 4108/8340 [1:04:10<1:00:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:41,315 >> Initializing global attention on CLS token...\n",
            " 49% 4109/8340 [1:04:10<1:00:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:42,168 >> Initializing global attention on CLS token...\n",
            " 49% 4110/8340 [1:04:11<1:00:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:43,027 >> Initializing global attention on CLS token...\n",
            " 49% 4111/8340 [1:04:12<1:00:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:43,879 >> Initializing global attention on CLS token...\n",
            " 49% 4112/8340 [1:04:13<1:00:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:44,726 >> Initializing global attention on CLS token...\n",
            " 49% 4113/8340 [1:04:14<1:00:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:45,577 >> Initializing global attention on CLS token...\n",
            " 49% 4114/8340 [1:04:15<1:00:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:46,432 >> Initializing global attention on CLS token...\n",
            " 49% 4115/8340 [1:04:16<1:00:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:47,292 >> Initializing global attention on CLS token...\n",
            " 49% 4116/8340 [1:04:16<1:00:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:48,143 >> Initializing global attention on CLS token...\n",
            " 49% 4117/8340 [1:04:17<1:00:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:48,997 >> Initializing global attention on CLS token...\n",
            " 49% 4118/8340 [1:04:18<1:00:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:49,851 >> Initializing global attention on CLS token...\n",
            " 49% 4119/8340 [1:04:19<1:00:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:50,702 >> Initializing global attention on CLS token...\n",
            " 49% 4120/8340 [1:04:20<1:00:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:51,559 >> Initializing global attention on CLS token...\n",
            " 49% 4121/8340 [1:04:21<1:00:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:52,413 >> Initializing global attention on CLS token...\n",
            " 49% 4122/8340 [1:04:22<1:00:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:53,265 >> Initializing global attention on CLS token...\n",
            " 49% 4123/8340 [1:04:22<1:00:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:54,121 >> Initializing global attention on CLS token...\n",
            " 49% 4124/8340 [1:04:23<1:00:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:54,977 >> Initializing global attention on CLS token...\n",
            " 49% 4125/8340 [1:04:24<1:00:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:55,829 >> Initializing global attention on CLS token...\n",
            " 49% 4126/8340 [1:04:25<1:00:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:56,684 >> Initializing global attention on CLS token...\n",
            " 49% 4127/8340 [1:04:26<1:00:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:57,541 >> Initializing global attention on CLS token...\n",
            " 49% 4128/8340 [1:04:27<1:00:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:58,395 >> Initializing global attention on CLS token...\n",
            " 50% 4129/8340 [1:04:27<1:00:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:02:59,251 >> Initializing global attention on CLS token...\n",
            " 50% 4130/8340 [1:04:28<59:59,  1.17it/s]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:03:00,106 >> Initializing global attention on CLS token...\n",
            " 50% 4131/8340 [1:04:29<59:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:00,961 >> Initializing global attention on CLS token...\n",
            " 50% 4132/8340 [1:04:30<1:00:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:01,821 >> Initializing global attention on CLS token...\n",
            " 50% 4133/8340 [1:04:31<59:59,  1.17it/s]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:03:02,676 >> Initializing global attention on CLS token...\n",
            " 50% 4134/8340 [1:04:32<1:00:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:03,533 >> Initializing global attention on CLS token...\n",
            " 50% 4135/8340 [1:04:33<59:52,  1.17it/s]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:03:04,383 >> Initializing global attention on CLS token...\n",
            " 50% 4136/8340 [1:04:33<59:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:05,237 >> Initializing global attention on CLS token...\n",
            " 50% 4137/8340 [1:04:34<59:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:06,093 >> Initializing global attention on CLS token...\n",
            " 50% 4138/8340 [1:04:35<59:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:06,945 >> Initializing global attention on CLS token...\n",
            " 50% 4139/8340 [1:04:36<59:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:07,803 >> Initializing global attention on CLS token...\n",
            " 50% 4140/8340 [1:04:37<59:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:08,659 >> Initializing global attention on CLS token...\n",
            " 50% 4141/8340 [1:04:38<59:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:09,515 >> Initializing global attention on CLS token...\n",
            " 50% 4142/8340 [1:04:39<59:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:10,370 >> Initializing global attention on CLS token...\n",
            " 50% 4143/8340 [1:04:39<59:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:11,224 >> Initializing global attention on CLS token...\n",
            " 50% 4144/8340 [1:04:40<59:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:12,078 >> Initializing global attention on CLS token...\n",
            " 50% 4145/8340 [1:04:41<59:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:12,933 >> Initializing global attention on CLS token...\n",
            " 50% 4146/8340 [1:04:42<59:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:13,789 >> Initializing global attention on CLS token...\n",
            " 50% 4147/8340 [1:04:43<59:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:14,643 >> Initializing global attention on CLS token...\n",
            " 50% 4148/8340 [1:04:44<59:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:15,496 >> Initializing global attention on CLS token...\n",
            " 50% 4149/8340 [1:04:45<59:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:16,346 >> Initializing global attention on CLS token...\n",
            " 50% 4150/8340 [1:04:45<59:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:17,199 >> Initializing global attention on CLS token...\n",
            " 50% 4151/8340 [1:04:46<59:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:18,054 >> Initializing global attention on CLS token...\n",
            " 50% 4152/8340 [1:04:47<59:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:18,911 >> Initializing global attention on CLS token...\n",
            " 50% 4153/8340 [1:04:48<59:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:19,761 >> Initializing global attention on CLS token...\n",
            " 50% 4154/8340 [1:04:49<59:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:20,617 >> Initializing global attention on CLS token...\n",
            " 50% 4155/8340 [1:04:50<59:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:21,469 >> Initializing global attention on CLS token...\n",
            " 50% 4156/8340 [1:04:51<59:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:22,323 >> Initializing global attention on CLS token...\n",
            " 50% 4157/8340 [1:04:51<59:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:23,179 >> Initializing global attention on CLS token...\n",
            " 50% 4158/8340 [1:04:52<59:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:24,028 >> Initializing global attention on CLS token...\n",
            " 50% 4159/8340 [1:04:53<59:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:24,900 >> Initializing global attention on CLS token...\n",
            " 50% 4160/8340 [1:04:54<59:53,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:25,774 >> Initializing global attention on CLS token...\n",
            " 50% 4161/8340 [1:04:55<1:00:08,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:26,634 >> Initializing global attention on CLS token...\n",
            " 50% 4162/8340 [1:04:56<59:56,  1.16it/s]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:03:27,488 >> Initializing global attention on CLS token...\n",
            " 50% 4163/8340 [1:04:57<59:58,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:28,361 >> Initializing global attention on CLS token...\n",
            " 50% 4164/8340 [1:04:57<1:00:06,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:29,228 >> Initializing global attention on CLS token...\n",
            " 50% 4165/8340 [1:04:58<1:00:14,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:30,093 >> Initializing global attention on CLS token...\n",
            " 50% 4166/8340 [1:04:59<1:00:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:30,950 >> Initializing global attention on CLS token...\n",
            " 50% 4167/8340 [1:05:00<59:53,  1.16it/s]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:03:31,830 >> Initializing global attention on CLS token...\n",
            " 50% 4168/8340 [1:05:01<1:00:51,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:32,741 >> Initializing global attention on CLS token...\n",
            " 50% 4169/8340 [1:05:02<1:01:26,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:03:33,606 >> Initializing global attention on CLS token...\n",
            " 50% 4170/8340 [1:05:02<50:09,  1.39it/s]  [INFO|trainer.py:726] 2022-11-21 17:03:33,931 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2022-11-21 17:03:33,934 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-11-21 17:03:33,934 >>   Num examples = 1400\n",
            "[INFO|trainer.py:2912] 2022-11-21 17:03:33,935 >>   Batch size = 6\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:33,986 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 0/234 [00:00<?, ?it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:34,298 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 2/234 [00:00<00:37,  6.23it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:34,602 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 3/234 [00:00<00:50,  4.58it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:34,897 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 4/234 [00:00<00:56,  4.10it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:35,182 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 5/234 [00:01<00:59,  3.87it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:35,484 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 6/234 [00:01<01:02,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:35,764 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 7/234 [00:01<01:02,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:36,056 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 8/234 [00:02<01:05,  3.46it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:36,372 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 9/234 [00:02<01:05,  3.42it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:36,669 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 10/234 [00:02<01:05,  3.43it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:36,962 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 11/234 [00:02<01:05,  3.43it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:37,253 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 12/234 [00:03<01:06,  3.36it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:37,629 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 13/234 [00:03<01:09,  3.16it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:37,900 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 14/234 [00:03<01:06,  3.32it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:38,186 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 15/234 [00:04<01:04,  3.38it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:38,449 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 16/234 [00:04<01:02,  3.49it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:38,715 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 17/234 [00:04<01:00,  3.57it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:38,982 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 18/234 [00:04<00:59,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:39,247 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 19/234 [00:05<00:58,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:39,508 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 20/234 [00:05<00:57,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:39,771 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 21/234 [00:05<00:57,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:40,054 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 22/234 [00:06<00:57,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:40,322 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 23/234 [00:06<00:56,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:40,581 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 24/234 [00:06<00:56,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:40,850 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 25/234 [00:06<00:55,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:41,128 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 26/234 [00:07<00:56,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:41,402 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 27/234 [00:07<00:55,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:41,662 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 28/234 [00:07<00:55,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:41,945 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 29/234 [00:07<00:55,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:42,209 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 30/234 [00:08<00:55,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:42,497 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 31/234 [00:08<00:55,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:42,764 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 32/234 [00:08<00:55,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:43,026 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 33/234 [00:09<00:54,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:43,306 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 34/234 [00:09<00:54,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:43,569 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 35/234 [00:09<00:53,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:43,838 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 36/234 [00:09<00:53,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:44,114 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 37/234 [00:10<00:53,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:44,407 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 38/234 [00:10<00:54,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:44,693 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 39/234 [00:10<00:54,  3.57it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:44,977 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 40/234 [00:10<00:54,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:45,246 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 41/234 [00:11<00:53,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:45,511 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 42/234 [00:11<00:52,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:45,784 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 43/234 [00:11<00:52,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:46,052 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 44/234 [00:12<00:51,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:46,342 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 45/234 [00:12<00:53,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:46,641 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 46/234 [00:12<00:53,  3.54it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:46,904 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 47/234 [00:12<00:51,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:47,168 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 48/234 [00:13<00:50,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:47,435 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 49/234 [00:13<00:50,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:47,707 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 50/234 [00:13<00:50,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:47,994 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 51/234 [00:14<00:50,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:48,275 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 52/234 [00:14<00:50,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:48,560 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 53/234 [00:14<00:50,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:48,839 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 54/234 [00:14<00:50,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:49,107 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 55/234 [00:15<00:49,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:49,370 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 56/234 [00:15<00:48,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:49,644 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 57/234 [00:15<00:48,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:49,916 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 58/234 [00:15<00:47,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:50,196 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 59/234 [00:16<00:48,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:50,480 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 60/234 [00:16<00:48,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:50,767 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 61/234 [00:16<00:48,  3.57it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:51,051 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 62/234 [00:17<00:48,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:51,317 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 63/234 [00:17<00:47,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:51,579 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 64/234 [00:17<00:46,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:51,843 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 65/234 [00:17<00:45,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:52,108 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 66/234 [00:18<00:44,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:52,376 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 67/234 [00:18<00:44,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:52,636 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 68/234 [00:18<00:44,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:52,899 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 69/234 [00:18<00:43,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:53,164 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 70/234 [00:19<00:43,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:53,428 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 71/234 [00:19<00:43,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:53,692 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 72/234 [00:19<00:42,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:53,955 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 73/234 [00:19<00:42,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:54,229 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 74/234 [00:20<00:42,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:54,495 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 75/234 [00:20<00:42,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:54,761 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 76/234 [00:20<00:42,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:55,027 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 77/234 [00:21<00:41,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:55,295 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 78/234 [00:21<00:41,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:55,557 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 79/234 [00:21<00:41,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:55,822 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 80/234 [00:21<00:40,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:56,090 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 81/234 [00:22<00:40,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:56,359 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 82/234 [00:22<00:40,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:56,620 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 83/234 [00:22<00:40,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:56,883 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 84/234 [00:22<00:39,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:57,152 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 85/234 [00:23<00:39,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:57,425 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 86/234 [00:23<00:39,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:57,692 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 87/234 [00:23<00:39,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:57,958 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 88/234 [00:23<00:38,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:58,224 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 89/234 [00:24<00:38,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:58,492 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 90/234 [00:24<00:38,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:58,753 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 91/234 [00:24<00:37,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:59,019 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 92/234 [00:25<00:37,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:59,284 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 93/234 [00:25<00:37,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:59,547 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 94/234 [00:25<00:37,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:03:59,809 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 95/234 [00:25<00:36,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:00,078 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 96/234 [00:26<00:36,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:00,347 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 97/234 [00:26<00:36,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:00,621 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 98/234 [00:26<00:36,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:00,886 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 99/234 [00:26<00:36,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:01,155 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 100/234 [00:27<00:35,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:01,435 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 101/234 [00:27<00:36,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:01,722 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 102/234 [00:27<00:36,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:01,984 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 103/234 [00:27<00:35,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:02,253 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 104/234 [00:28<00:35,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:02,522 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 105/234 [00:28<00:34,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:02,806 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 106/234 [00:28<00:35,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:03,078 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 107/234 [00:29<00:34,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:03,345 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 108/234 [00:29<00:34,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:03,614 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 109/234 [00:29<00:33,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:03,891 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 110/234 [00:29<00:33,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:04,156 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 111/234 [00:30<00:33,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:04,422 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 112/234 [00:30<00:32,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:04,689 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 113/234 [00:30<00:32,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:04,955 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 114/234 [00:30<00:32,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:05,229 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 115/234 [00:31<00:31,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:05,490 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 116/234 [00:31<00:31,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:05,751 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 117/234 [00:31<00:31,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:06,016 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 118/234 [00:32<00:30,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:06,281 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 119/234 [00:32<00:30,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:06,550 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 120/234 [00:32<00:30,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:06,813 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 121/234 [00:32<00:29,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:07,073 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 122/234 [00:33<00:29,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:07,344 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 123/234 [00:33<00:29,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:07,609 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 124/234 [00:33<00:29,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:07,870 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 125/234 [00:33<00:28,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:08,132 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 126/234 [00:34<00:28,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:08,396 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 127/234 [00:34<00:28,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:08,669 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 128/234 [00:34<00:28,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:08,933 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 129/234 [00:34<00:27,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:09,193 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 130/234 [00:35<00:27,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:09,458 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 131/234 [00:35<00:27,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:09,723 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 132/234 [00:35<00:26,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:09,989 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 133/234 [00:35<00:26,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:10,272 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 134/234 [00:36<00:26,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:10,553 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 135/234 [00:36<00:27,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:10,826 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 136/234 [00:36<00:26,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:11,107 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 137/234 [00:37<00:26,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:11,392 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 138/234 [00:37<00:26,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:11,657 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 139/234 [00:37<00:26,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:11,940 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 140/234 [00:37<00:26,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:12,202 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 141/234 [00:38<00:25,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:12,484 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 142/234 [00:38<00:25,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:12,755 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 143/234 [00:38<00:24,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:13,036 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 144/234 [00:39<00:24,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:13,320 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 145/234 [00:39<00:24,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:13,604 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 146/234 [00:39<00:24,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:13,886 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 147/234 [00:39<00:24,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:14,167 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 148/234 [00:40<00:24,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:14,438 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 149/234 [00:40<00:23,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:14,704 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 150/234 [00:40<00:22,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:14,972 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 151/234 [00:40<00:22,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:15,235 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 152/234 [00:41<00:22,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:15,502 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 153/234 [00:41<00:21,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:15,759 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 154/234 [00:41<00:21,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:16,029 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 155/234 [00:42<00:21,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:16,296 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 156/234 [00:42<00:20,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:16,566 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 157/234 [00:42<00:20,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:16,832 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 158/234 [00:42<00:20,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:17,099 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 159/234 [00:43<00:20,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:17,367 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 160/234 [00:43<00:19,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:17,633 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 161/234 [00:43<00:19,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:17,899 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 162/234 [00:43<00:19,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:18,166 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 163/234 [00:44<00:18,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:18,437 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 164/234 [00:44<00:18,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:18,698 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 165/234 [00:44<00:18,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:18,962 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 166/234 [00:44<00:18,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:19,228 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 167/234 [00:45<00:17,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:19,493 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 168/234 [00:45<00:17,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:19,755 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 169/234 [00:45<00:17,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:20,017 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 170/234 [00:46<00:16,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:20,280 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 171/234 [00:46<00:16,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:20,550 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 172/234 [00:46<00:16,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:20,812 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 173/234 [00:46<00:16,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:21,074 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 174/234 [00:47<00:15,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:21,335 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 175/234 [00:47<00:15,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:21,602 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 176/234 [00:47<00:15,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:21,873 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 177/234 [00:47<00:15,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:22,140 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 178/234 [00:48<00:14,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:22,404 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 179/234 [00:48<00:14,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:22,674 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 180/234 [00:48<00:14,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:22,937 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 181/234 [00:48<00:14,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:23,205 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 182/234 [00:49<00:13,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:23,469 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 183/234 [00:49<00:13,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:23,733 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 184/234 [00:49<00:13,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:23,997 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 185/234 [00:50<00:12,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:24,263 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 186/234 [00:50<00:12,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:24,526 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 187/234 [00:50<00:12,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:24,789 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 188/234 [00:50<00:12,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:25,053 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 189/234 [00:51<00:11,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:25,337 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 190/234 [00:51<00:11,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:25,620 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 191/234 [00:51<00:11,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:25,889 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 192/234 [00:51<00:11,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:26,149 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 193/234 [00:52<00:11,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:26,433 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 194/234 [00:52<00:10,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:26,712 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 195/234 [00:52<00:10,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:26,979 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 196/234 [00:52<00:10,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:27,262 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 197/234 [00:53<00:10,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:27,529 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 198/234 [00:53<00:09,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:27,794 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 199/234 [00:53<00:09,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:28,057 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 200/234 [00:54<00:09,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:28,328 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 201/234 [00:54<00:08,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:28,596 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 202/234 [00:54<00:08,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:28,862 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 203/234 [00:54<00:08,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:29,133 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 204/234 [00:55<00:08,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:29,418 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 205/234 [00:55<00:07,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:29,686 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 206/234 [00:55<00:07,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:29,968 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 207/234 [00:55<00:07,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:30,245 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 208/234 [00:56<00:07,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:30,521 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 209/234 [00:56<00:06,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:30,834 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 210/234 [00:56<00:06,  3.43it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:31,150 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 211/234 [00:57<00:06,  3.41it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:31,415 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 212/234 [00:57<00:06,  3.51it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:31,695 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 213/234 [00:57<00:05,  3.52it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:31,975 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 214/234 [00:57<00:05,  3.51it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:32,248 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 215/234 [00:58<00:05,  3.58it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:32,513 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 216/234 [00:58<00:04,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:32,795 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 217/234 [00:58<00:04,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:33,062 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 218/234 [00:59<00:04,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:33,339 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 219/234 [00:59<00:04,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:33,626 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 220/234 [00:59<00:03,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:33,908 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 221/234 [00:59<00:03,  3.57it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:34,178 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 222/234 [01:00<00:03,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:34,446 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 223/234 [01:00<00:03,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:34,711 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 224/234 [01:00<00:02,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:34,990 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 225/234 [01:00<00:02,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:35,271 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 226/234 [01:01<00:02,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:35,536 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 227/234 [01:01<00:01,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:35,802 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 228/234 [01:01<00:01,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:36,066 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 229/234 [01:02<00:01,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:36,342 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 230/234 [01:02<00:01,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:36,625 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 231/234 [01:02<00:00,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:36,912 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 232/234 [01:02<00:00,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:37,177 >> Initializing global attention on CLS token...\n",
            "\n",
            "100% 233/234 [01:03<00:00,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:37,430 >> Initializing global attention on CLS token...\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 1.8034472465515137, 'eval_f1-micro': 0.7250000000000001, 'eval_f1-macro': 0.6058226781271041, 'eval_accuracy': 0.725, 'eval_runtime': 65.6657, 'eval_samples_per_second': 21.32, 'eval_steps_per_second': 3.564, 'epoch': 5.0}\n",
            " 50% 4170/8340 [1:06:08<50:09,  1.39it/s]\n",
            "100% 234/234 [01:05<00:00,  4.47it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-11-21 17:04:39,603 >> Saving model checkpoint to logs/output_1/checkpoint-4170\n",
            "[INFO|configuration_utils.py:447] 2022-11-21 17:04:39,605 >> Configuration saved in logs/output_1/checkpoint-4170/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-21 17:04:39,963 >> Model weights saved in logs/output_1/checkpoint-4170/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-21 17:04:39,964 >> tokenizer config file saved in logs/output_1/checkpoint-4170/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-21 17:04:39,965 >> Special tokens file saved in logs/output_1/checkpoint-4170/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-21 17:04:42,990 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-21 17:04:42,992 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-21 17:04:59,095 >> Initializing global attention on CLS token...\n",
            " 50% 4171/8340 [1:06:28<30:29:08, 26.32s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:00,024 >> Initializing global attention on CLS token...\n",
            " 50% 4172/8340 [1:06:29<21:37:46, 18.68s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:00,873 >> Initializing global attention on CLS token...\n",
            " 50% 4173/8340 [1:06:30<15:25:44, 13.33s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:01,713 >> Initializing global attention on CLS token...\n",
            " 50% 4174/8340 [1:06:31<11:05:32,  9.59s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:02,561 >> Initializing global attention on CLS token...\n",
            " 50% 4175/8340 [1:06:32<8:03:26,  6.96s/it] [INFO|modeling_longformer.py:1932] 2022-11-21 17:05:03,412 >> Initializing global attention on CLS token...\n",
            " 50% 4176/8340 [1:06:32<5:55:55,  5.13s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:04,257 >> Initializing global attention on CLS token...\n",
            " 50% 4177/8340 [1:06:33<4:26:52,  3.85s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:05,114 >> Initializing global attention on CLS token...\n",
            " 50% 4178/8340 [1:06:34<3:24:33,  2.95s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:05,968 >> Initializing global attention on CLS token...\n",
            " 50% 4179/8340 [1:06:35<2:40:57,  2.32s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:06,824 >> Initializing global attention on CLS token...\n",
            " 50% 4180/8340 [1:06:36<2:10:27,  1.88s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:07,683 >> Initializing global attention on CLS token...\n",
            " 50% 4181/8340 [1:06:37<1:49:10,  1.58s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:08,539 >> Initializing global attention on CLS token...\n",
            " 50% 4182/8340 [1:06:38<1:34:06,  1.36s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:09,391 >> Initializing global attention on CLS token...\n",
            " 50% 4183/8340 [1:06:38<1:23:35,  1.21s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:10,243 >> Initializing global attention on CLS token...\n",
            " 50% 4184/8340 [1:06:39<1:16:14,  1.10s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:11,097 >> Initializing global attention on CLS token...\n",
            " 50% 4185/8340 [1:06:40<1:11:11,  1.03s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:11,955 >> Initializing global attention on CLS token...\n",
            " 50% 4186/8340 [1:06:41<1:07:31,  1.03it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:12,813 >> Initializing global attention on CLS token...\n",
            " 50% 4187/8340 [1:06:42<1:05:02,  1.06it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:13,667 >> Initializing global attention on CLS token...\n",
            " 50% 4188/8340 [1:06:43<1:03:16,  1.09it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:14,520 >> Initializing global attention on CLS token...\n",
            " 50% 4189/8340 [1:06:44<1:02:15,  1.11it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:15,389 >> Initializing global attention on CLS token...\n",
            " 50% 4190/8340 [1:06:44<1:01:36,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:16,256 >> Initializing global attention on CLS token...\n",
            " 50% 4191/8340 [1:06:45<1:01:01,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:17,117 >> Initializing global attention on CLS token...\n",
            " 50% 4192/8340 [1:06:46<1:00:21,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:17,971 >> Initializing global attention on CLS token...\n",
            " 50% 4193/8340 [1:06:47<1:00:07,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:18,831 >> Initializing global attention on CLS token...\n",
            " 50% 4194/8340 [1:06:48<59:51,  1.15it/s]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:05:19,692 >> Initializing global attention on CLS token...\n",
            " 50% 4195/8340 [1:06:49<59:39,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:20,548 >> Initializing global attention on CLS token...\n",
            " 50% 4196/8340 [1:06:50<59:53,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:21,425 >> Initializing global attention on CLS token...\n",
            " 50% 4197/8340 [1:06:51<59:56,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:22,296 >> Initializing global attention on CLS token...\n",
            " 50% 4198/8340 [1:06:51<59:49,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:23,156 >> Initializing global attention on CLS token...\n",
            " 50% 4199/8340 [1:06:52<1:00:03,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:24,037 >> Initializing global attention on CLS token...\n",
            " 50% 4200/8340 [1:06:53<1:00:08,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:24,913 >> Initializing global attention on CLS token...\n",
            " 50% 4201/8340 [1:06:54<1:00:17,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:25,799 >> Initializing global attention on CLS token...\n",
            " 50% 4202/8340 [1:06:55<1:00:15,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:26,662 >> Initializing global attention on CLS token...\n",
            " 50% 4203/8340 [1:06:56<1:00:06,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:27,529 >> Initializing global attention on CLS token...\n",
            " 50% 4204/8340 [1:06:57<59:58,  1.15it/s]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:05:28,397 >> Initializing global attention on CLS token...\n",
            " 50% 4205/8340 [1:06:58<59:58,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:29,267 >> Initializing global attention on CLS token...\n",
            " 50% 4206/8340 [1:06:58<59:54,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:30,136 >> Initializing global attention on CLS token...\n",
            " 50% 4207/8340 [1:06:59<59:52,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:31,002 >> Initializing global attention on CLS token...\n",
            " 50% 4208/8340 [1:07:00<59:47,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:31,868 >> Initializing global attention on CLS token...\n",
            " 50% 4209/8340 [1:07:01<59:41,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:32,734 >> Initializing global attention on CLS token...\n",
            " 50% 4210/8340 [1:07:02<59:37,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:33,598 >> Initializing global attention on CLS token...\n",
            " 50% 4211/8340 [1:07:03<59:36,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:34,466 >> Initializing global attention on CLS token...\n",
            " 51% 4212/8340 [1:07:04<59:34,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:35,330 >> Initializing global attention on CLS token...\n",
            " 51% 4213/8340 [1:07:04<59:41,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:36,201 >> Initializing global attention on CLS token...\n",
            " 51% 4214/8340 [1:07:05<59:38,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:37,067 >> Initializing global attention on CLS token...\n",
            " 51% 4215/8340 [1:07:06<59:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:37,922 >> Initializing global attention on CLS token...\n",
            " 51% 4216/8340 [1:07:07<59:14,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:38,780 >> Initializing global attention on CLS token...\n",
            " 51% 4217/8340 [1:07:08<59:07,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:39,635 >> Initializing global attention on CLS token...\n",
            " 51% 4218/8340 [1:07:09<59:04,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:40,497 >> Initializing global attention on CLS token...\n",
            " 51% 4219/8340 [1:07:10<59:04,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:41,356 >> Initializing global attention on CLS token...\n",
            " 51% 4220/8340 [1:07:10<59:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:42,221 >> Initializing global attention on CLS token...\n",
            " 51% 4221/8340 [1:07:11<59:14,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:43,090 >> Initializing global attention on CLS token...\n",
            " 51% 4222/8340 [1:07:12<59:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:43,951 >> Initializing global attention on CLS token...\n",
            " 51% 4223/8340 [1:07:13<59:13,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:44,815 >> Initializing global attention on CLS token...\n",
            " 51% 4224/8340 [1:07:14<59:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:45,674 >> Initializing global attention on CLS token...\n",
            " 51% 4225/8340 [1:07:15<59:03,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:46,534 >> Initializing global attention on CLS token...\n",
            " 51% 4226/8340 [1:07:16<59:00,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:47,392 >> Initializing global attention on CLS token...\n",
            " 51% 4227/8340 [1:07:16<59:02,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:48,255 >> Initializing global attention on CLS token...\n",
            " 51% 4228/8340 [1:07:17<58:54,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:49,112 >> Initializing global attention on CLS token...\n",
            " 51% 4229/8340 [1:07:18<58:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:49,966 >> Initializing global attention on CLS token...\n",
            " 51% 4230/8340 [1:07:19<58:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:50,818 >> Initializing global attention on CLS token...\n",
            " 51% 4231/8340 [1:07:20<58:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:51,672 >> Initializing global attention on CLS token...\n",
            " 51% 4232/8340 [1:07:21<58:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:52,521 >> Initializing global attention on CLS token...\n",
            " 51% 4233/8340 [1:07:22<58:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:53,380 >> Initializing global attention on CLS token...\n",
            " 51% 4234/8340 [1:07:22<58:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:54,236 >> Initializing global attention on CLS token...\n",
            " 51% 4235/8340 [1:07:23<58:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:55,089 >> Initializing global attention on CLS token...\n",
            " 51% 4236/8340 [1:07:24<58:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:55,938 >> Initializing global attention on CLS token...\n",
            " 51% 4237/8340 [1:07:25<58:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:56,796 >> Initializing global attention on CLS token...\n",
            " 51% 4238/8340 [1:07:26<58:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:57,654 >> Initializing global attention on CLS token...\n",
            " 51% 4239/8340 [1:07:27<58:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:58,506 >> Initializing global attention on CLS token...\n",
            " 51% 4240/8340 [1:07:28<58:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:05:59,366 >> Initializing global attention on CLS token...\n",
            " 51% 4241/8340 [1:07:28<58:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:00,214 >> Initializing global attention on CLS token...\n",
            " 51% 4242/8340 [1:07:29<58:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:01,068 >> Initializing global attention on CLS token...\n",
            " 51% 4243/8340 [1:07:30<58:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:01,922 >> Initializing global attention on CLS token...\n",
            " 51% 4244/8340 [1:07:31<58:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:02,775 >> Initializing global attention on CLS token...\n",
            " 51% 4245/8340 [1:07:32<58:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:03,633 >> Initializing global attention on CLS token...\n",
            " 51% 4246/8340 [1:07:33<58:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:04,486 >> Initializing global attention on CLS token...\n",
            " 51% 4247/8340 [1:07:34<58:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:05,337 >> Initializing global attention on CLS token...\n",
            " 51% 4248/8340 [1:07:34<58:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:06,197 >> Initializing global attention on CLS token...\n",
            " 51% 4249/8340 [1:07:35<58:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:07,047 >> Initializing global attention on CLS token...\n",
            " 51% 4250/8340 [1:07:36<58:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:07,899 >> Initializing global attention on CLS token...\n",
            " 51% 4251/8340 [1:07:37<58:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:08,754 >> Initializing global attention on CLS token...\n",
            " 51% 4252/8340 [1:07:38<58:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:09,609 >> Initializing global attention on CLS token...\n",
            " 51% 4253/8340 [1:07:39<58:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:10,462 >> Initializing global attention on CLS token...\n",
            " 51% 4254/8340 [1:07:40<58:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:11,314 >> Initializing global attention on CLS token...\n",
            " 51% 4255/8340 [1:07:40<58:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:12,168 >> Initializing global attention on CLS token...\n",
            " 51% 4256/8340 [1:07:41<58:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:13,024 >> Initializing global attention on CLS token...\n",
            " 51% 4257/8340 [1:07:42<58:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:13,880 >> Initializing global attention on CLS token...\n",
            " 51% 4258/8340 [1:07:43<58:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:14,735 >> Initializing global attention on CLS token...\n",
            " 51% 4259/8340 [1:07:44<58:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:15,588 >> Initializing global attention on CLS token...\n",
            " 51% 4260/8340 [1:07:45<58:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:16,447 >> Initializing global attention on CLS token...\n",
            " 51% 4261/8340 [1:07:46<58:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:17,302 >> Initializing global attention on CLS token...\n",
            " 51% 4262/8340 [1:07:46<58:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:18,156 >> Initializing global attention on CLS token...\n",
            " 51% 4263/8340 [1:07:47<58:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:19,011 >> Initializing global attention on CLS token...\n",
            " 51% 4264/8340 [1:07:48<58:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:19,863 >> Initializing global attention on CLS token...\n",
            " 51% 4265/8340 [1:07:49<58:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:20,721 >> Initializing global attention on CLS token...\n",
            " 51% 4266/8340 [1:07:50<58:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:21,576 >> Initializing global attention on CLS token...\n",
            " 51% 4267/8340 [1:07:51<58:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:22,432 >> Initializing global attention on CLS token...\n",
            " 51% 4268/8340 [1:07:52<58:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:23,289 >> Initializing global attention on CLS token...\n",
            " 51% 4269/8340 [1:07:52<58:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:24,141 >> Initializing global attention on CLS token...\n",
            " 51% 4270/8340 [1:07:53<57:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:24,996 >> Initializing global attention on CLS token...\n",
            " 51% 4271/8340 [1:07:54<58:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:25,854 >> Initializing global attention on CLS token...\n",
            " 51% 4272/8340 [1:07:55<57:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:26,709 >> Initializing global attention on CLS token...\n",
            " 51% 4273/8340 [1:07:56<57:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:27,560 >> Initializing global attention on CLS token...\n",
            " 51% 4274/8340 [1:07:57<57:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:28,415 >> Initializing global attention on CLS token...\n",
            " 51% 4275/8340 [1:07:58<57:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:29,273 >> Initializing global attention on CLS token...\n",
            " 51% 4276/8340 [1:07:58<57:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:30,127 >> Initializing global attention on CLS token...\n",
            " 51% 4277/8340 [1:07:59<57:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:30,983 >> Initializing global attention on CLS token...\n",
            " 51% 4278/8340 [1:08:00<57:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:31,836 >> Initializing global attention on CLS token...\n",
            " 51% 4279/8340 [1:08:01<57:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:32,687 >> Initializing global attention on CLS token...\n",
            " 51% 4280/8340 [1:08:02<57:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:33,549 >> Initializing global attention on CLS token...\n",
            " 51% 4281/8340 [1:08:03<57:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:34,408 >> Initializing global attention on CLS token...\n",
            " 51% 4282/8340 [1:08:03<57:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:35,255 >> Initializing global attention on CLS token...\n",
            " 51% 4283/8340 [1:08:04<57:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:36,110 >> Initializing global attention on CLS token...\n",
            " 51% 4284/8340 [1:08:05<57:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:36,971 >> Initializing global attention on CLS token...\n",
            " 51% 4285/8340 [1:08:06<57:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:37,820 >> Initializing global attention on CLS token...\n",
            " 51% 4286/8340 [1:08:07<57:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:38,675 >> Initializing global attention on CLS token...\n",
            " 51% 4287/8340 [1:08:08<57:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:39,528 >> Initializing global attention on CLS token...\n",
            " 51% 4288/8340 [1:08:09<57:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:40,388 >> Initializing global attention on CLS token...\n",
            " 51% 4289/8340 [1:08:09<57:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:41,241 >> Initializing global attention on CLS token...\n",
            " 51% 4290/8340 [1:08:10<57:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:42,096 >> Initializing global attention on CLS token...\n",
            " 51% 4291/8340 [1:08:11<57:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:42,954 >> Initializing global attention on CLS token...\n",
            " 51% 4292/8340 [1:08:12<57:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:43,815 >> Initializing global attention on CLS token...\n",
            " 51% 4293/8340 [1:08:13<57:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:44,673 >> Initializing global attention on CLS token...\n",
            " 51% 4294/8340 [1:08:14<57:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:45,533 >> Initializing global attention on CLS token...\n",
            " 51% 4295/8340 [1:08:15<57:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:46,384 >> Initializing global attention on CLS token...\n",
            " 52% 4296/8340 [1:08:15<57:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:47,240 >> Initializing global attention on CLS token...\n",
            " 52% 4297/8340 [1:08:16<57:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:48,102 >> Initializing global attention on CLS token...\n",
            " 52% 4298/8340 [1:08:17<57:52,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:48,966 >> Initializing global attention on CLS token...\n",
            " 52% 4299/8340 [1:08:18<57:53,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:49,825 >> Initializing global attention on CLS token...\n",
            " 52% 4300/8340 [1:08:19<57:52,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:50,683 >> Initializing global attention on CLS token...\n",
            " 52% 4301/8340 [1:08:20<57:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:51,545 >> Initializing global attention on CLS token...\n",
            " 52% 4302/8340 [1:08:21<57:52,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:52,403 >> Initializing global attention on CLS token...\n",
            " 52% 4303/8340 [1:08:22<57:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:53,262 >> Initializing global attention on CLS token...\n",
            " 52% 4304/8340 [1:08:22<57:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:54,119 >> Initializing global attention on CLS token...\n",
            " 52% 4305/8340 [1:08:23<57:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:54,979 >> Initializing global attention on CLS token...\n",
            " 52% 4306/8340 [1:08:24<57:48,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:55,840 >> Initializing global attention on CLS token...\n",
            " 52% 4307/8340 [1:08:25<57:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:56,697 >> Initializing global attention on CLS token...\n",
            " 52% 4308/8340 [1:08:26<57:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:57,548 >> Initializing global attention on CLS token...\n",
            " 52% 4309/8340 [1:08:27<57:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:58,402 >> Initializing global attention on CLS token...\n",
            " 52% 4310/8340 [1:08:28<57:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:06:59,258 >> Initializing global attention on CLS token...\n",
            " 52% 4311/8340 [1:08:28<57:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:00,113 >> Initializing global attention on CLS token...\n",
            " 52% 4312/8340 [1:08:29<57:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:00,967 >> Initializing global attention on CLS token...\n",
            " 52% 4313/8340 [1:08:30<57:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:01,821 >> Initializing global attention on CLS token...\n",
            " 52% 4314/8340 [1:08:31<57:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:02,673 >> Initializing global attention on CLS token...\n",
            " 52% 4315/8340 [1:08:32<57:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:03,531 >> Initializing global attention on CLS token...\n",
            " 52% 4316/8340 [1:08:33<57:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:04,388 >> Initializing global attention on CLS token...\n",
            " 52% 4317/8340 [1:08:33<57:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:05,244 >> Initializing global attention on CLS token...\n",
            " 52% 4318/8340 [1:08:34<57:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:06,106 >> Initializing global attention on CLS token...\n",
            " 52% 4319/8340 [1:08:35<57:36,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:06,972 >> Initializing global attention on CLS token...\n",
            " 52% 4320/8340 [1:08:36<57:35,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:07,830 >> Initializing global attention on CLS token...\n",
            " 52% 4321/8340 [1:08:37<57:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:08,688 >> Initializing global attention on CLS token...\n",
            " 52% 4322/8340 [1:08:38<57:31,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:09,549 >> Initializing global attention on CLS token...\n",
            " 52% 4323/8340 [1:08:39<57:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:10,408 >> Initializing global attention on CLS token...\n",
            " 52% 4324/8340 [1:08:40<57:31,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:11,268 >> Initializing global attention on CLS token...\n",
            " 52% 4325/8340 [1:08:40<57:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:12,129 >> Initializing global attention on CLS token...\n",
            " 52% 4326/8340 [1:08:41<57:29,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:12,988 >> Initializing global attention on CLS token...\n",
            " 52% 4327/8340 [1:08:42<57:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:13,837 >> Initializing global attention on CLS token...\n",
            " 52% 4328/8340 [1:08:43<57:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:14,690 >> Initializing global attention on CLS token...\n",
            " 52% 4329/8340 [1:08:44<57:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:15,547 >> Initializing global attention on CLS token...\n",
            " 52% 4330/8340 [1:08:45<57:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:16,402 >> Initializing global attention on CLS token...\n",
            " 52% 4331/8340 [1:08:45<57:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:17,258 >> Initializing global attention on CLS token...\n",
            " 52% 4332/8340 [1:08:46<57:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:18,115 >> Initializing global attention on CLS token...\n",
            " 52% 4333/8340 [1:08:47<57:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:18,972 >> Initializing global attention on CLS token...\n",
            " 52% 4334/8340 [1:08:48<57:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:19,829 >> Initializing global attention on CLS token...\n",
            " 52% 4335/8340 [1:08:49<57:20,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:20,693 >> Initializing global attention on CLS token...\n",
            " 52% 4336/8340 [1:08:50<57:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:21,546 >> Initializing global attention on CLS token...\n",
            " 52% 4337/8340 [1:08:51<57:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:22,407 >> Initializing global attention on CLS token...\n",
            " 52% 4338/8340 [1:08:52<57:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:23,263 >> Initializing global attention on CLS token...\n",
            " 52% 4339/8340 [1:08:52<57:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:24,113 >> Initializing global attention on CLS token...\n",
            " 52% 4340/8340 [1:08:53<56:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:24,967 >> Initializing global attention on CLS token...\n",
            " 52% 4341/8340 [1:08:54<56:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:25,820 >> Initializing global attention on CLS token...\n",
            " 52% 4342/8340 [1:08:55<56:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:26,674 >> Initializing global attention on CLS token...\n",
            " 52% 4343/8340 [1:08:56<56:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:27,530 >> Initializing global attention on CLS token...\n",
            " 52% 4344/8340 [1:08:57<56:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:28,387 >> Initializing global attention on CLS token...\n",
            " 52% 4345/8340 [1:08:57<57:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:29,243 >> Initializing global attention on CLS token...\n",
            " 52% 4346/8340 [1:08:58<56:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:30,093 >> Initializing global attention on CLS token...\n",
            " 52% 4347/8340 [1:08:59<56:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:30,950 >> Initializing global attention on CLS token...\n",
            " 52% 4348/8340 [1:09:00<56:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:31,802 >> Initializing global attention on CLS token...\n",
            " 52% 4349/8340 [1:09:01<56:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:32,655 >> Initializing global attention on CLS token...\n",
            " 52% 4350/8340 [1:09:02<56:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:33,504 >> Initializing global attention on CLS token...\n",
            " 52% 4351/8340 [1:09:03<56:34,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:34,353 >> Initializing global attention on CLS token...\n",
            " 52% 4352/8340 [1:09:03<56:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:35,208 >> Initializing global attention on CLS token...\n",
            " 52% 4353/8340 [1:09:04<56:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:36,060 >> Initializing global attention on CLS token...\n",
            " 52% 4354/8340 [1:09:05<56:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:36,915 >> Initializing global attention on CLS token...\n",
            " 52% 4355/8340 [1:09:06<56:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:37,771 >> Initializing global attention on CLS token...\n",
            " 52% 4356/8340 [1:09:07<56:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:38,630 >> Initializing global attention on CLS token...\n",
            " 52% 4357/8340 [1:09:08<56:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:39,485 >> Initializing global attention on CLS token...\n",
            " 52% 4358/8340 [1:09:09<56:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:40,340 >> Initializing global attention on CLS token...\n",
            " 52% 4359/8340 [1:09:09<56:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:41,194 >> Initializing global attention on CLS token...\n",
            " 52% 4360/8340 [1:09:10<56:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:42,049 >> Initializing global attention on CLS token...\n",
            " 52% 4361/8340 [1:09:11<56:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:42,904 >> Initializing global attention on CLS token...\n",
            " 52% 4362/8340 [1:09:12<56:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:43,757 >> Initializing global attention on CLS token...\n",
            " 52% 4363/8340 [1:09:13<56:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:44,613 >> Initializing global attention on CLS token...\n",
            " 52% 4364/8340 [1:09:14<56:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:45,466 >> Initializing global attention on CLS token...\n",
            " 52% 4365/8340 [1:09:15<56:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:46,321 >> Initializing global attention on CLS token...\n",
            " 52% 4366/8340 [1:09:15<56:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:47,174 >> Initializing global attention on CLS token...\n",
            " 52% 4367/8340 [1:09:16<56:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:48,032 >> Initializing global attention on CLS token...\n",
            " 52% 4368/8340 [1:09:17<56:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:48,883 >> Initializing global attention on CLS token...\n",
            " 52% 4369/8340 [1:09:18<56:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:49,736 >> Initializing global attention on CLS token...\n",
            " 52% 4370/8340 [1:09:19<56:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:50,588 >> Initializing global attention on CLS token...\n",
            " 52% 4371/8340 [1:09:20<56:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:51,458 >> Initializing global attention on CLS token...\n",
            " 52% 4372/8340 [1:09:21<56:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:52,317 >> Initializing global attention on CLS token...\n",
            " 52% 4373/8340 [1:09:21<56:46,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:53,184 >> Initializing global attention on CLS token...\n",
            " 52% 4374/8340 [1:09:22<56:58,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:54,068 >> Initializing global attention on CLS token...\n",
            " 52% 4375/8340 [1:09:23<57:18,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:54,925 >> Initializing global attention on CLS token...\n",
            " 52% 4376/8340 [1:09:24<57:00,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:55,784 >> Initializing global attention on CLS token...\n",
            " 52% 4377/8340 [1:09:25<56:50,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:56,634 >> Initializing global attention on CLS token...\n",
            " 52% 4378/8340 [1:09:26<56:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:57,483 >> Initializing global attention on CLS token...\n",
            " 53% 4379/8340 [1:09:27<56:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:58,337 >> Initializing global attention on CLS token...\n",
            " 53% 4380/8340 [1:09:27<56:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:07:59,191 >> Initializing global attention on CLS token...\n",
            " 53% 4381/8340 [1:09:28<56:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:00,047 >> Initializing global attention on CLS token...\n",
            " 53% 4382/8340 [1:09:29<56:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:00,899 >> Initializing global attention on CLS token...\n",
            " 53% 4383/8340 [1:09:30<56:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:01,753 >> Initializing global attention on CLS token...\n",
            " 53% 4384/8340 [1:09:31<56:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:02,610 >> Initializing global attention on CLS token...\n",
            " 53% 4385/8340 [1:09:32<56:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:03,462 >> Initializing global attention on CLS token...\n",
            " 53% 4386/8340 [1:09:33<56:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:04,316 >> Initializing global attention on CLS token...\n",
            " 53% 4387/8340 [1:09:33<56:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:05,168 >> Initializing global attention on CLS token...\n",
            " 53% 4388/8340 [1:09:34<56:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:06,021 >> Initializing global attention on CLS token...\n",
            " 53% 4389/8340 [1:09:35<56:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:06,876 >> Initializing global attention on CLS token...\n",
            " 53% 4390/8340 [1:09:36<56:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:07,735 >> Initializing global attention on CLS token...\n",
            " 53% 4391/8340 [1:09:37<56:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:08,584 >> Initializing global attention on CLS token...\n",
            " 53% 4392/8340 [1:09:38<56:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:09,440 >> Initializing global attention on CLS token...\n",
            " 53% 4393/8340 [1:09:39<56:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:10,295 >> Initializing global attention on CLS token...\n",
            " 53% 4394/8340 [1:09:39<56:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:11,148 >> Initializing global attention on CLS token...\n",
            " 53% 4395/8340 [1:09:40<56:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:11,998 >> Initializing global attention on CLS token...\n",
            " 53% 4396/8340 [1:09:41<56:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:12,852 >> Initializing global attention on CLS token...\n",
            " 53% 4397/8340 [1:09:42<56:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:13,706 >> Initializing global attention on CLS token...\n",
            " 53% 4398/8340 [1:09:43<56:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:14,562 >> Initializing global attention on CLS token...\n",
            " 53% 4399/8340 [1:09:44<56:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:15,419 >> Initializing global attention on CLS token...\n",
            " 53% 4400/8340 [1:09:45<56:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:16,271 >> Initializing global attention on CLS token...\n",
            " 53% 4401/8340 [1:09:45<56:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:17,127 >> Initializing global attention on CLS token...\n",
            " 53% 4402/8340 [1:09:46<56:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:17,980 >> Initializing global attention on CLS token...\n",
            " 53% 4403/8340 [1:09:47<56:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:18,837 >> Initializing global attention on CLS token...\n",
            " 53% 4404/8340 [1:09:48<56:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:19,688 >> Initializing global attention on CLS token...\n",
            " 53% 4405/8340 [1:09:49<56:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:20,544 >> Initializing global attention on CLS token...\n",
            " 53% 4406/8340 [1:09:50<55:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:21,395 >> Initializing global attention on CLS token...\n",
            " 53% 4407/8340 [1:09:50<55:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:22,250 >> Initializing global attention on CLS token...\n",
            " 53% 4408/8340 [1:09:51<55:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:23,104 >> Initializing global attention on CLS token...\n",
            " 53% 4409/8340 [1:09:52<55:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:23,960 >> Initializing global attention on CLS token...\n",
            " 53% 4410/8340 [1:09:53<55:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:24,812 >> Initializing global attention on CLS token...\n",
            " 53% 4411/8340 [1:09:54<55:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:25,669 >> Initializing global attention on CLS token...\n",
            " 53% 4412/8340 [1:09:55<55:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:26,521 >> Initializing global attention on CLS token...\n",
            " 53% 4413/8340 [1:09:56<55:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:27,378 >> Initializing global attention on CLS token...\n",
            " 53% 4414/8340 [1:09:56<55:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:28,231 >> Initializing global attention on CLS token...\n",
            " 53% 4415/8340 [1:09:57<55:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:29,086 >> Initializing global attention on CLS token...\n",
            " 53% 4416/8340 [1:09:58<55:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:29,941 >> Initializing global attention on CLS token...\n",
            " 53% 4417/8340 [1:09:59<55:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:30,795 >> Initializing global attention on CLS token...\n",
            " 53% 4418/8340 [1:10:00<55:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:31,652 >> Initializing global attention on CLS token...\n",
            " 53% 4419/8340 [1:10:01<55:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:32,506 >> Initializing global attention on CLS token...\n",
            " 53% 4420/8340 [1:10:02<55:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:33,361 >> Initializing global attention on CLS token...\n",
            " 53% 4421/8340 [1:10:02<55:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:34,213 >> Initializing global attention on CLS token...\n",
            " 53% 4422/8340 [1:10:03<55:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:35,067 >> Initializing global attention on CLS token...\n",
            " 53% 4423/8340 [1:10:04<55:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:35,925 >> Initializing global attention on CLS token...\n",
            " 53% 4424/8340 [1:10:05<55:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:36,779 >> Initializing global attention on CLS token...\n",
            " 53% 4425/8340 [1:10:06<55:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:37,634 >> Initializing global attention on CLS token...\n",
            " 53% 4426/8340 [1:10:07<55:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:38,491 >> Initializing global attention on CLS token...\n",
            " 53% 4427/8340 [1:10:08<55:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:39,346 >> Initializing global attention on CLS token...\n",
            " 53% 4428/8340 [1:10:08<55:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:40,200 >> Initializing global attention on CLS token...\n",
            " 53% 4429/8340 [1:10:09<55:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:41,054 >> Initializing global attention on CLS token...\n",
            " 53% 4430/8340 [1:10:10<55:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:41,913 >> Initializing global attention on CLS token...\n",
            " 53% 4431/8340 [1:10:11<55:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:42,766 >> Initializing global attention on CLS token...\n",
            " 53% 4432/8340 [1:10:12<55:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:43,621 >> Initializing global attention on CLS token...\n",
            " 53% 4433/8340 [1:10:13<55:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:44,474 >> Initializing global attention on CLS token...\n",
            " 53% 4434/8340 [1:10:14<55:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:45,332 >> Initializing global attention on CLS token...\n",
            " 53% 4435/8340 [1:10:14<55:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:46,186 >> Initializing global attention on CLS token...\n",
            " 53% 4436/8340 [1:10:15<55:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:47,041 >> Initializing global attention on CLS token...\n",
            " 53% 4437/8340 [1:10:16<55:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:47,895 >> Initializing global attention on CLS token...\n",
            " 53% 4438/8340 [1:10:17<55:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:48,756 >> Initializing global attention on CLS token...\n",
            " 53% 4439/8340 [1:10:18<55:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:49,608 >> Initializing global attention on CLS token...\n",
            " 53% 4440/8340 [1:10:19<55:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:50,460 >> Initializing global attention on CLS token...\n",
            " 53% 4441/8340 [1:10:20<55:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:51,317 >> Initializing global attention on CLS token...\n",
            " 53% 4442/8340 [1:10:20<55:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:52,169 >> Initializing global attention on CLS token...\n",
            " 53% 4443/8340 [1:10:21<55:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:53,022 >> Initializing global attention on CLS token...\n",
            " 53% 4444/8340 [1:10:22<55:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:53,875 >> Initializing global attention on CLS token...\n",
            " 53% 4445/8340 [1:10:23<55:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:54,729 >> Initializing global attention on CLS token...\n",
            " 53% 4446/8340 [1:10:24<55:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:55,584 >> Initializing global attention on CLS token...\n",
            " 53% 4447/8340 [1:10:25<55:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:56,438 >> Initializing global attention on CLS token...\n",
            " 53% 4448/8340 [1:10:26<55:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:57,295 >> Initializing global attention on CLS token...\n",
            " 53% 4449/8340 [1:10:26<55:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:58,153 >> Initializing global attention on CLS token...\n",
            " 53% 4450/8340 [1:10:27<55:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:59,004 >> Initializing global attention on CLS token...\n",
            " 53% 4451/8340 [1:10:28<55:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:08:59,856 >> Initializing global attention on CLS token...\n",
            " 53% 4452/8340 [1:10:29<55:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:00,716 >> Initializing global attention on CLS token...\n",
            " 53% 4453/8340 [1:10:30<55:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:01,570 >> Initializing global attention on CLS token...\n",
            " 53% 4454/8340 [1:10:31<55:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:02,422 >> Initializing global attention on CLS token...\n",
            " 53% 4455/8340 [1:10:32<55:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:03,274 >> Initializing global attention on CLS token...\n",
            " 53% 4456/8340 [1:10:32<55:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:04,127 >> Initializing global attention on CLS token...\n",
            " 53% 4457/8340 [1:10:33<55:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:04,981 >> Initializing global attention on CLS token...\n",
            " 53% 4458/8340 [1:10:34<55:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:05,835 >> Initializing global attention on CLS token...\n",
            " 53% 4459/8340 [1:10:35<55:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:06,688 >> Initializing global attention on CLS token...\n",
            " 53% 4460/8340 [1:10:36<55:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:07,544 >> Initializing global attention on CLS token...\n",
            " 53% 4461/8340 [1:10:37<55:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:08,398 >> Initializing global attention on CLS token...\n",
            " 54% 4462/8340 [1:10:37<55:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:09,252 >> Initializing global attention on CLS token...\n",
            " 54% 4463/8340 [1:10:38<55:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:10,112 >> Initializing global attention on CLS token...\n",
            " 54% 4464/8340 [1:10:39<55:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:10,956 >> Initializing global attention on CLS token...\n",
            " 54% 4465/8340 [1:10:40<55:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:11,815 >> Initializing global attention on CLS token...\n",
            " 54% 4466/8340 [1:10:41<55:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:12,668 >> Initializing global attention on CLS token...\n",
            " 54% 4467/8340 [1:10:42<55:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:13,519 >> Initializing global attention on CLS token...\n",
            " 54% 4468/8340 [1:10:43<55:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:14,376 >> Initializing global attention on CLS token...\n",
            " 54% 4469/8340 [1:10:43<55:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:15,230 >> Initializing global attention on CLS token...\n",
            " 54% 4470/8340 [1:10:44<55:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:16,083 >> Initializing global attention on CLS token...\n",
            " 54% 4471/8340 [1:10:45<55:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:16,937 >> Initializing global attention on CLS token...\n",
            " 54% 4472/8340 [1:10:46<55:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:17,791 >> Initializing global attention on CLS token...\n",
            " 54% 4473/8340 [1:10:47<55:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:18,646 >> Initializing global attention on CLS token...\n",
            " 54% 4474/8340 [1:10:48<55:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:19,502 >> Initializing global attention on CLS token...\n",
            " 54% 4475/8340 [1:10:49<54:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:20,353 >> Initializing global attention on CLS token...\n",
            " 54% 4476/8340 [1:10:49<54:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:21,206 >> Initializing global attention on CLS token...\n",
            " 54% 4477/8340 [1:10:50<54:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:22,061 >> Initializing global attention on CLS token...\n",
            " 54% 4478/8340 [1:10:51<54:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:22,916 >> Initializing global attention on CLS token...\n",
            " 54% 4479/8340 [1:10:52<54:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:23,770 >> Initializing global attention on CLS token...\n",
            " 54% 4480/8340 [1:10:53<54:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:24,623 >> Initializing global attention on CLS token...\n",
            " 54% 4481/8340 [1:10:54<54:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:25,476 >> Initializing global attention on CLS token...\n",
            " 54% 4482/8340 [1:10:55<54:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:26,328 >> Initializing global attention on CLS token...\n",
            " 54% 4483/8340 [1:10:55<54:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:27,185 >> Initializing global attention on CLS token...\n",
            " 54% 4484/8340 [1:10:56<54:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:28,036 >> Initializing global attention on CLS token...\n",
            " 54% 4485/8340 [1:10:57<54:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:28,891 >> Initializing global attention on CLS token...\n",
            " 54% 4486/8340 [1:10:58<54:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:29,745 >> Initializing global attention on CLS token...\n",
            " 54% 4487/8340 [1:10:59<54:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:30,598 >> Initializing global attention on CLS token...\n",
            " 54% 4488/8340 [1:11:00<54:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:31,452 >> Initializing global attention on CLS token...\n",
            " 54% 4489/8340 [1:11:01<54:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:32,308 >> Initializing global attention on CLS token...\n",
            " 54% 4490/8340 [1:11:01<54:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:33,159 >> Initializing global attention on CLS token...\n",
            " 54% 4491/8340 [1:11:02<54:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:34,015 >> Initializing global attention on CLS token...\n",
            " 54% 4492/8340 [1:11:03<54:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:34,870 >> Initializing global attention on CLS token...\n",
            " 54% 4493/8340 [1:11:04<54:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:35,727 >> Initializing global attention on CLS token...\n",
            " 54% 4494/8340 [1:11:05<54:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:36,578 >> Initializing global attention on CLS token...\n",
            " 54% 4495/8340 [1:11:06<54:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:37,432 >> Initializing global attention on CLS token...\n",
            " 54% 4496/8340 [1:11:07<54:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:38,288 >> Initializing global attention on CLS token...\n",
            " 54% 4497/8340 [1:11:07<54:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:39,144 >> Initializing global attention on CLS token...\n",
            " 54% 4498/8340 [1:11:08<54:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:39,999 >> Initializing global attention on CLS token...\n",
            " 54% 4499/8340 [1:11:09<54:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:40,856 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.1054, 'learning_rate': 2.3051558752997606e-05, 'epoch': 5.4}\n",
            " 54% 4500/8340 [1:11:10<57:17,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:41,845 >> Initializing global attention on CLS token...\n",
            " 54% 4501/8340 [1:11:11<56:26,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:42,696 >> Initializing global attention on CLS token...\n",
            " 54% 4502/8340 [1:11:12<55:45,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:43,543 >> Initializing global attention on CLS token...\n",
            " 54% 4503/8340 [1:11:13<55:25,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:44,398 >> Initializing global attention on CLS token...\n",
            " 54% 4504/8340 [1:11:14<58:28,  1.09it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:45,422 >> Initializing global attention on CLS token...\n",
            " 54% 4505/8340 [1:11:15<57:11,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:46,272 >> Initializing global attention on CLS token...\n",
            " 54% 4506/8340 [1:11:15<56:23,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:47,127 >> Initializing global attention on CLS token...\n",
            " 54% 4507/8340 [1:11:16<55:49,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:47,979 >> Initializing global attention on CLS token...\n",
            " 54% 4508/8340 [1:11:17<55:27,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:48,836 >> Initializing global attention on CLS token...\n",
            " 54% 4509/8340 [1:11:18<55:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:49,689 >> Initializing global attention on CLS token...\n",
            " 54% 4510/8340 [1:11:19<54:59,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:50,544 >> Initializing global attention on CLS token...\n",
            " 54% 4511/8340 [1:11:20<54:51,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:51,399 >> Initializing global attention on CLS token...\n",
            " 54% 4512/8340 [1:11:21<54:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:52,257 >> Initializing global attention on CLS token...\n",
            " 54% 4513/8340 [1:11:21<54:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:53,111 >> Initializing global attention on CLS token...\n",
            " 54% 4514/8340 [1:11:22<54:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:53,966 >> Initializing global attention on CLS token...\n",
            " 54% 4515/8340 [1:11:23<54:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:54,821 >> Initializing global attention on CLS token...\n",
            " 54% 4516/8340 [1:11:24<54:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:55,677 >> Initializing global attention on CLS token...\n",
            " 54% 4517/8340 [1:11:25<54:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:56,533 >> Initializing global attention on CLS token...\n",
            " 54% 4518/8340 [1:11:26<54:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:57,389 >> Initializing global attention on CLS token...\n",
            " 54% 4519/8340 [1:11:26<54:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:58,242 >> Initializing global attention on CLS token...\n",
            " 54% 4520/8340 [1:11:27<54:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:59,095 >> Initializing global attention on CLS token...\n",
            " 54% 4521/8340 [1:11:28<54:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:09:59,947 >> Initializing global attention on CLS token...\n",
            " 54% 4522/8340 [1:11:29<54:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:00,802 >> Initializing global attention on CLS token...\n",
            " 54% 4523/8340 [1:11:30<54:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:01,656 >> Initializing global attention on CLS token...\n",
            " 54% 4524/8340 [1:11:31<54:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:02,511 >> Initializing global attention on CLS token...\n",
            " 54% 4525/8340 [1:11:32<54:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:03,367 >> Initializing global attention on CLS token...\n",
            " 54% 4526/8340 [1:11:32<54:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:04,220 >> Initializing global attention on CLS token...\n",
            " 54% 4527/8340 [1:11:33<54:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:05,074 >> Initializing global attention on CLS token...\n",
            " 54% 4528/8340 [1:11:34<54:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:05,932 >> Initializing global attention on CLS token...\n",
            " 54% 4529/8340 [1:11:35<54:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:06,799 >> Initializing global attention on CLS token...\n",
            " 54% 4530/8340 [1:11:36<54:34,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:07,658 >> Initializing global attention on CLS token...\n",
            " 54% 4531/8340 [1:11:37<54:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:08,514 >> Initializing global attention on CLS token...\n",
            " 54% 4532/8340 [1:11:38<54:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:09,366 >> Initializing global attention on CLS token...\n",
            " 54% 4533/8340 [1:11:38<54:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:10,215 >> Initializing global attention on CLS token...\n",
            " 54% 4534/8340 [1:11:39<54:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:11,069 >> Initializing global attention on CLS token...\n",
            " 54% 4535/8340 [1:11:40<54:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:11,924 >> Initializing global attention on CLS token...\n",
            " 54% 4536/8340 [1:11:41<54:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:12,778 >> Initializing global attention on CLS token...\n",
            " 54% 4537/8340 [1:11:42<54:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:13,632 >> Initializing global attention on CLS token...\n",
            " 54% 4538/8340 [1:11:43<54:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:14,489 >> Initializing global attention on CLS token...\n",
            " 54% 4539/8340 [1:11:44<54:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:15,341 >> Initializing global attention on CLS token...\n",
            " 54% 4540/8340 [1:11:44<54:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:16,194 >> Initializing global attention on CLS token...\n",
            " 54% 4541/8340 [1:11:45<54:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:17,048 >> Initializing global attention on CLS token...\n",
            " 54% 4542/8340 [1:11:46<54:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:17,902 >> Initializing global attention on CLS token...\n",
            " 54% 4543/8340 [1:11:47<54:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:18,755 >> Initializing global attention on CLS token...\n",
            " 54% 4544/8340 [1:11:48<54:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:19,612 >> Initializing global attention on CLS token...\n",
            " 54% 4545/8340 [1:11:49<54:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:20,464 >> Initializing global attention on CLS token...\n",
            " 55% 4546/8340 [1:11:50<54:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:21,318 >> Initializing global attention on CLS token...\n",
            " 55% 4547/8340 [1:11:50<53:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:22,175 >> Initializing global attention on CLS token...\n",
            " 55% 4548/8340 [1:11:51<53:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:23,025 >> Initializing global attention on CLS token...\n",
            " 55% 4549/8340 [1:11:52<53:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:23,880 >> Initializing global attention on CLS token...\n",
            " 55% 4550/8340 [1:11:53<53:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:24,732 >> Initializing global attention on CLS token...\n",
            " 55% 4551/8340 [1:11:54<53:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:25,590 >> Initializing global attention on CLS token...\n",
            " 55% 4552/8340 [1:11:55<53:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:26,440 >> Initializing global attention on CLS token...\n",
            " 55% 4553/8340 [1:11:56<53:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:27,294 >> Initializing global attention on CLS token...\n",
            " 55% 4554/8340 [1:11:56<53:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:28,150 >> Initializing global attention on CLS token...\n",
            " 55% 4555/8340 [1:11:57<53:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:29,006 >> Initializing global attention on CLS token...\n",
            " 55% 4556/8340 [1:11:58<53:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:29,858 >> Initializing global attention on CLS token...\n",
            " 55% 4557/8340 [1:11:59<53:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:30,712 >> Initializing global attention on CLS token...\n",
            " 55% 4558/8340 [1:12:00<53:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:31,568 >> Initializing global attention on CLS token...\n",
            " 55% 4559/8340 [1:12:01<53:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:32,422 >> Initializing global attention on CLS token...\n",
            " 55% 4560/8340 [1:12:02<53:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:33,276 >> Initializing global attention on CLS token...\n",
            " 55% 4561/8340 [1:12:02<53:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:34,130 >> Initializing global attention on CLS token...\n",
            " 55% 4562/8340 [1:12:03<53:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:34,984 >> Initializing global attention on CLS token...\n",
            " 55% 4563/8340 [1:12:04<53:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:35,836 >> Initializing global attention on CLS token...\n",
            " 55% 4564/8340 [1:12:05<53:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:36,693 >> Initializing global attention on CLS token...\n",
            " 55% 4565/8340 [1:12:06<53:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:37,549 >> Initializing global attention on CLS token...\n",
            " 55% 4566/8340 [1:12:07<53:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:38,403 >> Initializing global attention on CLS token...\n",
            " 55% 4567/8340 [1:12:08<53:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:39,257 >> Initializing global attention on CLS token...\n",
            " 55% 4568/8340 [1:12:08<53:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:40,114 >> Initializing global attention on CLS token...\n",
            " 55% 4569/8340 [1:12:09<53:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:40,963 >> Initializing global attention on CLS token...\n",
            " 55% 4570/8340 [1:12:10<53:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:41,814 >> Initializing global attention on CLS token...\n",
            " 55% 4571/8340 [1:12:11<53:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:42,670 >> Initializing global attention on CLS token...\n",
            " 55% 4572/8340 [1:12:12<53:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:43,526 >> Initializing global attention on CLS token...\n",
            " 55% 4573/8340 [1:12:13<53:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:44,378 >> Initializing global attention on CLS token...\n",
            " 55% 4574/8340 [1:12:13<53:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:45,234 >> Initializing global attention on CLS token...\n",
            " 55% 4575/8340 [1:12:14<53:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:46,086 >> Initializing global attention on CLS token...\n",
            " 55% 4576/8340 [1:12:15<53:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:46,936 >> Initializing global attention on CLS token...\n",
            " 55% 4577/8340 [1:12:16<53:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:47,792 >> Initializing global attention on CLS token...\n",
            " 55% 4578/8340 [1:12:17<53:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:48,647 >> Initializing global attention on CLS token...\n",
            " 55% 4579/8340 [1:12:18<53:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:49,502 >> Initializing global attention on CLS token...\n",
            " 55% 4580/8340 [1:12:19<53:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:50,357 >> Initializing global attention on CLS token...\n",
            " 55% 4581/8340 [1:12:19<53:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:51,212 >> Initializing global attention on CLS token...\n",
            " 55% 4582/8340 [1:12:20<53:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:52,067 >> Initializing global attention on CLS token...\n",
            " 55% 4583/8340 [1:12:21<53:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:52,922 >> Initializing global attention on CLS token...\n",
            " 55% 4584/8340 [1:12:22<53:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:53,778 >> Initializing global attention on CLS token...\n",
            " 55% 4585/8340 [1:12:23<53:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:54,634 >> Initializing global attention on CLS token...\n",
            " 55% 4586/8340 [1:12:24<53:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:55,496 >> Initializing global attention on CLS token...\n",
            " 55% 4587/8340 [1:12:25<53:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:56,345 >> Initializing global attention on CLS token...\n",
            " 55% 4588/8340 [1:12:25<53:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:57,201 >> Initializing global attention on CLS token...\n",
            " 55% 4589/8340 [1:12:26<53:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:58,059 >> Initializing global attention on CLS token...\n",
            " 55% 4590/8340 [1:12:27<53:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:58,911 >> Initializing global attention on CLS token...\n",
            " 55% 4591/8340 [1:12:28<53:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:10:59,770 >> Initializing global attention on CLS token...\n",
            " 55% 4592/8340 [1:12:29<53:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:00,624 >> Initializing global attention on CLS token...\n",
            " 55% 4593/8340 [1:12:30<53:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:01,476 >> Initializing global attention on CLS token...\n",
            " 55% 4594/8340 [1:12:31<53:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:02,325 >> Initializing global attention on CLS token...\n",
            " 55% 4595/8340 [1:12:31<53:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:03,177 >> Initializing global attention on CLS token...\n",
            " 55% 4596/8340 [1:12:32<53:05,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:04,024 >> Initializing global attention on CLS token...\n",
            " 55% 4597/8340 [1:12:33<53:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:04,879 >> Initializing global attention on CLS token...\n",
            " 55% 4598/8340 [1:12:34<53:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:05,735 >> Initializing global attention on CLS token...\n",
            " 55% 4599/8340 [1:12:35<53:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:06,587 >> Initializing global attention on CLS token...\n",
            " 55% 4600/8340 [1:12:36<53:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:07,442 >> Initializing global attention on CLS token...\n",
            " 55% 4601/8340 [1:12:37<53:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:08,295 >> Initializing global attention on CLS token...\n",
            " 55% 4602/8340 [1:12:37<53:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:09,147 >> Initializing global attention on CLS token...\n",
            " 55% 4603/8340 [1:12:38<53:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:10,002 >> Initializing global attention on CLS token...\n",
            " 55% 4604/8340 [1:12:39<53:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:10,858 >> Initializing global attention on CLS token...\n",
            " 55% 4605/8340 [1:12:40<53:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:11,710 >> Initializing global attention on CLS token...\n",
            " 55% 4606/8340 [1:12:41<53:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:12,573 >> Initializing global attention on CLS token...\n",
            " 55% 4607/8340 [1:12:42<53:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:13,423 >> Initializing global attention on CLS token...\n",
            " 55% 4608/8340 [1:12:43<53:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:14,277 >> Initializing global attention on CLS token...\n",
            " 55% 4609/8340 [1:12:43<53:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:15,126 >> Initializing global attention on CLS token...\n",
            " 55% 4610/8340 [1:12:44<53:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:15,982 >> Initializing global attention on CLS token...\n",
            " 55% 4611/8340 [1:12:45<53:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:16,837 >> Initializing global attention on CLS token...\n",
            " 55% 4612/8340 [1:12:46<53:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:17,695 >> Initializing global attention on CLS token...\n",
            " 55% 4613/8340 [1:12:47<53:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:18,547 >> Initializing global attention on CLS token...\n",
            " 55% 4614/8340 [1:12:48<53:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:19,414 >> Initializing global attention on CLS token...\n",
            " 55% 4615/8340 [1:12:49<53:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:20,269 >> Initializing global attention on CLS token...\n",
            " 55% 4616/8340 [1:12:49<53:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:21,124 >> Initializing global attention on CLS token...\n",
            " 55% 4617/8340 [1:12:50<53:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:21,990 >> Initializing global attention on CLS token...\n",
            " 55% 4618/8340 [1:12:51<53:14,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:22,856 >> Initializing global attention on CLS token...\n",
            " 55% 4619/8340 [1:12:52<53:27,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:23,714 >> Initializing global attention on CLS token...\n",
            " 55% 4620/8340 [1:12:53<53:14,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:24,565 >> Initializing global attention on CLS token...\n",
            " 55% 4621/8340 [1:12:54<53:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:25,419 >> Initializing global attention on CLS token...\n",
            " 55% 4622/8340 [1:12:55<53:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:26,273 >> Initializing global attention on CLS token...\n",
            " 55% 4623/8340 [1:12:55<53:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:27,127 >> Initializing global attention on CLS token...\n",
            " 55% 4624/8340 [1:12:56<53:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:27,983 >> Initializing global attention on CLS token...\n",
            " 55% 4625/8340 [1:12:57<52:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:28,835 >> Initializing global attention on CLS token...\n",
            " 55% 4626/8340 [1:12:58<52:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:29,691 >> Initializing global attention on CLS token...\n",
            " 55% 4627/8340 [1:12:59<52:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:30,544 >> Initializing global attention on CLS token...\n",
            " 55% 4628/8340 [1:13:00<52:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:31,404 >> Initializing global attention on CLS token...\n",
            " 56% 4629/8340 [1:13:00<52:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:32,251 >> Initializing global attention on CLS token...\n",
            " 56% 4630/8340 [1:13:01<52:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:33,110 >> Initializing global attention on CLS token...\n",
            " 56% 4631/8340 [1:13:02<52:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:33,962 >> Initializing global attention on CLS token...\n",
            " 56% 4632/8340 [1:13:03<52:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:34,824 >> Initializing global attention on CLS token...\n",
            " 56% 4633/8340 [1:13:04<52:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:35,672 >> Initializing global attention on CLS token...\n",
            " 56% 4634/8340 [1:13:05<52:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:36,520 >> Initializing global attention on CLS token...\n",
            " 56% 4635/8340 [1:13:06<52:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:37,374 >> Initializing global attention on CLS token...\n",
            " 56% 4636/8340 [1:13:06<52:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:38,230 >> Initializing global attention on CLS token...\n",
            " 56% 4637/8340 [1:13:07<52:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:39,096 >> Initializing global attention on CLS token...\n",
            " 56% 4638/8340 [1:13:08<52:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:39,947 >> Initializing global attention on CLS token...\n",
            " 56% 4639/8340 [1:13:09<52:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:40,799 >> Initializing global attention on CLS token...\n",
            " 56% 4640/8340 [1:13:10<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:41,645 >> Initializing global attention on CLS token...\n",
            " 56% 4641/8340 [1:13:11<52:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:42,498 >> Initializing global attention on CLS token...\n",
            " 56% 4642/8340 [1:13:12<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:43,350 >> Initializing global attention on CLS token...\n",
            " 56% 4643/8340 [1:13:12<52:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:44,204 >> Initializing global attention on CLS token...\n",
            " 56% 4644/8340 [1:13:13<52:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:45,060 >> Initializing global attention on CLS token...\n",
            " 56% 4645/8340 [1:13:14<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:45,911 >> Initializing global attention on CLS token...\n",
            " 56% 4646/8340 [1:13:15<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:46,765 >> Initializing global attention on CLS token...\n",
            " 56% 4647/8340 [1:13:16<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:47,619 >> Initializing global attention on CLS token...\n",
            " 56% 4648/8340 [1:13:17<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:48,476 >> Initializing global attention on CLS token...\n",
            " 56% 4649/8340 [1:13:18<52:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:49,326 >> Initializing global attention on CLS token...\n",
            " 56% 4650/8340 [1:13:18<52:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:50,194 >> Initializing global attention on CLS token...\n",
            " 56% 4651/8340 [1:13:19<52:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:51,048 >> Initializing global attention on CLS token...\n",
            " 56% 4652/8340 [1:13:20<52:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:51,905 >> Initializing global attention on CLS token...\n",
            " 56% 4653/8340 [1:13:21<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:52,755 >> Initializing global attention on CLS token...\n",
            " 56% 4654/8340 [1:13:22<52:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:53,614 >> Initializing global attention on CLS token...\n",
            " 56% 4655/8340 [1:13:23<52:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:54,474 >> Initializing global attention on CLS token...\n",
            " 56% 4656/8340 [1:13:24<52:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:55,338 >> Initializing global attention on CLS token...\n",
            " 56% 4657/8340 [1:13:24<52:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:56,185 >> Initializing global attention on CLS token...\n",
            " 56% 4658/8340 [1:13:25<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:57,050 >> Initializing global attention on CLS token...\n",
            " 56% 4659/8340 [1:13:26<52:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:57,898 >> Initializing global attention on CLS token...\n",
            " 56% 4660/8340 [1:13:27<52:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:58,760 >> Initializing global attention on CLS token...\n",
            " 56% 4661/8340 [1:13:28<52:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:11:59,606 >> Initializing global attention on CLS token...\n",
            " 56% 4662/8340 [1:13:29<52:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:00,465 >> Initializing global attention on CLS token...\n",
            " 56% 4663/8340 [1:13:30<52:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:01,318 >> Initializing global attention on CLS token...\n",
            " 56% 4664/8340 [1:13:30<52:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:02,173 >> Initializing global attention on CLS token...\n",
            " 56% 4665/8340 [1:13:31<52:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:03,033 >> Initializing global attention on CLS token...\n",
            " 56% 4666/8340 [1:13:32<52:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:03,891 >> Initializing global attention on CLS token...\n",
            " 56% 4667/8340 [1:13:33<52:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:04,743 >> Initializing global attention on CLS token...\n",
            " 56% 4668/8340 [1:13:34<52:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:05,596 >> Initializing global attention on CLS token...\n",
            " 56% 4669/8340 [1:13:35<52:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:06,468 >> Initializing global attention on CLS token...\n",
            " 56% 4670/8340 [1:13:36<52:34,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:07,342 >> Initializing global attention on CLS token...\n",
            " 56% 4671/8340 [1:13:36<53:46,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:08,320 >> Initializing global attention on CLS token...\n",
            " 56% 4672/8340 [1:13:37<54:43,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:09,195 >> Initializing global attention on CLS token...\n",
            " 56% 4673/8340 [1:13:38<54:13,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:10,050 >> Initializing global attention on CLS token...\n",
            " 56% 4674/8340 [1:13:39<53:38,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:10,907 >> Initializing global attention on CLS token...\n",
            " 56% 4675/8340 [1:13:40<53:20,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:11,792 >> Initializing global attention on CLS token...\n",
            " 56% 4676/8340 [1:13:41<53:22,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:12,644 >> Initializing global attention on CLS token...\n",
            " 56% 4677/8340 [1:13:42<53:03,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:13,503 >> Initializing global attention on CLS token...\n",
            " 56% 4678/8340 [1:13:43<52:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:14,354 >> Initializing global attention on CLS token...\n",
            " 56% 4679/8340 [1:13:43<52:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:15,208 >> Initializing global attention on CLS token...\n",
            " 56% 4680/8340 [1:13:44<52:23,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:16,062 >> Initializing global attention on CLS token...\n",
            " 56% 4681/8340 [1:13:45<52:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:16,945 >> Initializing global attention on CLS token...\n",
            " 56% 4682/8340 [1:13:46<52:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:17,823 >> Initializing global attention on CLS token...\n",
            " 56% 4683/8340 [1:13:47<53:05,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:18,692 >> Initializing global attention on CLS token...\n",
            " 56% 4684/8340 [1:13:48<52:57,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:19,571 >> Initializing global attention on CLS token...\n",
            " 56% 4685/8340 [1:13:49<53:10,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:20,440 >> Initializing global attention on CLS token...\n",
            " 56% 4686/8340 [1:13:50<52:50,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:21,289 >> Initializing global attention on CLS token...\n",
            " 56% 4687/8340 [1:13:50<52:37,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:22,147 >> Initializing global attention on CLS token...\n",
            " 56% 4688/8340 [1:13:51<52:24,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:22,998 >> Initializing global attention on CLS token...\n",
            " 56% 4689/8340 [1:13:52<52:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:23,851 >> Initializing global attention on CLS token...\n",
            " 56% 4690/8340 [1:13:53<52:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:24,705 >> Initializing global attention on CLS token...\n",
            " 56% 4691/8340 [1:13:54<52:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:25,556 >> Initializing global attention on CLS token...\n",
            " 56% 4692/8340 [1:13:55<52:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:26,411 >> Initializing global attention on CLS token...\n",
            " 56% 4693/8340 [1:13:56<51:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:27,266 >> Initializing global attention on CLS token...\n",
            " 56% 4694/8340 [1:13:56<51:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:28,119 >> Initializing global attention on CLS token...\n",
            " 56% 4695/8340 [1:13:57<51:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:28,977 >> Initializing global attention on CLS token...\n",
            " 56% 4696/8340 [1:13:58<51:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:29,829 >> Initializing global attention on CLS token...\n",
            " 56% 4697/8340 [1:13:59<51:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:30,681 >> Initializing global attention on CLS token...\n",
            " 56% 4698/8340 [1:14:00<51:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:31,535 >> Initializing global attention on CLS token...\n",
            " 56% 4699/8340 [1:14:01<51:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:32,389 >> Initializing global attention on CLS token...\n",
            " 56% 4700/8340 [1:14:01<51:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:33,250 >> Initializing global attention on CLS token...\n",
            " 56% 4701/8340 [1:14:02<51:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:34,098 >> Initializing global attention on CLS token...\n",
            " 56% 4702/8340 [1:14:03<51:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:34,952 >> Initializing global attention on CLS token...\n",
            " 56% 4703/8340 [1:14:04<51:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:35,806 >> Initializing global attention on CLS token...\n",
            " 56% 4704/8340 [1:14:05<51:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:36,660 >> Initializing global attention on CLS token...\n",
            " 56% 4705/8340 [1:14:06<51:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:37,518 >> Initializing global attention on CLS token...\n",
            " 56% 4706/8340 [1:14:07<51:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:38,371 >> Initializing global attention on CLS token...\n",
            " 56% 4707/8340 [1:14:07<51:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:39,247 >> Initializing global attention on CLS token...\n",
            " 56% 4708/8340 [1:14:08<52:02,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:40,097 >> Initializing global attention on CLS token...\n",
            " 56% 4709/8340 [1:14:09<52:03,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:40,961 >> Initializing global attention on CLS token...\n",
            " 56% 4710/8340 [1:14:10<52:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:41,822 >> Initializing global attention on CLS token...\n",
            " 56% 4711/8340 [1:14:11<52:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:42,683 >> Initializing global attention on CLS token...\n",
            " 56% 4712/8340 [1:14:12<51:57,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:43,537 >> Initializing global attention on CLS token...\n",
            " 57% 4713/8340 [1:14:13<51:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:44,397 >> Initializing global attention on CLS token...\n",
            " 57% 4714/8340 [1:14:13<51:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:45,248 >> Initializing global attention on CLS token...\n",
            " 57% 4715/8340 [1:14:14<51:59,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:46,120 >> Initializing global attention on CLS token...\n",
            " 57% 4716/8340 [1:14:15<51:54,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:46,986 >> Initializing global attention on CLS token...\n",
            " 57% 4717/8340 [1:14:16<52:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:47,840 >> Initializing global attention on CLS token...\n",
            " 57% 4718/8340 [1:14:17<51:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:48,711 >> Initializing global attention on CLS token...\n",
            " 57% 4719/8340 [1:14:18<51:57,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:49,559 >> Initializing global attention on CLS token...\n",
            " 57% 4720/8340 [1:14:19<51:48,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:50,413 >> Initializing global attention on CLS token...\n",
            " 57% 4721/8340 [1:14:20<51:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:51,264 >> Initializing global attention on CLS token...\n",
            " 57% 4722/8340 [1:14:20<51:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:52,121 >> Initializing global attention on CLS token...\n",
            " 57% 4723/8340 [1:14:21<51:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:52,974 >> Initializing global attention on CLS token...\n",
            " 57% 4724/8340 [1:14:22<51:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:53,835 >> Initializing global attention on CLS token...\n",
            " 57% 4725/8340 [1:14:23<51:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:54,682 >> Initializing global attention on CLS token...\n",
            " 57% 4726/8340 [1:14:24<51:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:55,547 >> Initializing global attention on CLS token...\n",
            " 57% 4727/8340 [1:14:25<51:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:56,397 >> Initializing global attention on CLS token...\n",
            " 57% 4728/8340 [1:14:25<51:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:57,250 >> Initializing global attention on CLS token...\n",
            " 57% 4729/8340 [1:14:26<51:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:58,103 >> Initializing global attention on CLS token...\n",
            " 57% 4730/8340 [1:14:27<51:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:58,955 >> Initializing global attention on CLS token...\n",
            " 57% 4731/8340 [1:14:28<51:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:12:59,806 >> Initializing global attention on CLS token...\n",
            " 57% 4732/8340 [1:14:29<51:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:00,663 >> Initializing global attention on CLS token...\n",
            " 57% 4733/8340 [1:14:30<51:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:01,517 >> Initializing global attention on CLS token...\n",
            " 57% 4734/8340 [1:14:31<51:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:02,374 >> Initializing global attention on CLS token...\n",
            " 57% 4735/8340 [1:14:31<51:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:03,224 >> Initializing global attention on CLS token...\n",
            " 57% 4736/8340 [1:14:32<51:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:04,082 >> Initializing global attention on CLS token...\n",
            " 57% 4737/8340 [1:14:33<51:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:04,942 >> Initializing global attention on CLS token...\n",
            " 57% 4738/8340 [1:14:34<51:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:05,789 >> Initializing global attention on CLS token...\n",
            " 57% 4739/8340 [1:14:35<51:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:06,647 >> Initializing global attention on CLS token...\n",
            " 57% 4740/8340 [1:14:36<51:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:07,496 >> Initializing global attention on CLS token...\n",
            " 57% 4741/8340 [1:14:37<51:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:08,352 >> Initializing global attention on CLS token...\n",
            " 57% 4742/8340 [1:14:37<51:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:09,215 >> Initializing global attention on CLS token...\n",
            " 57% 4743/8340 [1:14:38<51:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:10,063 >> Initializing global attention on CLS token...\n",
            " 57% 4744/8340 [1:14:39<51:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:10,929 >> Initializing global attention on CLS token...\n",
            " 57% 4745/8340 [1:14:40<51:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:11,782 >> Initializing global attention on CLS token...\n",
            " 57% 4746/8340 [1:14:41<51:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:12,629 >> Initializing global attention on CLS token...\n",
            " 57% 4747/8340 [1:14:42<51:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:13,481 >> Initializing global attention on CLS token...\n",
            " 57% 4748/8340 [1:14:43<51:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:14,337 >> Initializing global attention on CLS token...\n",
            " 57% 4749/8340 [1:14:43<51:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:15,194 >> Initializing global attention on CLS token...\n",
            " 57% 4750/8340 [1:14:44<51:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:16,047 >> Initializing global attention on CLS token...\n",
            " 57% 4751/8340 [1:14:45<51:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:16,902 >> Initializing global attention on CLS token...\n",
            " 57% 4752/8340 [1:14:46<51:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:17,753 >> Initializing global attention on CLS token...\n",
            " 57% 4753/8340 [1:14:47<51:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:18,608 >> Initializing global attention on CLS token...\n",
            " 57% 4754/8340 [1:14:48<51:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:19,462 >> Initializing global attention on CLS token...\n",
            " 57% 4755/8340 [1:14:49<50:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:20,313 >> Initializing global attention on CLS token...\n",
            " 57% 4756/8340 [1:14:49<50:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:21,170 >> Initializing global attention on CLS token...\n",
            " 57% 4757/8340 [1:14:50<51:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:22,026 >> Initializing global attention on CLS token...\n",
            " 57% 4758/8340 [1:14:51<51:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:22,886 >> Initializing global attention on CLS token...\n",
            " 57% 4759/8340 [1:14:52<51:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:23,742 >> Initializing global attention on CLS token...\n",
            " 57% 4760/8340 [1:14:53<51:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:24,598 >> Initializing global attention on CLS token...\n",
            " 57% 4761/8340 [1:14:54<51:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:25,451 >> Initializing global attention on CLS token...\n",
            " 57% 4762/8340 [1:14:55<50:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:26,300 >> Initializing global attention on CLS token...\n",
            " 57% 4763/8340 [1:14:55<51:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:27,159 >> Initializing global attention on CLS token...\n",
            " 57% 4764/8340 [1:14:56<50:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:28,026 >> Initializing global attention on CLS token...\n",
            " 57% 4765/8340 [1:14:57<51:13,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:28,884 >> Initializing global attention on CLS token...\n",
            " 57% 4766/8340 [1:14:58<51:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:29,737 >> Initializing global attention on CLS token...\n",
            " 57% 4767/8340 [1:14:59<51:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:30,592 >> Initializing global attention on CLS token...\n",
            " 57% 4768/8340 [1:15:00<50:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:31,447 >> Initializing global attention on CLS token...\n",
            " 57% 4769/8340 [1:15:01<50:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:32,303 >> Initializing global attention on CLS token...\n",
            " 57% 4770/8340 [1:15:01<50:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:33,156 >> Initializing global attention on CLS token...\n",
            " 57% 4771/8340 [1:15:02<50:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:34,010 >> Initializing global attention on CLS token...\n",
            " 57% 4772/8340 [1:15:03<50:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:34,863 >> Initializing global attention on CLS token...\n",
            " 57% 4773/8340 [1:15:04<50:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:35,718 >> Initializing global attention on CLS token...\n",
            " 57% 4774/8340 [1:15:05<50:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:36,571 >> Initializing global attention on CLS token...\n",
            " 57% 4775/8340 [1:15:06<50:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:37,426 >> Initializing global attention on CLS token...\n",
            " 57% 4776/8340 [1:15:07<50:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:38,280 >> Initializing global attention on CLS token...\n",
            " 57% 4777/8340 [1:15:07<50:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:39,134 >> Initializing global attention on CLS token...\n",
            " 57% 4778/8340 [1:15:08<50:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:39,987 >> Initializing global attention on CLS token...\n",
            " 57% 4779/8340 [1:15:09<50:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:40,841 >> Initializing global attention on CLS token...\n",
            " 57% 4780/8340 [1:15:10<50:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:41,698 >> Initializing global attention on CLS token...\n",
            " 57% 4781/8340 [1:15:11<50:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:42,564 >> Initializing global attention on CLS token...\n",
            " 57% 4782/8340 [1:15:12<50:57,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:43,422 >> Initializing global attention on CLS token...\n",
            " 57% 4783/8340 [1:15:13<50:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:44,277 >> Initializing global attention on CLS token...\n",
            " 57% 4784/8340 [1:15:13<50:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:45,130 >> Initializing global attention on CLS token...\n",
            " 57% 4785/8340 [1:15:14<50:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:45,984 >> Initializing global attention on CLS token...\n",
            " 57% 4786/8340 [1:15:15<50:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:46,839 >> Initializing global attention on CLS token...\n",
            " 57% 4787/8340 [1:15:16<50:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:47,700 >> Initializing global attention on CLS token...\n",
            " 57% 4788/8340 [1:15:17<50:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:48,554 >> Initializing global attention on CLS token...\n",
            " 57% 4789/8340 [1:15:18<50:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:49,422 >> Initializing global attention on CLS token...\n",
            " 57% 4790/8340 [1:15:19<50:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:50,280 >> Initializing global attention on CLS token...\n",
            " 57% 4791/8340 [1:15:19<50:50,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:51,152 >> Initializing global attention on CLS token...\n",
            " 57% 4792/8340 [1:15:20<51:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:52,008 >> Initializing global attention on CLS token...\n",
            " 57% 4793/8340 [1:15:21<50:46,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:52,859 >> Initializing global attention on CLS token...\n",
            " 57% 4794/8340 [1:15:22<50:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:53,711 >> Initializing global attention on CLS token...\n",
            " 57% 4795/8340 [1:15:23<50:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:54,596 >> Initializing global attention on CLS token...\n",
            " 58% 4796/8340 [1:15:24<50:57,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:55,453 >> Initializing global attention on CLS token...\n",
            " 58% 4797/8340 [1:15:25<51:06,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:56,341 >> Initializing global attention on CLS token...\n",
            " 58% 4798/8340 [1:15:25<51:33,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:57,231 >> Initializing global attention on CLS token...\n",
            " 58% 4799/8340 [1:15:26<51:36,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:58,088 >> Initializing global attention on CLS token...\n",
            " 58% 4800/8340 [1:15:27<51:11,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:58,938 >> Initializing global attention on CLS token...\n",
            " 58% 4801/8340 [1:15:28<50:57,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:13:59,795 >> Initializing global attention on CLS token...\n",
            " 58% 4802/8340 [1:15:29<50:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:00,646 >> Initializing global attention on CLS token...\n",
            " 58% 4803/8340 [1:15:30<50:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:01,499 >> Initializing global attention on CLS token...\n",
            " 58% 4804/8340 [1:15:31<50:37,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:02,362 >> Initializing global attention on CLS token...\n",
            " 58% 4805/8340 [1:15:31<50:39,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:03,227 >> Initializing global attention on CLS token...\n",
            " 58% 4806/8340 [1:15:32<50:43,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:04,085 >> Initializing global attention on CLS token...\n",
            " 58% 4807/8340 [1:15:33<50:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:04,934 >> Initializing global attention on CLS token...\n",
            " 58% 4808/8340 [1:15:34<50:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:05,791 >> Initializing global attention on CLS token...\n",
            " 58% 4809/8340 [1:15:35<50:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:06,644 >> Initializing global attention on CLS token...\n",
            " 58% 4810/8340 [1:15:36<50:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:07,496 >> Initializing global attention on CLS token...\n",
            " 58% 4811/8340 [1:15:37<50:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:08,370 >> Initializing global attention on CLS token...\n",
            " 58% 4812/8340 [1:15:37<50:36,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:09,227 >> Initializing global attention on CLS token...\n",
            " 58% 4813/8340 [1:15:38<50:28,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:10,080 >> Initializing global attention on CLS token...\n",
            " 58% 4814/8340 [1:15:39<50:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:10,933 >> Initializing global attention on CLS token...\n",
            " 58% 4815/8340 [1:15:40<50:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:11,791 >> Initializing global attention on CLS token...\n",
            " 58% 4816/8340 [1:15:41<50:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:12,647 >> Initializing global attention on CLS token...\n",
            " 58% 4817/8340 [1:15:42<50:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:13,497 >> Initializing global attention on CLS token...\n",
            " 58% 4818/8340 [1:15:43<50:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:14,353 >> Initializing global attention on CLS token...\n",
            " 58% 4819/8340 [1:15:43<50:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:15,206 >> Initializing global attention on CLS token...\n",
            " 58% 4820/8340 [1:15:44<50:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:16,066 >> Initializing global attention on CLS token...\n",
            " 58% 4821/8340 [1:15:45<50:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:16,920 >> Initializing global attention on CLS token...\n",
            " 58% 4822/8340 [1:15:46<50:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:17,775 >> Initializing global attention on CLS token...\n",
            " 58% 4823/8340 [1:15:47<50:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:18,638 >> Initializing global attention on CLS token...\n",
            " 58% 4824/8340 [1:15:48<50:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:19,503 >> Initializing global attention on CLS token...\n",
            " 58% 4825/8340 [1:15:49<50:27,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:20,363 >> Initializing global attention on CLS token...\n",
            " 58% 4826/8340 [1:15:49<50:22,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:21,220 >> Initializing global attention on CLS token...\n",
            " 58% 4827/8340 [1:15:50<50:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:22,077 >> Initializing global attention on CLS token...\n",
            " 58% 4828/8340 [1:15:51<50:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:22,929 >> Initializing global attention on CLS token...\n",
            " 58% 4829/8340 [1:15:52<50:31,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:23,805 >> Initializing global attention on CLS token...\n",
            " 58% 4830/8340 [1:15:53<50:20,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:24,659 >> Initializing global attention on CLS token...\n",
            " 58% 4831/8340 [1:15:54<50:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:25,514 >> Initializing global attention on CLS token...\n",
            " 58% 4832/8340 [1:15:55<50:13,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:26,374 >> Initializing global attention on CLS token...\n",
            " 58% 4833/8340 [1:15:55<50:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:27,233 >> Initializing global attention on CLS token...\n",
            " 58% 4834/8340 [1:15:56<50:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:28,089 >> Initializing global attention on CLS token...\n",
            " 58% 4835/8340 [1:15:57<50:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:28,940 >> Initializing global attention on CLS token...\n",
            " 58% 4836/8340 [1:15:58<49:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:29,793 >> Initializing global attention on CLS token...\n",
            " 58% 4837/8340 [1:15:59<49:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:30,643 >> Initializing global attention on CLS token...\n",
            " 58% 4838/8340 [1:16:00<49:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:31,498 >> Initializing global attention on CLS token...\n",
            " 58% 4839/8340 [1:16:01<49:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:32,355 >> Initializing global attention on CLS token...\n",
            " 58% 4840/8340 [1:16:01<49:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:33,206 >> Initializing global attention on CLS token...\n",
            " 58% 4841/8340 [1:16:02<49:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:34,060 >> Initializing global attention on CLS token...\n",
            " 58% 4842/8340 [1:16:03<49:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:34,910 >> Initializing global attention on CLS token...\n",
            " 58% 4843/8340 [1:16:04<49:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:35,765 >> Initializing global attention on CLS token...\n",
            " 58% 4844/8340 [1:16:05<49:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:36,617 >> Initializing global attention on CLS token...\n",
            " 58% 4845/8340 [1:16:06<49:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:37,473 >> Initializing global attention on CLS token...\n",
            " 58% 4846/8340 [1:16:07<49:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:38,327 >> Initializing global attention on CLS token...\n",
            " 58% 4847/8340 [1:16:07<49:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:39,181 >> Initializing global attention on CLS token...\n",
            " 58% 4848/8340 [1:16:08<49:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:40,037 >> Initializing global attention on CLS token...\n",
            " 58% 4849/8340 [1:16:09<49:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:40,889 >> Initializing global attention on CLS token...\n",
            " 58% 4850/8340 [1:16:10<49:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:41,744 >> Initializing global attention on CLS token...\n",
            " 58% 4851/8340 [1:16:11<49:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:42,597 >> Initializing global attention on CLS token...\n",
            " 58% 4852/8340 [1:16:12<49:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:43,452 >> Initializing global attention on CLS token...\n",
            " 58% 4853/8340 [1:16:13<49:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:44,303 >> Initializing global attention on CLS token...\n",
            " 58% 4854/8340 [1:16:13<49:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:45,159 >> Initializing global attention on CLS token...\n",
            " 58% 4855/8340 [1:16:14<49:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:46,015 >> Initializing global attention on CLS token...\n",
            " 58% 4856/8340 [1:16:15<49:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:46,868 >> Initializing global attention on CLS token...\n",
            " 58% 4857/8340 [1:16:16<49:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:47,723 >> Initializing global attention on CLS token...\n",
            " 58% 4858/8340 [1:16:17<49:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:48,576 >> Initializing global attention on CLS token...\n",
            " 58% 4859/8340 [1:16:18<49:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:49,438 >> Initializing global attention on CLS token...\n",
            " 58% 4860/8340 [1:16:19<49:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:50,288 >> Initializing global attention on CLS token...\n",
            " 58% 4861/8340 [1:16:19<49:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:51,138 >> Initializing global attention on CLS token...\n",
            " 58% 4862/8340 [1:16:20<49:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:52,000 >> Initializing global attention on CLS token...\n",
            " 58% 4863/8340 [1:16:21<49:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:52,886 >> Initializing global attention on CLS token...\n",
            " 58% 4864/8340 [1:16:22<50:11,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:53,752 >> Initializing global attention on CLS token...\n",
            " 58% 4865/8340 [1:16:23<50:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:54,606 >> Initializing global attention on CLS token...\n",
            " 58% 4866/8340 [1:16:24<49:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:55,453 >> Initializing global attention on CLS token...\n",
            " 58% 4867/8340 [1:16:25<49:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:56,306 >> Initializing global attention on CLS token...\n",
            " 58% 4868/8340 [1:16:25<49:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:57,159 >> Initializing global attention on CLS token...\n",
            " 58% 4869/8340 [1:16:26<49:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:58,012 >> Initializing global attention on CLS token...\n",
            " 58% 4870/8340 [1:16:27<49:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:58,865 >> Initializing global attention on CLS token...\n",
            " 58% 4871/8340 [1:16:28<49:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:14:59,723 >> Initializing global attention on CLS token...\n",
            " 58% 4872/8340 [1:16:29<49:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:00,575 >> Initializing global attention on CLS token...\n",
            " 58% 4873/8340 [1:16:30<49:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:01,432 >> Initializing global attention on CLS token...\n",
            " 58% 4874/8340 [1:16:31<49:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:02,285 >> Initializing global attention on CLS token...\n",
            " 58% 4875/8340 [1:16:31<49:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:03,141 >> Initializing global attention on CLS token...\n",
            " 58% 4876/8340 [1:16:32<49:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:03,993 >> Initializing global attention on CLS token...\n",
            " 58% 4877/8340 [1:16:33<49:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:04,849 >> Initializing global attention on CLS token...\n",
            " 58% 4878/8340 [1:16:34<49:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:05,701 >> Initializing global attention on CLS token...\n",
            " 59% 4879/8340 [1:16:35<49:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:06,556 >> Initializing global attention on CLS token...\n",
            " 59% 4880/8340 [1:16:36<49:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:07,412 >> Initializing global attention on CLS token...\n",
            " 59% 4881/8340 [1:16:37<49:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:08,265 >> Initializing global attention on CLS token...\n",
            " 59% 4882/8340 [1:16:37<49:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:09,118 >> Initializing global attention on CLS token...\n",
            " 59% 4883/8340 [1:16:38<49:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:09,972 >> Initializing global attention on CLS token...\n",
            " 59% 4884/8340 [1:16:39<49:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:10,825 >> Initializing global attention on CLS token...\n",
            " 59% 4885/8340 [1:16:40<49:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:11,679 >> Initializing global attention on CLS token...\n",
            " 59% 4886/8340 [1:16:41<49:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:12,534 >> Initializing global attention on CLS token...\n",
            " 59% 4887/8340 [1:16:42<49:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:13,389 >> Initializing global attention on CLS token...\n",
            " 59% 4888/8340 [1:16:42<49:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:14,243 >> Initializing global attention on CLS token...\n",
            " 59% 4889/8340 [1:16:43<49:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:15,098 >> Initializing global attention on CLS token...\n",
            " 59% 4890/8340 [1:16:44<49:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:15,952 >> Initializing global attention on CLS token...\n",
            " 59% 4891/8340 [1:16:45<49:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:16,805 >> Initializing global attention on CLS token...\n",
            " 59% 4892/8340 [1:16:46<49:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:17,661 >> Initializing global attention on CLS token...\n",
            " 59% 4893/8340 [1:16:47<49:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:18,517 >> Initializing global attention on CLS token...\n",
            " 59% 4894/8340 [1:16:48<49:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:19,372 >> Initializing global attention on CLS token...\n",
            " 59% 4895/8340 [1:16:48<49:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:20,224 >> Initializing global attention on CLS token...\n",
            " 59% 4896/8340 [1:16:49<49:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:21,080 >> Initializing global attention on CLS token...\n",
            " 59% 4897/8340 [1:16:50<49:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:21,937 >> Initializing global attention on CLS token...\n",
            " 59% 4898/8340 [1:16:51<49:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:22,792 >> Initializing global attention on CLS token...\n",
            " 59% 4899/8340 [1:16:52<49:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:23,648 >> Initializing global attention on CLS token...\n",
            " 59% 4900/8340 [1:16:53<48:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:24,494 >> Initializing global attention on CLS token...\n",
            " 59% 4901/8340 [1:16:54<48:45,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:25,340 >> Initializing global attention on CLS token...\n",
            " 59% 4902/8340 [1:16:54<48:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:26,198 >> Initializing global attention on CLS token...\n",
            " 59% 4903/8340 [1:16:55<48:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:27,047 >> Initializing global attention on CLS token...\n",
            " 59% 4904/8340 [1:16:56<48:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:27,904 >> Initializing global attention on CLS token...\n",
            " 59% 4905/8340 [1:16:57<48:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:28,757 >> Initializing global attention on CLS token...\n",
            " 59% 4906/8340 [1:16:58<48:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:29,608 >> Initializing global attention on CLS token...\n",
            " 59% 4907/8340 [1:16:59<48:41,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:30,455 >> Initializing global attention on CLS token...\n",
            " 59% 4908/8340 [1:17:00<48:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:31,311 >> Initializing global attention on CLS token...\n",
            " 59% 4909/8340 [1:17:00<48:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:32,166 >> Initializing global attention on CLS token...\n",
            " 59% 4910/8340 [1:17:01<48:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:33,021 >> Initializing global attention on CLS token...\n",
            " 59% 4911/8340 [1:17:02<48:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:33,876 >> Initializing global attention on CLS token...\n",
            " 59% 4912/8340 [1:17:03<48:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:34,727 >> Initializing global attention on CLS token...\n",
            " 59% 4913/8340 [1:17:04<48:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:35,579 >> Initializing global attention on CLS token...\n",
            " 59% 4914/8340 [1:17:05<48:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:36,434 >> Initializing global attention on CLS token...\n",
            " 59% 4915/8340 [1:17:06<48:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:37,283 >> Initializing global attention on CLS token...\n",
            " 59% 4916/8340 [1:17:06<48:33,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:38,131 >> Initializing global attention on CLS token...\n",
            " 59% 4917/8340 [1:17:07<48:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:38,985 >> Initializing global attention on CLS token...\n",
            " 59% 4918/8340 [1:17:08<48:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:39,835 >> Initializing global attention on CLS token...\n",
            " 59% 4919/8340 [1:17:09<48:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:40,688 >> Initializing global attention on CLS token...\n",
            " 59% 4920/8340 [1:17:10<48:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:41,541 >> Initializing global attention on CLS token...\n",
            " 59% 4921/8340 [1:17:11<48:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:42,395 >> Initializing global attention on CLS token...\n",
            " 59% 4922/8340 [1:17:11<48:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:43,255 >> Initializing global attention on CLS token...\n",
            " 59% 4923/8340 [1:17:12<48:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:44,104 >> Initializing global attention on CLS token...\n",
            " 59% 4924/8340 [1:17:13<48:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:44,958 >> Initializing global attention on CLS token...\n",
            " 59% 4925/8340 [1:17:14<48:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:45,811 >> Initializing global attention on CLS token...\n",
            " 59% 4926/8340 [1:17:15<48:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:46,668 >> Initializing global attention on CLS token...\n",
            " 59% 4927/8340 [1:17:16<48:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:47,519 >> Initializing global attention on CLS token...\n",
            " 59% 4928/8340 [1:17:17<48:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:48,373 >> Initializing global attention on CLS token...\n",
            " 59% 4929/8340 [1:17:17<48:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:49,229 >> Initializing global attention on CLS token...\n",
            " 59% 4930/8340 [1:17:18<48:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:50,086 >> Initializing global attention on CLS token...\n",
            " 59% 4931/8340 [1:17:19<48:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:50,940 >> Initializing global attention on CLS token...\n",
            " 59% 4932/8340 [1:17:20<48:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:51,793 >> Initializing global attention on CLS token...\n",
            " 59% 4933/8340 [1:17:21<48:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:52,646 >> Initializing global attention on CLS token...\n",
            " 59% 4934/8340 [1:17:22<48:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:53,502 >> Initializing global attention on CLS token...\n",
            " 59% 4935/8340 [1:17:23<48:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:54,356 >> Initializing global attention on CLS token...\n",
            " 59% 4936/8340 [1:17:23<48:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:55,210 >> Initializing global attention on CLS token...\n",
            " 59% 4937/8340 [1:17:24<48:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:56,070 >> Initializing global attention on CLS token...\n",
            " 59% 4938/8340 [1:17:25<48:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:56,918 >> Initializing global attention on CLS token...\n",
            " 59% 4939/8340 [1:17:26<48:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:57,770 >> Initializing global attention on CLS token...\n",
            " 59% 4940/8340 [1:17:27<48:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:58,629 >> Initializing global attention on CLS token...\n",
            " 59% 4941/8340 [1:17:28<48:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:15:59,478 >> Initializing global attention on CLS token...\n",
            " 59% 4942/8340 [1:17:29<48:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:00,334 >> Initializing global attention on CLS token...\n",
            " 59% 4943/8340 [1:17:29<48:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:01,188 >> Initializing global attention on CLS token...\n",
            " 59% 4944/8340 [1:17:30<48:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:02,040 >> Initializing global attention on CLS token...\n",
            " 59% 4945/8340 [1:17:31<48:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:02,895 >> Initializing global attention on CLS token...\n",
            " 59% 4946/8340 [1:17:32<48:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:03,750 >> Initializing global attention on CLS token...\n",
            " 59% 4947/8340 [1:17:33<48:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:04,604 >> Initializing global attention on CLS token...\n",
            " 59% 4948/8340 [1:17:34<48:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:05,456 >> Initializing global attention on CLS token...\n",
            " 59% 4949/8340 [1:17:35<48:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:06,312 >> Initializing global attention on CLS token...\n",
            " 59% 4950/8340 [1:17:35<48:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:07,164 >> Initializing global attention on CLS token...\n",
            " 59% 4951/8340 [1:17:36<48:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:08,019 >> Initializing global attention on CLS token...\n",
            " 59% 4952/8340 [1:17:37<48:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:08,875 >> Initializing global attention on CLS token...\n",
            " 59% 4953/8340 [1:17:38<48:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:09,730 >> Initializing global attention on CLS token...\n",
            " 59% 4954/8340 [1:17:39<48:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:10,582 >> Initializing global attention on CLS token...\n",
            " 59% 4955/8340 [1:17:40<48:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:11,432 >> Initializing global attention on CLS token...\n",
            " 59% 4956/8340 [1:17:41<48:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:12,286 >> Initializing global attention on CLS token...\n",
            " 59% 4957/8340 [1:17:41<48:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:13,140 >> Initializing global attention on CLS token...\n",
            " 59% 4958/8340 [1:17:42<48:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:13,998 >> Initializing global attention on CLS token...\n",
            " 59% 4959/8340 [1:17:43<48:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:14,848 >> Initializing global attention on CLS token...\n",
            " 59% 4960/8340 [1:17:44<48:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:15,702 >> Initializing global attention on CLS token...\n",
            " 59% 4961/8340 [1:17:45<48:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:16,560 >> Initializing global attention on CLS token...\n",
            " 59% 4962/8340 [1:17:46<48:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:17,412 >> Initializing global attention on CLS token...\n",
            " 60% 4963/8340 [1:17:47<48:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:18,263 >> Initializing global attention on CLS token...\n",
            " 60% 4964/8340 [1:17:47<48:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:19,121 >> Initializing global attention on CLS token...\n",
            " 60% 4965/8340 [1:17:48<48:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:19,973 >> Initializing global attention on CLS token...\n",
            " 60% 4966/8340 [1:17:49<48:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:20,827 >> Initializing global attention on CLS token...\n",
            " 60% 4967/8340 [1:17:50<48:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:21,682 >> Initializing global attention on CLS token...\n",
            " 60% 4968/8340 [1:17:51<47:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:22,537 >> Initializing global attention on CLS token...\n",
            " 60% 4969/8340 [1:17:52<48:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:23,394 >> Initializing global attention on CLS token...\n",
            " 60% 4970/8340 [1:17:52<47:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:24,246 >> Initializing global attention on CLS token...\n",
            " 60% 4971/8340 [1:17:53<47:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:25,101 >> Initializing global attention on CLS token...\n",
            " 60% 4972/8340 [1:17:54<47:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:25,955 >> Initializing global attention on CLS token...\n",
            " 60% 4973/8340 [1:17:55<47:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:26,811 >> Initializing global attention on CLS token...\n",
            " 60% 4974/8340 [1:17:56<47:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:27,665 >> Initializing global attention on CLS token...\n",
            " 60% 4975/8340 [1:17:57<47:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:28,520 >> Initializing global attention on CLS token...\n",
            " 60% 4976/8340 [1:17:58<47:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:29,378 >> Initializing global attention on CLS token...\n",
            " 60% 4977/8340 [1:17:58<47:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:30,232 >> Initializing global attention on CLS token...\n",
            " 60% 4978/8340 [1:17:59<47:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:31,088 >> Initializing global attention on CLS token...\n",
            " 60% 4979/8340 [1:18:00<47:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:31,940 >> Initializing global attention on CLS token...\n",
            " 60% 4980/8340 [1:18:01<47:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:32,795 >> Initializing global attention on CLS token...\n",
            " 60% 4981/8340 [1:18:02<47:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:33,651 >> Initializing global attention on CLS token...\n",
            " 60% 4982/8340 [1:18:03<47:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:34,507 >> Initializing global attention on CLS token...\n",
            " 60% 4983/8340 [1:18:04<47:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:35,365 >> Initializing global attention on CLS token...\n",
            " 60% 4984/8340 [1:18:04<47:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:36,222 >> Initializing global attention on CLS token...\n",
            " 60% 4985/8340 [1:18:05<47:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:37,081 >> Initializing global attention on CLS token...\n",
            " 60% 4986/8340 [1:18:06<47:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:37,941 >> Initializing global attention on CLS token...\n",
            " 60% 4987/8340 [1:18:07<47:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:38,797 >> Initializing global attention on CLS token...\n",
            " 60% 4988/8340 [1:18:08<47:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:39,646 >> Initializing global attention on CLS token...\n",
            " 60% 4989/8340 [1:18:09<47:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:40,503 >> Initializing global attention on CLS token...\n",
            " 60% 4990/8340 [1:18:10<47:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:41,379 >> Initializing global attention on CLS token...\n",
            " 60% 4991/8340 [1:18:10<48:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:42,251 >> Initializing global attention on CLS token...\n",
            " 60% 4992/8340 [1:18:11<48:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:43,119 >> Initializing global attention on CLS token...\n",
            " 60% 4993/8340 [1:18:12<48:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:43,973 >> Initializing global attention on CLS token...\n",
            " 60% 4994/8340 [1:18:13<48:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:44,841 >> Initializing global attention on CLS token...\n",
            " 60% 4995/8340 [1:18:14<48:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:45,760 >> Initializing global attention on CLS token...\n",
            " 60% 4996/8340 [1:18:15<49:03,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:46,618 >> Initializing global attention on CLS token...\n",
            " 60% 4997/8340 [1:18:16<48:40,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:47,482 >> Initializing global attention on CLS token...\n",
            " 60% 4998/8340 [1:18:17<48:26,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:48,338 >> Initializing global attention on CLS token...\n",
            " 60% 4999/8340 [1:18:17<48:09,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:49,189 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.0985, 'learning_rate': 2.0053956834532375e-05, 'epoch': 6.0}\n",
            " 60% 5000/8340 [1:18:18<50:12,  1.11it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:50,191 >> Initializing global attention on CLS token...\n",
            " 60% 5001/8340 [1:18:19<49:35,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:51,048 >> Initializing global attention on CLS token...\n",
            " 60% 5002/8340 [1:18:20<49:06,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:51,910 >> Initializing global attention on CLS token...\n",
            " 60% 5003/8340 [1:18:21<48:41,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:16:52,748 >> Initializing global attention on CLS token...\n",
            " 60% 5004/8340 [1:18:21<39:26,  1.41it/s][INFO|trainer.py:726] 2022-11-21 17:16:53,057 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2022-11-21 17:16:53,060 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-11-21 17:16:53,061 >>   Num examples = 1400\n",
            "[INFO|trainer.py:2912] 2022-11-21 17:16:53,061 >>   Batch size = 6\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:53,100 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 0/234 [00:00<?, ?it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:53,363 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 2/234 [00:00<00:30,  7.58it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:53,635 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 3/234 [00:00<00:44,  5.22it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:53,914 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 4/234 [00:00<00:51,  4.50it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:54,190 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 5/234 [00:01<00:55,  4.11it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:54,458 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 6/234 [00:01<00:56,  4.00it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:54,721 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 7/234 [00:01<00:57,  3.92it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:55,005 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 8/234 [00:01<00:59,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:55,289 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 9/234 [00:02<01:00,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:55,559 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 10/234 [00:02<01:00,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:55,834 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 11/234 [00:02<01:00,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:56,100 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 12/234 [00:03<00:59,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:56,381 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 13/234 [00:03<01:00,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:56,649 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 14/234 [00:03<00:59,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:56,917 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 15/234 [00:03<00:58,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:57,182 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 16/234 [00:04<00:58,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:57,448 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 17/234 [00:04<00:58,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:57,717 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 18/234 [00:04<00:58,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:57,986 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 19/234 [00:04<00:57,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:58,259 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 20/234 [00:05<00:57,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:58,519 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 21/234 [00:05<00:56,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:58,790 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 22/234 [00:05<00:56,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:59,067 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 23/234 [00:05<00:57,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:59,329 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 24/234 [00:06<00:56,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:59,595 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 25/234 [00:06<00:55,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:16:59,864 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 26/234 [00:06<00:55,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:00,147 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 27/234 [00:07<00:56,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:00,414 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 28/234 [00:07<00:55,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:00,684 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 29/234 [00:07<00:55,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:00,972 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 30/234 [00:07<00:56,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:01,253 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 31/234 [00:08<00:56,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:01,529 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 32/234 [00:08<00:55,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:01,810 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 33/234 [00:08<00:55,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:02,073 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 34/234 [00:08<00:54,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:02,340 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 35/234 [00:09<00:53,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:02,601 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 36/234 [00:09<00:53,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:02,879 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 37/234 [00:09<00:53,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:03,145 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 38/234 [00:10<00:53,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:03,421 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 39/234 [00:10<00:53,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:03,685 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 40/234 [00:10<00:52,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:03,957 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 41/234 [00:10<00:52,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:04,234 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 42/234 [00:11<00:52,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:04,503 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 43/234 [00:11<00:52,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:04,771 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 44/234 [00:11<00:51,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:05,035 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 45/234 [00:11<00:50,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:05,299 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 46/234 [00:12<00:50,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:05,569 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 47/234 [00:12<00:49,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:05,839 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 48/234 [00:12<00:49,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:06,126 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 49/234 [00:13<00:50,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:06,411 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 50/234 [00:13<00:51,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:06,680 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 51/234 [00:13<00:50,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:06,948 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 52/234 [00:13<00:49,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:07,212 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 53/234 [00:14<00:48,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:07,477 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 54/234 [00:14<00:48,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:07,738 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 55/234 [00:14<00:47,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:08,001 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 56/234 [00:14<00:47,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:08,264 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 57/234 [00:15<00:46,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:08,533 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 58/234 [00:15<00:46,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:08,793 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 59/234 [00:15<00:46,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:09,056 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 60/234 [00:15<00:46,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:09,321 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 61/234 [00:16<00:45,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:09,588 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 62/234 [00:16<00:45,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:09,849 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 63/234 [00:16<00:45,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:10,114 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 64/234 [00:17<00:44,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:10,376 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 65/234 [00:17<00:44,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:10,640 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 66/234 [00:17<00:44,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:10,906 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 67/234 [00:17<00:44,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:11,169 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 68/234 [00:18<00:43,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:11,430 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 69/234 [00:18<00:43,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:11,691 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 70/234 [00:18<00:43,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:11,960 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 71/234 [00:18<00:43,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:12,227 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 72/234 [00:19<00:42,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:12,489 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 73/234 [00:19<00:42,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:12,757 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 74/234 [00:19<00:42,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:13,019 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 75/234 [00:19<00:42,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:13,283 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 76/234 [00:20<00:41,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:13,548 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 77/234 [00:20<00:41,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:13,807 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 78/234 [00:20<00:41,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:14,078 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 79/234 [00:20<00:40,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:14,339 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 80/234 [00:21<00:40,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:14,602 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 81/234 [00:21<00:40,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:14,880 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 82/234 [00:21<00:40,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:15,150 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 83/234 [00:22<00:40,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:15,417 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 84/234 [00:22<00:40,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:15,684 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 85/234 [00:22<00:39,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:15,945 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 86/234 [00:22<00:39,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:16,213 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 87/234 [00:23<00:39,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:16,481 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 88/234 [00:23<00:38,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:16,740 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 89/234 [00:23<00:38,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:17,004 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 90/234 [00:23<00:38,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:17,270 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 91/234 [00:24<00:37,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:17,531 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 92/234 [00:24<00:37,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:17,792 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 93/234 [00:24<00:37,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:18,054 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 94/234 [00:24<00:36,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:18,321 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 95/234 [00:25<00:36,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:18,589 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 96/234 [00:25<00:36,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:18,850 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 97/234 [00:25<00:36,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:19,111 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 98/234 [00:26<00:35,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:19,380 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 99/234 [00:26<00:35,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:19,645 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 100/234 [00:26<00:35,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:19,905 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 101/234 [00:26<00:35,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:20,167 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 102/234 [00:27<00:34,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:20,434 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 103/234 [00:27<00:34,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:20,707 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 104/234 [00:27<00:34,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:20,970 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 105/234 [00:27<00:34,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:21,232 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 106/234 [00:28<00:33,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:21,492 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 107/234 [00:28<00:33,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:21,760 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 108/234 [00:28<00:33,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:22,022 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 109/234 [00:28<00:33,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:22,284 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 110/234 [00:29<00:32,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:22,565 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 111/234 [00:29<00:33,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:22,836 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 112/234 [00:29<00:32,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:23,095 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 113/234 [00:29<00:32,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:23,360 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 114/234 [00:30<00:31,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:23,630 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 115/234 [00:30<00:31,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:23,892 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 116/234 [00:30<00:31,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:24,153 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 117/234 [00:31<00:30,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:24,415 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 118/234 [00:31<00:30,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:24,684 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 119/234 [00:31<00:30,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:24,958 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 120/234 [00:31<00:30,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:25,224 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 121/234 [00:32<00:30,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:25,493 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 122/234 [00:32<00:29,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:25,758 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 123/234 [00:32<00:29,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:26,021 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 124/234 [00:32<00:29,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:26,287 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 125/234 [00:33<00:28,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:26,555 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 126/234 [00:33<00:28,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:26,817 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 127/234 [00:33<00:28,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:27,104 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 128/234 [00:34<00:28,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:27,370 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 129/234 [00:34<00:28,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:27,631 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 130/234 [00:34<00:27,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:27,909 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 131/234 [00:34<00:27,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:28,187 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 132/234 [00:35<00:27,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:28,455 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 133/234 [00:35<00:27,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:28,723 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 134/234 [00:35<00:27,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:28,991 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 135/234 [00:35<00:26,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:29,253 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 136/234 [00:36<00:26,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:29,518 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 137/234 [00:36<00:25,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:29,786 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 138/234 [00:36<00:25,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:30,047 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 139/234 [00:36<00:25,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:30,307 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 140/234 [00:37<00:24,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:30,573 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 141/234 [00:37<00:24,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:30,837 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 142/234 [00:37<00:24,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:31,106 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 143/234 [00:38<00:24,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:31,376 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 144/234 [00:38<00:24,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:31,660 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 145/234 [00:38<00:24,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:31,942 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 146/234 [00:38<00:24,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:32,221 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 147/234 [00:39<00:23,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:32,483 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 148/234 [00:39<00:23,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:32,746 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 149/234 [00:39<00:22,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:33,013 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 150/234 [00:39<00:22,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:33,278 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 151/234 [00:40<00:22,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:33,562 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 152/234 [00:40<00:22,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:33,830 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 153/234 [00:40<00:21,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:34,090 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 154/234 [00:40<00:21,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:34,355 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 155/234 [00:41<00:21,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:34,616 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 156/234 [00:41<00:20,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:34,898 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 157/234 [00:41<00:20,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:35,164 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 158/234 [00:42<00:20,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:35,431 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 159/234 [00:42<00:20,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:35,698 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 160/234 [00:42<00:19,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:35,980 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 161/234 [00:42<00:19,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:36,246 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 162/234 [00:43<00:19,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:36,520 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 163/234 [00:43<00:19,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:36,806 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 164/234 [00:43<00:19,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:37,090 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 165/234 [00:43<00:19,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:37,363 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 166/234 [00:44<00:18,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:37,630 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 167/234 [00:44<00:18,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:37,911 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 168/234 [00:44<00:18,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:38,176 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 169/234 [00:45<00:17,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:38,442 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 170/234 [00:45<00:17,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:38,703 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 171/234 [00:45<00:16,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:38,975 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 172/234 [00:45<00:16,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:39,248 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 173/234 [00:46<00:16,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:39,536 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 174/234 [00:46<00:16,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:39,812 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 175/234 [00:46<00:16,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:40,104 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 176/234 [00:47<00:16,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:40,388 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 177/234 [00:47<00:16,  3.52it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:40,689 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 178/234 [00:47<00:16,  3.48it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:40,982 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 179/234 [00:47<00:15,  3.47it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:41,268 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 180/234 [00:48<00:15,  3.49it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:41,551 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 181/234 [00:48<00:15,  3.50it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:41,833 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 182/234 [00:48<00:14,  3.51it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:42,114 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 183/234 [00:49<00:14,  3.51it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:42,396 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 184/234 [00:49<00:14,  3.53it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:42,673 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 185/234 [00:49<00:13,  3.57it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:42,944 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 186/234 [00:49<00:13,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:43,223 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 187/234 [00:50<00:13,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:43,501 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 188/234 [00:50<00:12,  3.58it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:43,790 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 189/234 [00:50<00:12,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:44,070 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 190/234 [00:50<00:12,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:44,356 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 191/234 [00:51<00:12,  3.53it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:44,660 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 192/234 [00:51<00:12,  3.46it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:44,924 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 193/234 [00:51<00:11,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:45,195 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 194/234 [00:52<00:11,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:45,478 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 195/234 [00:52<00:10,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:45,755 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 196/234 [00:52<00:10,  3.57it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:46,029 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 197/234 [00:52<00:10,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:46,310 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 198/234 [00:53<00:10,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:46,594 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 199/234 [00:53<00:09,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:46,879 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 200/234 [00:53<00:09,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:47,162 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 201/234 [00:54<00:09,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:47,445 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 202/234 [00:54<00:09,  3.53it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:47,733 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 203/234 [00:54<00:08,  3.52it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:48,019 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 204/234 [00:54<00:08,  3.50it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:48,315 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 205/234 [00:55<00:08,  3.45it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:48,609 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 206/234 [00:55<00:08,  3.46it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:48,895 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 207/234 [00:55<00:07,  3.48it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:49,161 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 208/234 [00:56<00:07,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:49,422 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 209/234 [00:56<00:06,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:49,707 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 210/234 [00:56<00:06,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:49,975 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 211/234 [00:56<00:06,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:50,243 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 212/234 [00:57<00:05,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:50,504 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 213/234 [00:57<00:05,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:50,777 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 214/234 [00:57<00:05,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:51,040 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 215/234 [00:57<00:05,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:51,303 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 216/234 [00:58<00:04,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:51,563 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 217/234 [00:58<00:04,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:51,829 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 218/234 [00:58<00:04,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:52,091 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 219/234 [00:58<00:03,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:52,355 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 220/234 [00:59<00:03,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:52,621 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 221/234 [00:59<00:03,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:52,880 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 222/234 [00:59<00:03,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:53,145 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 223/234 [01:00<00:02,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:53,411 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 224/234 [01:00<00:02,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:53,672 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 225/234 [01:00<00:02,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:53,936 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 226/234 [01:00<00:02,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:54,196 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 227/234 [01:01<00:01,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:54,462 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 228/234 [01:01<00:01,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:54,739 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 229/234 [01:01<00:01,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:55,001 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 230/234 [01:01<00:01,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:55,262 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 231/234 [01:02<00:00,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:55,522 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 232/234 [01:02<00:00,  3.82it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:55,781 >> Initializing global attention on CLS token...\n",
            "\n",
            "100% 233/234 [01:02<00:00,  3.82it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:17:56,028 >> Initializing global attention on CLS token...\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 1.9382017850875854, 'eval_f1-micro': 0.7128571428571427, 'eval_f1-macro': 0.5920602964352558, 'eval_accuracy': 0.7128571428571429, 'eval_runtime': 65.4026, 'eval_samples_per_second': 21.406, 'eval_steps_per_second': 3.578, 'epoch': 6.0}\n",
            " 60% 5004/8340 [1:19:27<39:26,  1.41it/s]\n",
            "100% 234/234 [01:05<00:00,  3.82it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-11-21 17:17:58,466 >> Saving model checkpoint to logs/output_1/checkpoint-5004\n",
            "[INFO|configuration_utils.py:447] 2022-11-21 17:17:58,467 >> Configuration saved in logs/output_1/checkpoint-5004/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-21 17:17:58,813 >> Model weights saved in logs/output_1/checkpoint-5004/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-21 17:17:58,814 >> tokenizer config file saved in logs/output_1/checkpoint-5004/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-21 17:17:58,815 >> Special tokens file saved in logs/output_1/checkpoint-5004/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-21 17:18:01,825 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-21 17:18:01,826 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-21 17:18:19,072 >> Initializing global attention on CLS token...\n",
            " 60% 5005/8340 [1:19:48<24:36:26, 26.56s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:19,973 >> Initializing global attention on CLS token...\n",
            " 60% 5006/8340 [1:19:49<17:27:15, 18.85s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:20,815 >> Initializing global attention on CLS token...\n",
            " 60% 5007/8340 [1:19:50<12:26:55, 13.45s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:21,659 >> Initializing global attention on CLS token...\n",
            " 60% 5008/8340 [1:19:51<8:56:50,  9.67s/it] [INFO|modeling_longformer.py:1932] 2022-11-21 17:18:22,513 >> Initializing global attention on CLS token...\n",
            " 60% 5009/8340 [1:19:52<6:29:56,  7.02s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:23,372 >> Initializing global attention on CLS token...\n",
            " 60% 5010/8340 [1:19:52<4:47:11,  5.17s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:24,229 >> Initializing global attention on CLS token...\n",
            " 60% 5011/8340 [1:19:53<3:35:14,  3.88s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:25,088 >> Initializing global attention on CLS token...\n",
            " 60% 5012/8340 [1:19:54<2:44:48,  2.97s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:25,938 >> Initializing global attention on CLS token...\n",
            " 60% 5013/8340 [1:19:55<2:09:31,  2.34s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:26,793 >> Initializing global attention on CLS token...\n",
            " 60% 5014/8340 [1:19:56<1:44:48,  1.89s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:27,644 >> Initializing global attention on CLS token...\n",
            " 60% 5015/8340 [1:19:57<1:27:35,  1.58s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:28,508 >> Initializing global attention on CLS token...\n",
            " 60% 5016/8340 [1:19:58<1:15:40,  1.37s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:29,382 >> Initializing global attention on CLS token...\n",
            " 60% 5017/8340 [1:19:58<1:07:33,  1.22s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:30,259 >> Initializing global attention on CLS token...\n",
            " 60% 5018/8340 [1:19:59<1:01:48,  1.12s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:31,119 >> Initializing global attention on CLS token...\n",
            " 60% 5019/8340 [1:20:00<57:28,  1.04s/it]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:18:31,975 >> Initializing global attention on CLS token...\n",
            " 60% 5020/8340 [1:20:01<54:24,  1.02it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:32,829 >> Initializing global attention on CLS token...\n",
            " 60% 5021/8340 [1:20:02<52:19,  1.06it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:33,689 >> Initializing global attention on CLS token...\n",
            " 60% 5022/8340 [1:20:03<51:00,  1.08it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:34,558 >> Initializing global attention on CLS token...\n",
            " 60% 5023/8340 [1:20:04<49:56,  1.11it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:35,416 >> Initializing global attention on CLS token...\n",
            " 60% 5024/8340 [1:20:05<49:13,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:36,281 >> Initializing global attention on CLS token...\n",
            " 60% 5025/8340 [1:20:05<48:45,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:37,141 >> Initializing global attention on CLS token...\n",
            " 60% 5026/8340 [1:20:06<48:25,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:38,003 >> Initializing global attention on CLS token...\n",
            " 60% 5027/8340 [1:20:07<48:13,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:38,872 >> Initializing global attention on CLS token...\n",
            " 60% 5028/8340 [1:20:08<48:10,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:39,765 >> Initializing global attention on CLS token...\n",
            " 60% 5029/8340 [1:20:09<48:29,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:40,646 >> Initializing global attention on CLS token...\n",
            " 60% 5030/8340 [1:20:10<48:38,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:41,528 >> Initializing global attention on CLS token...\n",
            " 60% 5031/8340 [1:20:11<48:37,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:42,405 >> Initializing global attention on CLS token...\n",
            " 60% 5032/8340 [1:20:12<48:38,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:43,310 >> Initializing global attention on CLS token...\n",
            " 60% 5033/8340 [1:20:12<48:44,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:44,178 >> Initializing global attention on CLS token...\n",
            " 60% 5034/8340 [1:20:13<48:28,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:45,051 >> Initializing global attention on CLS token...\n",
            " 60% 5035/8340 [1:20:14<48:25,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:45,941 >> Initializing global attention on CLS token...\n",
            " 60% 5036/8340 [1:20:15<48:28,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:46,827 >> Initializing global attention on CLS token...\n",
            " 60% 5037/8340 [1:20:16<48:38,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:47,706 >> Initializing global attention on CLS token...\n",
            " 60% 5038/8340 [1:20:17<48:43,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:48,609 >> Initializing global attention on CLS token...\n",
            " 60% 5039/8340 [1:20:18<48:45,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:49,475 >> Initializing global attention on CLS token...\n",
            " 60% 5040/8340 [1:20:19<48:27,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:50,350 >> Initializing global attention on CLS token...\n",
            " 60% 5041/8340 [1:20:19<48:32,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:51,255 >> Initializing global attention on CLS token...\n",
            " 60% 5042/8340 [1:20:20<48:41,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:52,130 >> Initializing global attention on CLS token...\n",
            " 60% 5043/8340 [1:20:21<48:46,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:53,036 >> Initializing global attention on CLS token...\n",
            " 60% 5044/8340 [1:20:22<48:47,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:53,905 >> Initializing global attention on CLS token...\n",
            " 60% 5045/8340 [1:20:23<48:22,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:54,769 >> Initializing global attention on CLS token...\n",
            " 61% 5046/8340 [1:20:24<48:04,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:55,637 >> Initializing global attention on CLS token...\n",
            " 61% 5047/8340 [1:20:25<47:57,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:56,504 >> Initializing global attention on CLS token...\n",
            " 61% 5048/8340 [1:20:26<47:49,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:57,377 >> Initializing global attention on CLS token...\n",
            " 61% 5049/8340 [1:20:26<47:51,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:58,257 >> Initializing global attention on CLS token...\n",
            " 61% 5050/8340 [1:20:27<48:03,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:18:59,138 >> Initializing global attention on CLS token...\n",
            " 61% 5051/8340 [1:20:28<47:57,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:00,007 >> Initializing global attention on CLS token...\n",
            " 61% 5052/8340 [1:20:29<47:53,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:00,875 >> Initializing global attention on CLS token...\n",
            " 61% 5053/8340 [1:20:30<47:35,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:01,730 >> Initializing global attention on CLS token...\n",
            " 61% 5054/8340 [1:20:31<47:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:02,587 >> Initializing global attention on CLS token...\n",
            " 61% 5055/8340 [1:20:32<47:08,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:03,436 >> Initializing global attention on CLS token...\n",
            " 61% 5056/8340 [1:20:33<47:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:04,291 >> Initializing global attention on CLS token...\n",
            " 61% 5057/8340 [1:20:33<46:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:05,147 >> Initializing global attention on CLS token...\n",
            " 61% 5058/8340 [1:20:34<46:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:05,998 >> Initializing global attention on CLS token...\n",
            " 61% 5059/8340 [1:20:35<46:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:06,858 >> Initializing global attention on CLS token...\n",
            " 61% 5060/8340 [1:20:36<46:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:07,716 >> Initializing global attention on CLS token...\n",
            " 61% 5061/8340 [1:20:37<46:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:08,574 >> Initializing global attention on CLS token...\n",
            " 61% 5062/8340 [1:20:38<46:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:09,440 >> Initializing global attention on CLS token...\n",
            " 61% 5063/8340 [1:20:39<46:54,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:10,294 >> Initializing global attention on CLS token...\n",
            " 61% 5064/8340 [1:20:39<46:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:11,155 >> Initializing global attention on CLS token...\n",
            " 61% 5065/8340 [1:20:40<46:54,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:12,015 >> Initializing global attention on CLS token...\n",
            " 61% 5066/8340 [1:20:41<46:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:12,876 >> Initializing global attention on CLS token...\n",
            " 61% 5067/8340 [1:20:42<46:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:13,730 >> Initializing global attention on CLS token...\n",
            " 61% 5068/8340 [1:20:43<46:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:14,581 >> Initializing global attention on CLS token...\n",
            " 61% 5069/8340 [1:20:44<46:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:15,438 >> Initializing global attention on CLS token...\n",
            " 61% 5070/8340 [1:20:45<46:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:16,304 >> Initializing global attention on CLS token...\n",
            " 61% 5071/8340 [1:20:45<46:48,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:17,162 >> Initializing global attention on CLS token...\n",
            " 61% 5072/8340 [1:20:46<46:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:18,020 >> Initializing global attention on CLS token...\n",
            " 61% 5073/8340 [1:20:47<46:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:18,872 >> Initializing global attention on CLS token...\n",
            " 61% 5074/8340 [1:20:48<46:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:19,727 >> Initializing global attention on CLS token...\n",
            " 61% 5075/8340 [1:20:49<46:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:20,581 >> Initializing global attention on CLS token...\n",
            " 61% 5076/8340 [1:20:50<46:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:21,436 >> Initializing global attention on CLS token...\n",
            " 61% 5077/8340 [1:20:51<46:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:22,289 >> Initializing global attention on CLS token...\n",
            " 61% 5078/8340 [1:20:51<46:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:23,145 >> Initializing global attention on CLS token...\n",
            " 61% 5079/8340 [1:20:52<46:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:23,998 >> Initializing global attention on CLS token...\n",
            " 61% 5080/8340 [1:20:53<46:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:24,854 >> Initializing global attention on CLS token...\n",
            " 61% 5081/8340 [1:20:54<46:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:25,731 >> Initializing global attention on CLS token...\n",
            " 61% 5082/8340 [1:20:55<46:40,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:26,583 >> Initializing global attention on CLS token...\n",
            " 61% 5083/8340 [1:20:56<46:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:27,432 >> Initializing global attention on CLS token...\n",
            " 61% 5084/8340 [1:20:57<46:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:28,289 >> Initializing global attention on CLS token...\n",
            " 61% 5085/8340 [1:20:57<46:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:29,157 >> Initializing global attention on CLS token...\n",
            " 61% 5086/8340 [1:20:58<46:41,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:30,018 >> Initializing global attention on CLS token...\n",
            " 61% 5087/8340 [1:20:59<47:05,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:30,922 >> Initializing global attention on CLS token...\n",
            " 61% 5088/8340 [1:21:00<47:23,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:31,809 >> Initializing global attention on CLS token...\n",
            " 61% 5089/8340 [1:21:01<47:17,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:32,661 >> Initializing global attention on CLS token...\n",
            " 61% 5090/8340 [1:21:02<47:10,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:33,531 >> Initializing global attention on CLS token...\n",
            " 61% 5091/8340 [1:21:03<46:58,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:34,402 >> Initializing global attention on CLS token...\n",
            " 61% 5092/8340 [1:21:04<47:19,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:35,338 >> Initializing global attention on CLS token...\n",
            " 61% 5093/8340 [1:21:04<48:05,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:36,197 >> Initializing global attention on CLS token...\n",
            " 61% 5094/8340 [1:21:05<47:31,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:37,054 >> Initializing global attention on CLS token...\n",
            " 61% 5095/8340 [1:21:06<47:05,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:37,904 >> Initializing global attention on CLS token...\n",
            " 61% 5096/8340 [1:21:07<46:49,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:38,759 >> Initializing global attention on CLS token...\n",
            " 61% 5097/8340 [1:21:08<46:36,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:39,615 >> Initializing global attention on CLS token...\n",
            " 61% 5098/8340 [1:21:09<46:28,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:40,471 >> Initializing global attention on CLS token...\n",
            " 61% 5099/8340 [1:21:10<46:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:41,324 >> Initializing global attention on CLS token...\n",
            " 61% 5100/8340 [1:21:10<46:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:42,177 >> Initializing global attention on CLS token...\n",
            " 61% 5101/8340 [1:21:11<46:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:43,031 >> Initializing global attention on CLS token...\n",
            " 61% 5102/8340 [1:21:12<46:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:43,885 >> Initializing global attention on CLS token...\n",
            " 61% 5103/8340 [1:21:13<46:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:44,738 >> Initializing global attention on CLS token...\n",
            " 61% 5104/8340 [1:21:14<46:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:45,600 >> Initializing global attention on CLS token...\n",
            " 61% 5105/8340 [1:21:15<46:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:46,456 >> Initializing global attention on CLS token...\n",
            " 61% 5106/8340 [1:21:16<46:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:47,326 >> Initializing global attention on CLS token...\n",
            " 61% 5107/8340 [1:21:16<46:19,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:48,190 >> Initializing global attention on CLS token...\n",
            " 61% 5108/8340 [1:21:17<46:29,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:49,059 >> Initializing global attention on CLS token...\n",
            " 61% 5109/8340 [1:21:18<46:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:49,918 >> Initializing global attention on CLS token...\n",
            " 61% 5110/8340 [1:21:19<46:27,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:50,782 >> Initializing global attention on CLS token...\n",
            " 61% 5111/8340 [1:21:20<46:25,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:51,662 >> Initializing global attention on CLS token...\n",
            " 61% 5112/8340 [1:21:21<46:39,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:52,528 >> Initializing global attention on CLS token...\n",
            " 61% 5113/8340 [1:21:22<46:39,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:53,391 >> Initializing global attention on CLS token...\n",
            " 61% 5114/8340 [1:21:22<46:30,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:54,249 >> Initializing global attention on CLS token...\n",
            " 61% 5115/8340 [1:21:23<46:28,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:55,109 >> Initializing global attention on CLS token...\n",
            " 61% 5116/8340 [1:21:24<46:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:55,972 >> Initializing global attention on CLS token...\n",
            " 61% 5117/8340 [1:21:25<46:23,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:56,836 >> Initializing global attention on CLS token...\n",
            " 61% 5118/8340 [1:21:26<46:22,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:57,706 >> Initializing global attention on CLS token...\n",
            " 61% 5119/8340 [1:21:27<46:23,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:58,573 >> Initializing global attention on CLS token...\n",
            " 61% 5120/8340 [1:21:28<46:40,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:19:59,474 >> Initializing global attention on CLS token...\n",
            " 61% 5121/8340 [1:21:29<47:02,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:00,346 >> Initializing global attention on CLS token...\n",
            " 61% 5122/8340 [1:21:29<46:54,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:01,210 >> Initializing global attention on CLS token...\n",
            " 61% 5123/8340 [1:21:30<46:34,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:02,063 >> Initializing global attention on CLS token...\n",
            " 61% 5124/8340 [1:21:31<46:20,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:02,919 >> Initializing global attention on CLS token...\n",
            " 61% 5125/8340 [1:21:32<46:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:03,777 >> Initializing global attention on CLS token...\n",
            " 61% 5126/8340 [1:21:33<46:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:04,636 >> Initializing global attention on CLS token...\n",
            " 61% 5127/8340 [1:21:34<46:06,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:05,493 >> Initializing global attention on CLS token...\n",
            " 61% 5128/8340 [1:21:35<46:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:06,358 >> Initializing global attention on CLS token...\n",
            " 61% 5129/8340 [1:21:35<46:02,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:07,215 >> Initializing global attention on CLS token...\n",
            " 62% 5130/8340 [1:21:36<45:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:08,071 >> Initializing global attention on CLS token...\n",
            " 62% 5131/8340 [1:21:37<45:57,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:08,929 >> Initializing global attention on CLS token...\n",
            " 62% 5132/8340 [1:21:38<45:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:09,787 >> Initializing global attention on CLS token...\n",
            " 62% 5133/8340 [1:21:39<45:58,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:10,650 >> Initializing global attention on CLS token...\n",
            " 62% 5134/8340 [1:21:40<45:55,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:11,508 >> Initializing global attention on CLS token...\n",
            " 62% 5135/8340 [1:21:41<45:52,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:12,369 >> Initializing global attention on CLS token...\n",
            " 62% 5136/8340 [1:21:41<45:59,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:13,236 >> Initializing global attention on CLS token...\n",
            " 62% 5137/8340 [1:21:42<45:54,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:14,093 >> Initializing global attention on CLS token...\n",
            " 62% 5138/8340 [1:21:43<45:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:14,942 >> Initializing global attention on CLS token...\n",
            " 62% 5139/8340 [1:21:44<45:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:15,800 >> Initializing global attention on CLS token...\n",
            " 62% 5140/8340 [1:21:45<45:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:16,654 >> Initializing global attention on CLS token...\n",
            " 62% 5141/8340 [1:21:46<45:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:17,523 >> Initializing global attention on CLS token...\n",
            " 62% 5142/8340 [1:21:47<45:51,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:18,382 >> Initializing global attention on CLS token...\n",
            " 62% 5143/8340 [1:21:47<45:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:19,237 >> Initializing global attention on CLS token...\n",
            " 62% 5144/8340 [1:21:48<45:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:20,101 >> Initializing global attention on CLS token...\n",
            " 62% 5145/8340 [1:21:49<45:52,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:20,965 >> Initializing global attention on CLS token...\n",
            " 62% 5146/8340 [1:21:50<45:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:21,819 >> Initializing global attention on CLS token...\n",
            " 62% 5147/8340 [1:21:51<45:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:22,681 >> Initializing global attention on CLS token...\n",
            " 62% 5148/8340 [1:21:52<45:43,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:23,539 >> Initializing global attention on CLS token...\n",
            " 62% 5149/8340 [1:21:53<45:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:24,401 >> Initializing global attention on CLS token...\n",
            " 62% 5150/8340 [1:21:54<45:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:25,269 >> Initializing global attention on CLS token...\n",
            " 62% 5151/8340 [1:21:54<45:53,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:26,132 >> Initializing global attention on CLS token...\n",
            " 62% 5152/8340 [1:21:55<45:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:26,988 >> Initializing global attention on CLS token...\n",
            " 62% 5153/8340 [1:21:56<45:37,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:27,842 >> Initializing global attention on CLS token...\n",
            " 62% 5154/8340 [1:21:57<45:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:28,697 >> Initializing global attention on CLS token...\n",
            " 62% 5155/8340 [1:21:58<45:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:29,548 >> Initializing global attention on CLS token...\n",
            " 62% 5156/8340 [1:21:59<45:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:30,409 >> Initializing global attention on CLS token...\n",
            " 62% 5157/8340 [1:22:00<45:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:31,269 >> Initializing global attention on CLS token...\n",
            " 62% 5158/8340 [1:22:00<45:34,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:32,131 >> Initializing global attention on CLS token...\n",
            " 62% 5159/8340 [1:22:01<45:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:32,988 >> Initializing global attention on CLS token...\n",
            " 62% 5160/8340 [1:22:02<45:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:33,842 >> Initializing global attention on CLS token...\n",
            " 62% 5161/8340 [1:22:03<45:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:34,696 >> Initializing global attention on CLS token...\n",
            " 62% 5162/8340 [1:22:04<45:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:35,548 >> Initializing global attention on CLS token...\n",
            " 62% 5163/8340 [1:22:05<45:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:36,406 >> Initializing global attention on CLS token...\n",
            " 62% 5164/8340 [1:22:06<45:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:37,264 >> Initializing global attention on CLS token...\n",
            " 62% 5165/8340 [1:22:06<45:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:38,128 >> Initializing global attention on CLS token...\n",
            " 62% 5166/8340 [1:22:07<45:40,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:38,999 >> Initializing global attention on CLS token...\n",
            " 62% 5167/8340 [1:22:08<45:41,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:39,868 >> Initializing global attention on CLS token...\n",
            " 62% 5168/8340 [1:22:09<45:36,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:40,723 >> Initializing global attention on CLS token...\n",
            " 62% 5169/8340 [1:22:10<45:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:41,583 >> Initializing global attention on CLS token...\n",
            " 62% 5170/8340 [1:22:11<45:31,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:42,445 >> Initializing global attention on CLS token...\n",
            " 62% 5171/8340 [1:22:12<45:26,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:43,304 >> Initializing global attention on CLS token...\n",
            " 62% 5172/8340 [1:22:12<45:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:44,150 >> Initializing global attention on CLS token...\n",
            " 62% 5173/8340 [1:22:13<45:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:45,007 >> Initializing global attention on CLS token...\n",
            " 62% 5174/8340 [1:22:14<45:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:45,860 >> Initializing global attention on CLS token...\n",
            " 62% 5175/8340 [1:22:15<45:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:46,736 >> Initializing global attention on CLS token...\n",
            " 62% 5176/8340 [1:22:16<45:20,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:47,586 >> Initializing global attention on CLS token...\n",
            " 62% 5177/8340 [1:22:17<45:15,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:48,440 >> Initializing global attention on CLS token...\n",
            " 62% 5178/8340 [1:22:18<45:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:49,294 >> Initializing global attention on CLS token...\n",
            " 62% 5179/8340 [1:22:18<45:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:50,180 >> Initializing global attention on CLS token...\n",
            " 62% 5180/8340 [1:22:19<45:40,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:51,051 >> Initializing global attention on CLS token...\n",
            " 62% 5181/8340 [1:22:20<45:40,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:51,910 >> Initializing global attention on CLS token...\n",
            " 62% 5182/8340 [1:22:21<45:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:52,772 >> Initializing global attention on CLS token...\n",
            " 62% 5183/8340 [1:22:22<45:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:53,624 >> Initializing global attention on CLS token...\n",
            " 62% 5184/8340 [1:22:23<45:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:54,478 >> Initializing global attention on CLS token...\n",
            " 62% 5185/8340 [1:22:24<45:09,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:55,342 >> Initializing global attention on CLS token...\n",
            " 62% 5186/8340 [1:22:24<45:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:56,199 >> Initializing global attention on CLS token...\n",
            " 62% 5187/8340 [1:22:25<45:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:57,087 >> Initializing global attention on CLS token...\n",
            " 62% 5188/8340 [1:22:26<45:39,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:57,965 >> Initializing global attention on CLS token...\n",
            " 62% 5189/8340 [1:22:27<45:36,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:58,836 >> Initializing global attention on CLS token...\n",
            " 62% 5190/8340 [1:22:28<45:44,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:20:59,692 >> Initializing global attention on CLS token...\n",
            " 62% 5191/8340 [1:22:29<45:24,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:00,541 >> Initializing global attention on CLS token...\n",
            " 62% 5192/8340 [1:22:30<45:17,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:01,414 >> Initializing global attention on CLS token...\n",
            " 62% 5193/8340 [1:22:31<45:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:02,267 >> Initializing global attention on CLS token...\n",
            " 62% 5194/8340 [1:22:31<45:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:03,127 >> Initializing global attention on CLS token...\n",
            " 62% 5195/8340 [1:22:32<45:15,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:03,989 >> Initializing global attention on CLS token...\n",
            " 62% 5196/8340 [1:22:33<45:03,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:04,842 >> Initializing global attention on CLS token...\n",
            " 62% 5197/8340 [1:22:34<45:02,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:05,702 >> Initializing global attention on CLS token...\n",
            " 62% 5198/8340 [1:22:35<45:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:06,559 >> Initializing global attention on CLS token...\n",
            " 62% 5199/8340 [1:22:36<44:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:07,413 >> Initializing global attention on CLS token...\n",
            " 62% 5200/8340 [1:22:37<44:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:08,263 >> Initializing global attention on CLS token...\n",
            " 62% 5201/8340 [1:22:37<44:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:09,121 >> Initializing global attention on CLS token...\n",
            " 62% 5202/8340 [1:22:38<44:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:09,974 >> Initializing global attention on CLS token...\n",
            " 62% 5203/8340 [1:22:39<44:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:10,830 >> Initializing global attention on CLS token...\n",
            " 62% 5204/8340 [1:22:40<44:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:11,686 >> Initializing global attention on CLS token...\n",
            " 62% 5205/8340 [1:22:41<44:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:12,542 >> Initializing global attention on CLS token...\n",
            " 62% 5206/8340 [1:22:42<44:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:13,397 >> Initializing global attention on CLS token...\n",
            " 62% 5207/8340 [1:22:42<44:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:14,253 >> Initializing global attention on CLS token...\n",
            " 62% 5208/8340 [1:22:43<44:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:15,105 >> Initializing global attention on CLS token...\n",
            " 62% 5209/8340 [1:22:44<44:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:15,954 >> Initializing global attention on CLS token...\n",
            " 62% 5210/8340 [1:22:45<44:21,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:16,799 >> Initializing global attention on CLS token...\n",
            " 62% 5211/8340 [1:22:46<44:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:17,651 >> Initializing global attention on CLS token...\n",
            " 62% 5212/8340 [1:22:47<44:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:18,506 >> Initializing global attention on CLS token...\n",
            " 63% 5213/8340 [1:22:48<44:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:19,364 >> Initializing global attention on CLS token...\n",
            " 63% 5214/8340 [1:22:48<44:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:20,214 >> Initializing global attention on CLS token...\n",
            " 63% 5215/8340 [1:22:49<44:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:21,070 >> Initializing global attention on CLS token...\n",
            " 63% 5216/8340 [1:22:50<44:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:21,923 >> Initializing global attention on CLS token...\n",
            " 63% 5217/8340 [1:22:51<44:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:22,780 >> Initializing global attention on CLS token...\n",
            " 63% 5218/8340 [1:22:52<44:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:23,637 >> Initializing global attention on CLS token...\n",
            " 63% 5219/8340 [1:22:53<44:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:24,500 >> Initializing global attention on CLS token...\n",
            " 63% 5220/8340 [1:22:54<44:39,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:25,378 >> Initializing global attention on CLS token...\n",
            " 63% 5221/8340 [1:22:54<45:01,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:26,241 >> Initializing global attention on CLS token...\n",
            " 63% 5222/8340 [1:22:55<44:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:27,090 >> Initializing global attention on CLS token...\n",
            " 63% 5223/8340 [1:22:56<44:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:27,941 >> Initializing global attention on CLS token...\n",
            " 63% 5224/8340 [1:22:57<44:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:28,797 >> Initializing global attention on CLS token...\n",
            " 63% 5225/8340 [1:22:58<44:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:29,653 >> Initializing global attention on CLS token...\n",
            " 63% 5226/8340 [1:22:59<44:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:30,505 >> Initializing global attention on CLS token...\n",
            " 63% 5227/8340 [1:23:00<44:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:31,362 >> Initializing global attention on CLS token...\n",
            " 63% 5228/8340 [1:23:00<44:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:32,227 >> Initializing global attention on CLS token...\n",
            " 63% 5229/8340 [1:23:01<44:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:33,095 >> Initializing global attention on CLS token...\n",
            " 63% 5230/8340 [1:23:02<44:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:33,942 >> Initializing global attention on CLS token...\n",
            " 63% 5231/8340 [1:23:03<44:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:34,794 >> Initializing global attention on CLS token...\n",
            " 63% 5232/8340 [1:23:04<44:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:35,650 >> Initializing global attention on CLS token...\n",
            " 63% 5233/8340 [1:23:05<44:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:36,503 >> Initializing global attention on CLS token...\n",
            " 63% 5234/8340 [1:23:06<44:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:37,357 >> Initializing global attention on CLS token...\n",
            " 63% 5235/8340 [1:23:06<44:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:38,225 >> Initializing global attention on CLS token...\n",
            " 63% 5236/8340 [1:23:07<44:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:39,079 >> Initializing global attention on CLS token...\n",
            " 63% 5237/8340 [1:23:08<44:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:39,931 >> Initializing global attention on CLS token...\n",
            " 63% 5238/8340 [1:23:09<44:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:40,806 >> Initializing global attention on CLS token...\n",
            " 63% 5239/8340 [1:23:10<44:39,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:41,669 >> Initializing global attention on CLS token...\n",
            " 63% 5240/8340 [1:23:11<44:30,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:42,524 >> Initializing global attention on CLS token...\n",
            " 63% 5241/8340 [1:23:12<44:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:43,377 >> Initializing global attention on CLS token...\n",
            " 63% 5242/8340 [1:23:12<44:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:44,234 >> Initializing global attention on CLS token...\n",
            " 63% 5243/8340 [1:23:13<44:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:45,090 >> Initializing global attention on CLS token...\n",
            " 63% 5244/8340 [1:23:14<44:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:45,943 >> Initializing global attention on CLS token...\n",
            " 63% 5245/8340 [1:23:15<44:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:46,805 >> Initializing global attention on CLS token...\n",
            " 63% 5246/8340 [1:23:16<44:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:47,654 >> Initializing global attention on CLS token...\n",
            " 63% 5247/8340 [1:23:17<44:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:48,517 >> Initializing global attention on CLS token...\n",
            " 63% 5248/8340 [1:23:18<44:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:49,383 >> Initializing global attention on CLS token...\n",
            " 63% 5249/8340 [1:23:18<44:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:50,242 >> Initializing global attention on CLS token...\n",
            " 63% 5250/8340 [1:23:19<44:15,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:51,119 >> Initializing global attention on CLS token...\n",
            " 63% 5251/8340 [1:23:20<44:27,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:51,970 >> Initializing global attention on CLS token...\n",
            " 63% 5252/8340 [1:23:21<44:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:52,829 >> Initializing global attention on CLS token...\n",
            " 63% 5253/8340 [1:23:22<44:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:53,685 >> Initializing global attention on CLS token...\n",
            " 63% 5254/8340 [1:23:23<44:14,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:54,551 >> Initializing global attention on CLS token...\n",
            " 63% 5255/8340 [1:23:24<44:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:55,413 >> Initializing global attention on CLS token...\n",
            " 63% 5256/8340 [1:23:25<44:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:56,264 >> Initializing global attention on CLS token...\n",
            " 63% 5257/8340 [1:23:25<43:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:57,110 >> Initializing global attention on CLS token...\n",
            " 63% 5258/8340 [1:23:26<43:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:57,963 >> Initializing global attention on CLS token...\n",
            " 63% 5259/8340 [1:23:27<43:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:58,816 >> Initializing global attention on CLS token...\n",
            " 63% 5260/8340 [1:23:28<43:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:21:59,671 >> Initializing global attention on CLS token...\n",
            " 63% 5261/8340 [1:23:29<43:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:00,527 >> Initializing global attention on CLS token...\n",
            " 63% 5262/8340 [1:23:30<43:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:01,386 >> Initializing global attention on CLS token...\n",
            " 63% 5263/8340 [1:23:30<43:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:02,237 >> Initializing global attention on CLS token...\n",
            " 63% 5264/8340 [1:23:31<43:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:03,088 >> Initializing global attention on CLS token...\n",
            " 63% 5265/8340 [1:23:32<43:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:03,940 >> Initializing global attention on CLS token...\n",
            " 63% 5266/8340 [1:23:33<43:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:04,795 >> Initializing global attention on CLS token...\n",
            " 63% 5267/8340 [1:23:34<43:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:05,649 >> Initializing global attention on CLS token...\n",
            " 63% 5268/8340 [1:23:35<43:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:06,502 >> Initializing global attention on CLS token...\n",
            " 63% 5269/8340 [1:23:36<43:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:07,357 >> Initializing global attention on CLS token...\n",
            " 63% 5270/8340 [1:23:36<43:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:08,210 >> Initializing global attention on CLS token...\n",
            " 63% 5271/8340 [1:23:37<43:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:09,068 >> Initializing global attention on CLS token...\n",
            " 63% 5272/8340 [1:23:38<43:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:09,919 >> Initializing global attention on CLS token...\n",
            " 63% 5273/8340 [1:23:39<43:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:10,778 >> Initializing global attention on CLS token...\n",
            " 63% 5274/8340 [1:23:40<43:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:11,629 >> Initializing global attention on CLS token...\n",
            " 63% 5275/8340 [1:23:41<43:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:12,483 >> Initializing global attention on CLS token...\n",
            " 63% 5276/8340 [1:23:42<43:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:13,338 >> Initializing global attention on CLS token...\n",
            " 63% 5277/8340 [1:23:42<43:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:14,195 >> Initializing global attention on CLS token...\n",
            " 63% 5278/8340 [1:23:43<43:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:15,050 >> Initializing global attention on CLS token...\n",
            " 63% 5279/8340 [1:23:44<43:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:15,905 >> Initializing global attention on CLS token...\n",
            " 63% 5280/8340 [1:23:45<43:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:16,760 >> Initializing global attention on CLS token...\n",
            " 63% 5281/8340 [1:23:46<43:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:17,616 >> Initializing global attention on CLS token...\n",
            " 63% 5282/8340 [1:23:47<43:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:18,471 >> Initializing global attention on CLS token...\n",
            " 63% 5283/8340 [1:23:48<43:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:19,330 >> Initializing global attention on CLS token...\n",
            " 63% 5284/8340 [1:23:48<43:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:20,185 >> Initializing global attention on CLS token...\n",
            " 63% 5285/8340 [1:23:49<43:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:21,039 >> Initializing global attention on CLS token...\n",
            " 63% 5286/8340 [1:23:50<43:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:21,898 >> Initializing global attention on CLS token...\n",
            " 63% 5287/8340 [1:23:51<43:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:22,752 >> Initializing global attention on CLS token...\n",
            " 63% 5288/8340 [1:23:52<43:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:23,607 >> Initializing global attention on CLS token...\n",
            " 63% 5289/8340 [1:23:53<43:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:24,463 >> Initializing global attention on CLS token...\n",
            " 63% 5290/8340 [1:23:54<43:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:25,314 >> Initializing global attention on CLS token...\n",
            " 63% 5291/8340 [1:23:54<43:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:26,166 >> Initializing global attention on CLS token...\n",
            " 63% 5292/8340 [1:23:55<43:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:27,019 >> Initializing global attention on CLS token...\n",
            " 63% 5293/8340 [1:23:56<43:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:27,874 >> Initializing global attention on CLS token...\n",
            " 63% 5294/8340 [1:23:57<43:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:28,729 >> Initializing global attention on CLS token...\n",
            " 63% 5295/8340 [1:23:58<43:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:29,581 >> Initializing global attention on CLS token...\n",
            " 64% 5296/8340 [1:23:59<43:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:30,438 >> Initializing global attention on CLS token...\n",
            " 64% 5297/8340 [1:24:00<43:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:31,296 >> Initializing global attention on CLS token...\n",
            " 64% 5298/8340 [1:24:00<43:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:32,164 >> Initializing global attention on CLS token...\n",
            " 64% 5299/8340 [1:24:01<43:36,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:33,024 >> Initializing global attention on CLS token...\n",
            " 64% 5300/8340 [1:24:02<43:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:33,886 >> Initializing global attention on CLS token...\n",
            " 64% 5301/8340 [1:24:03<43:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:34,733 >> Initializing global attention on CLS token...\n",
            " 64% 5302/8340 [1:24:04<43:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:35,589 >> Initializing global attention on CLS token...\n",
            " 64% 5303/8340 [1:24:05<43:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:36,446 >> Initializing global attention on CLS token...\n",
            " 64% 5304/8340 [1:24:06<43:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:37,322 >> Initializing global attention on CLS token...\n",
            " 64% 5305/8340 [1:24:06<43:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:38,209 >> Initializing global attention on CLS token...\n",
            " 64% 5306/8340 [1:24:07<43:59,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:39,066 >> Initializing global attention on CLS token...\n",
            " 64% 5307/8340 [1:24:08<43:46,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:39,944 >> Initializing global attention on CLS token...\n",
            " 64% 5308/8340 [1:24:09<44:00,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:40,838 >> Initializing global attention on CLS token...\n",
            " 64% 5309/8340 [1:24:10<44:21,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:41,699 >> Initializing global attention on CLS token...\n",
            " 64% 5310/8340 [1:24:11<43:56,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:42,552 >> Initializing global attention on CLS token...\n",
            " 64% 5311/8340 [1:24:12<43:38,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:43,402 >> Initializing global attention on CLS token...\n",
            " 64% 5312/8340 [1:24:12<43:22,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:44,251 >> Initializing global attention on CLS token...\n",
            " 64% 5313/8340 [1:24:13<43:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:45,131 >> Initializing global attention on CLS token...\n",
            " 64% 5314/8340 [1:24:14<43:50,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:46,004 >> Initializing global attention on CLS token...\n",
            " 64% 5315/8340 [1:24:15<43:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:46,854 >> Initializing global attention on CLS token...\n",
            " 64% 5316/8340 [1:24:16<43:24,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:47,710 >> Initializing global attention on CLS token...\n",
            " 64% 5317/8340 [1:24:17<43:15,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:48,562 >> Initializing global attention on CLS token...\n",
            " 64% 5318/8340 [1:24:18<43:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:49,414 >> Initializing global attention on CLS token...\n",
            " 64% 5319/8340 [1:24:19<43:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:50,274 >> Initializing global attention on CLS token...\n",
            " 64% 5320/8340 [1:24:19<43:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:51,123 >> Initializing global attention on CLS token...\n",
            " 64% 5321/8340 [1:24:20<43:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:51,979 >> Initializing global attention on CLS token...\n",
            " 64% 5322/8340 [1:24:21<42:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:52,835 >> Initializing global attention on CLS token...\n",
            " 64% 5323/8340 [1:24:22<42:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:53,685 >> Initializing global attention on CLS token...\n",
            " 64% 5324/8340 [1:24:23<42:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:54,538 >> Initializing global attention on CLS token...\n",
            " 64% 5325/8340 [1:24:24<42:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:55,393 >> Initializing global attention on CLS token...\n",
            " 64% 5326/8340 [1:24:24<42:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:56,250 >> Initializing global attention on CLS token...\n",
            " 64% 5327/8340 [1:24:25<42:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:57,123 >> Initializing global attention on CLS token...\n",
            " 64% 5328/8340 [1:24:26<43:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:58,007 >> Initializing global attention on CLS token...\n",
            " 64% 5329/8340 [1:24:27<43:53,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:58,911 >> Initializing global attention on CLS token...\n",
            " 64% 5330/8340 [1:24:28<44:05,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:22:59,803 >> Initializing global attention on CLS token...\n",
            " 64% 5331/8340 [1:24:29<44:19,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:00,694 >> Initializing global attention on CLS token...\n",
            " 64% 5332/8340 [1:24:30<44:21,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:01,562 >> Initializing global attention on CLS token...\n",
            " 64% 5333/8340 [1:24:31<43:57,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:02,420 >> Initializing global attention on CLS token...\n",
            " 64% 5334/8340 [1:24:32<43:45,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:03,309 >> Initializing global attention on CLS token...\n",
            " 64% 5335/8340 [1:24:33<46:03,  1.09it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:04,345 >> Initializing global attention on CLS token...\n",
            " 64% 5336/8340 [1:24:33<45:46,  1.09it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:05,220 >> Initializing global attention on CLS token...\n",
            " 64% 5337/8340 [1:24:34<44:54,  1.11it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:06,076 >> Initializing global attention on CLS token...\n",
            " 64% 5338/8340 [1:24:35<44:22,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:06,949 >> Initializing global attention on CLS token...\n",
            " 64% 5339/8340 [1:24:36<44:23,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:07,854 >> Initializing global attention on CLS token...\n",
            " 64% 5340/8340 [1:24:37<44:39,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:08,755 >> Initializing global attention on CLS token...\n",
            " 64% 5341/8340 [1:24:38<44:35,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:09,640 >> Initializing global attention on CLS token...\n",
            " 64% 5342/8340 [1:24:39<44:24,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:10,500 >> Initializing global attention on CLS token...\n",
            " 64% 5343/8340 [1:24:40<43:49,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:11,352 >> Initializing global attention on CLS token...\n",
            " 64% 5344/8340 [1:24:40<43:27,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:12,204 >> Initializing global attention on CLS token...\n",
            " 64% 5345/8340 [1:24:41<43:11,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:13,056 >> Initializing global attention on CLS token...\n",
            " 64% 5346/8340 [1:24:42<43:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:13,912 >> Initializing global attention on CLS token...\n",
            " 64% 5347/8340 [1:24:43<42:55,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:14,768 >> Initializing global attention on CLS token...\n",
            " 64% 5348/8340 [1:24:44<42:48,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:15,622 >> Initializing global attention on CLS token...\n",
            " 64% 5349/8340 [1:24:45<42:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:16,478 >> Initializing global attention on CLS token...\n",
            " 64% 5350/8340 [1:24:46<42:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:17,332 >> Initializing global attention on CLS token...\n",
            " 64% 5351/8340 [1:24:46<42:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:18,190 >> Initializing global attention on CLS token...\n",
            " 64% 5352/8340 [1:24:47<42:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:19,040 >> Initializing global attention on CLS token...\n",
            " 64% 5353/8340 [1:24:48<42:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:19,897 >> Initializing global attention on CLS token...\n",
            " 64% 5354/8340 [1:24:49<42:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:20,752 >> Initializing global attention on CLS token...\n",
            " 64% 5355/8340 [1:24:50<42:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:21,606 >> Initializing global attention on CLS token...\n",
            " 64% 5356/8340 [1:24:51<42:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:22,458 >> Initializing global attention on CLS token...\n",
            " 64% 5357/8340 [1:24:52<42:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:23,314 >> Initializing global attention on CLS token...\n",
            " 64% 5358/8340 [1:24:52<42:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:24,168 >> Initializing global attention on CLS token...\n",
            " 64% 5359/8340 [1:24:53<42:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:25,037 >> Initializing global attention on CLS token...\n",
            " 64% 5360/8340 [1:24:54<42:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:25,887 >> Initializing global attention on CLS token...\n",
            " 64% 5361/8340 [1:24:55<42:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:26,744 >> Initializing global attention on CLS token...\n",
            " 64% 5362/8340 [1:24:56<42:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:27,603 >> Initializing global attention on CLS token...\n",
            " 64% 5363/8340 [1:24:57<42:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:28,459 >> Initializing global attention on CLS token...\n",
            " 64% 5364/8340 [1:24:58<42:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:29,308 >> Initializing global attention on CLS token...\n",
            " 64% 5365/8340 [1:24:58<42:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:30,163 >> Initializing global attention on CLS token...\n",
            " 64% 5366/8340 [1:24:59<42:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:31,019 >> Initializing global attention on CLS token...\n",
            " 64% 5367/8340 [1:25:00<42:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:31,871 >> Initializing global attention on CLS token...\n",
            " 64% 5368/8340 [1:25:01<42:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:32,725 >> Initializing global attention on CLS token...\n",
            " 64% 5369/8340 [1:25:02<42:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:33,583 >> Initializing global attention on CLS token...\n",
            " 64% 5370/8340 [1:25:03<42:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:34,435 >> Initializing global attention on CLS token...\n",
            " 64% 5371/8340 [1:25:04<42:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:35,291 >> Initializing global attention on CLS token...\n",
            " 64% 5372/8340 [1:25:04<42:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:36,142 >> Initializing global attention on CLS token...\n",
            " 64% 5373/8340 [1:25:05<42:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:36,997 >> Initializing global attention on CLS token...\n",
            " 64% 5374/8340 [1:25:06<42:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:37,848 >> Initializing global attention on CLS token...\n",
            " 64% 5375/8340 [1:25:07<42:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:38,702 >> Initializing global attention on CLS token...\n",
            " 64% 5376/8340 [1:25:08<42:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:39,561 >> Initializing global attention on CLS token...\n",
            " 64% 5377/8340 [1:25:09<42:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:40,418 >> Initializing global attention on CLS token...\n",
            " 64% 5378/8340 [1:25:10<42:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:41,292 >> Initializing global attention on CLS token...\n",
            " 64% 5379/8340 [1:25:10<42:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:42,149 >> Initializing global attention on CLS token...\n",
            " 65% 5380/8340 [1:25:11<42:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:42,995 >> Initializing global attention on CLS token...\n",
            " 65% 5381/8340 [1:25:12<42:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:43,853 >> Initializing global attention on CLS token...\n",
            " 65% 5382/8340 [1:25:13<42:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:44,706 >> Initializing global attention on CLS token...\n",
            " 65% 5383/8340 [1:25:14<42:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:45,558 >> Initializing global attention on CLS token...\n",
            " 65% 5384/8340 [1:25:15<42:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:46,410 >> Initializing global attention on CLS token...\n",
            " 65% 5385/8340 [1:25:16<42:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:47,264 >> Initializing global attention on CLS token...\n",
            " 65% 5386/8340 [1:25:16<42:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:48,121 >> Initializing global attention on CLS token...\n",
            " 65% 5387/8340 [1:25:17<42:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:48,975 >> Initializing global attention on CLS token...\n",
            " 65% 5388/8340 [1:25:18<42:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:49,830 >> Initializing global attention on CLS token...\n",
            " 65% 5389/8340 [1:25:19<42:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:50,686 >> Initializing global attention on CLS token...\n",
            " 65% 5390/8340 [1:25:20<42:15,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:51,574 >> Initializing global attention on CLS token...\n",
            " 65% 5391/8340 [1:25:21<42:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:52,435 >> Initializing global attention on CLS token...\n",
            " 65% 5392/8340 [1:25:22<42:24,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:53,292 >> Initializing global attention on CLS token...\n",
            " 65% 5393/8340 [1:25:22<42:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:54,145 >> Initializing global attention on CLS token...\n",
            " 65% 5394/8340 [1:25:23<42:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:54,997 >> Initializing global attention on CLS token...\n",
            " 65% 5395/8340 [1:25:24<42:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:55,850 >> Initializing global attention on CLS token...\n",
            " 65% 5396/8340 [1:25:25<41:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:56,704 >> Initializing global attention on CLS token...\n",
            " 65% 5397/8340 [1:25:26<41:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:57,559 >> Initializing global attention on CLS token...\n",
            " 65% 5398/8340 [1:25:27<41:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:58,414 >> Initializing global attention on CLS token...\n",
            " 65% 5399/8340 [1:25:28<41:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:23:59,280 >> Initializing global attention on CLS token...\n",
            " 65% 5400/8340 [1:25:28<42:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:00,133 >> Initializing global attention on CLS token...\n",
            " 65% 5401/8340 [1:25:29<41:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:00,986 >> Initializing global attention on CLS token...\n",
            " 65% 5402/8340 [1:25:30<41:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:01,839 >> Initializing global attention on CLS token...\n",
            " 65% 5403/8340 [1:25:31<41:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:02,701 >> Initializing global attention on CLS token...\n",
            " 65% 5404/8340 [1:25:32<42:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:03,559 >> Initializing global attention on CLS token...\n",
            " 65% 5405/8340 [1:25:33<41:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:04,415 >> Initializing global attention on CLS token...\n",
            " 65% 5406/8340 [1:25:34<41:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:05,276 >> Initializing global attention on CLS token...\n",
            " 65% 5407/8340 [1:25:34<41:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:06,126 >> Initializing global attention on CLS token...\n",
            " 65% 5408/8340 [1:25:35<41:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:06,977 >> Initializing global attention on CLS token...\n",
            " 65% 5409/8340 [1:25:36<41:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:07,840 >> Initializing global attention on CLS token...\n",
            " 65% 5410/8340 [1:25:37<41:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:08,693 >> Initializing global attention on CLS token...\n",
            " 65% 5411/8340 [1:25:38<41:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:09,543 >> Initializing global attention on CLS token...\n",
            " 65% 5412/8340 [1:25:39<41:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:10,399 >> Initializing global attention on CLS token...\n",
            " 65% 5413/8340 [1:25:39<41:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:11,252 >> Initializing global attention on CLS token...\n",
            " 65% 5414/8340 [1:25:40<41:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:12,105 >> Initializing global attention on CLS token...\n",
            " 65% 5415/8340 [1:25:41<41:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:12,957 >> Initializing global attention on CLS token...\n",
            " 65% 5416/8340 [1:25:42<41:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:13,815 >> Initializing global attention on CLS token...\n",
            " 65% 5417/8340 [1:25:43<41:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:14,668 >> Initializing global attention on CLS token...\n",
            " 65% 5418/8340 [1:25:44<41:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:15,520 >> Initializing global attention on CLS token...\n",
            " 65% 5419/8340 [1:25:45<41:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:16,393 >> Initializing global attention on CLS token...\n",
            " 65% 5420/8340 [1:25:45<41:50,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:17,249 >> Initializing global attention on CLS token...\n",
            " 65% 5421/8340 [1:25:46<41:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:18,104 >> Initializing global attention on CLS token...\n",
            " 65% 5422/8340 [1:25:47<41:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:18,961 >> Initializing global attention on CLS token...\n",
            " 65% 5423/8340 [1:25:48<41:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:19,815 >> Initializing global attention on CLS token...\n",
            " 65% 5424/8340 [1:25:49<41:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:20,670 >> Initializing global attention on CLS token...\n",
            " 65% 5425/8340 [1:25:50<41:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:21,535 >> Initializing global attention on CLS token...\n",
            " 65% 5426/8340 [1:25:51<41:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:22,384 >> Initializing global attention on CLS token...\n",
            " 65% 5427/8340 [1:25:51<41:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:23,239 >> Initializing global attention on CLS token...\n",
            " 65% 5428/8340 [1:25:52<41:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:24,091 >> Initializing global attention on CLS token...\n",
            " 65% 5429/8340 [1:25:53<41:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:24,960 >> Initializing global attention on CLS token...\n",
            " 65% 5430/8340 [1:25:54<41:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:25,809 >> Initializing global attention on CLS token...\n",
            " 65% 5431/8340 [1:25:55<41:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:26,660 >> Initializing global attention on CLS token...\n",
            " 65% 5432/8340 [1:25:56<41:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:27,513 >> Initializing global attention on CLS token...\n",
            " 65% 5433/8340 [1:25:57<41:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:28,364 >> Initializing global attention on CLS token...\n",
            " 65% 5434/8340 [1:25:57<41:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:29,224 >> Initializing global attention on CLS token...\n",
            " 65% 5435/8340 [1:25:58<41:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:30,079 >> Initializing global attention on CLS token...\n",
            " 65% 5436/8340 [1:25:59<41:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:30,944 >> Initializing global attention on CLS token...\n",
            " 65% 5437/8340 [1:26:00<41:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:31,804 >> Initializing global attention on CLS token...\n",
            " 65% 5438/8340 [1:26:01<41:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:32,661 >> Initializing global attention on CLS token...\n",
            " 65% 5439/8340 [1:26:02<41:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:33,598 >> Initializing global attention on CLS token...\n",
            " 65% 5440/8340 [1:26:03<44:07,  1.10it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:34,584 >> Initializing global attention on CLS token...\n",
            " 65% 5441/8340 [1:26:04<44:08,  1.09it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:35,563 >> Initializing global attention on CLS token...\n",
            " 65% 5442/8340 [1:26:05<45:46,  1.06it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:36,523 >> Initializing global attention on CLS token...\n",
            " 65% 5443/8340 [1:26:06<44:53,  1.08it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:37,397 >> Initializing global attention on CLS token...\n",
            " 65% 5444/8340 [1:26:06<43:46,  1.10it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:38,245 >> Initializing global attention on CLS token...\n",
            " 65% 5445/8340 [1:26:07<42:56,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:39,097 >> Initializing global attention on CLS token...\n",
            " 65% 5446/8340 [1:26:08<42:27,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:39,956 >> Initializing global attention on CLS token...\n",
            " 65% 5447/8340 [1:26:09<42:00,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:40,804 >> Initializing global attention on CLS token...\n",
            " 65% 5448/8340 [1:26:10<41:45,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:41,659 >> Initializing global attention on CLS token...\n",
            " 65% 5449/8340 [1:26:11<41:31,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:42,510 >> Initializing global attention on CLS token...\n",
            " 65% 5450/8340 [1:26:12<41:24,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:43,366 >> Initializing global attention on CLS token...\n",
            " 65% 5451/8340 [1:26:12<41:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:44,219 >> Initializing global attention on CLS token...\n",
            " 65% 5452/8340 [1:26:13<41:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:45,076 >> Initializing global attention on CLS token...\n",
            " 65% 5453/8340 [1:26:14<41:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:45,930 >> Initializing global attention on CLS token...\n",
            " 65% 5454/8340 [1:26:15<41:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:46,779 >> Initializing global attention on CLS token...\n",
            " 65% 5455/8340 [1:26:16<41:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:47,636 >> Initializing global attention on CLS token...\n",
            " 65% 5456/8340 [1:26:17<41:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:48,491 >> Initializing global attention on CLS token...\n",
            " 65% 5457/8340 [1:26:18<41:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:49,342 >> Initializing global attention on CLS token...\n",
            " 65% 5458/8340 [1:26:18<41:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:50,195 >> Initializing global attention on CLS token...\n",
            " 65% 5459/8340 [1:26:19<41:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:51,049 >> Initializing global attention on CLS token...\n",
            " 65% 5460/8340 [1:26:20<41:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:51,916 >> Initializing global attention on CLS token...\n",
            " 65% 5461/8340 [1:26:21<41:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:52,766 >> Initializing global attention on CLS token...\n",
            " 65% 5462/8340 [1:26:22<41:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:53,636 >> Initializing global attention on CLS token...\n",
            " 66% 5463/8340 [1:26:23<42:25,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:54,592 >> Initializing global attention on CLS token...\n",
            " 66% 5464/8340 [1:26:24<42:18,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:55,450 >> Initializing global attention on CLS token...\n",
            " 66% 5465/8340 [1:26:25<41:49,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:56,298 >> Initializing global attention on CLS token...\n",
            " 66% 5466/8340 [1:26:25<41:37,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:57,174 >> Initializing global attention on CLS token...\n",
            " 66% 5467/8340 [1:26:26<41:36,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:58,027 >> Initializing global attention on CLS token...\n",
            " 66% 5468/8340 [1:26:27<41:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:58,879 >> Initializing global attention on CLS token...\n",
            " 66% 5469/8340 [1:26:28<41:14,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:24:59,736 >> Initializing global attention on CLS token...\n",
            " 66% 5470/8340 [1:26:29<41:07,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:00,591 >> Initializing global attention on CLS token...\n",
            " 66% 5471/8340 [1:26:30<41:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:01,456 >> Initializing global attention on CLS token...\n",
            " 66% 5472/8340 [1:26:31<41:06,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:02,320 >> Initializing global attention on CLS token...\n",
            " 66% 5473/8340 [1:26:31<41:11,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:03,178 >> Initializing global attention on CLS token...\n",
            " 66% 5474/8340 [1:26:32<40:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:04,028 >> Initializing global attention on CLS token...\n",
            " 66% 5475/8340 [1:26:33<40:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:04,881 >> Initializing global attention on CLS token...\n",
            " 66% 5476/8340 [1:26:34<40:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:05,739 >> Initializing global attention on CLS token...\n",
            " 66% 5477/8340 [1:26:35<40:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:06,589 >> Initializing global attention on CLS token...\n",
            " 66% 5478/8340 [1:26:36<40:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:07,443 >> Initializing global attention on CLS token...\n",
            " 66% 5479/8340 [1:26:37<40:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:08,295 >> Initializing global attention on CLS token...\n",
            " 66% 5480/8340 [1:26:37<40:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:09,152 >> Initializing global attention on CLS token...\n",
            " 66% 5481/8340 [1:26:38<40:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:10,006 >> Initializing global attention on CLS token...\n",
            " 66% 5482/8340 [1:26:39<40:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:10,858 >> Initializing global attention on CLS token...\n",
            " 66% 5483/8340 [1:26:40<40:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:11,710 >> Initializing global attention on CLS token...\n",
            " 66% 5484/8340 [1:26:41<40:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:12,573 >> Initializing global attention on CLS token...\n",
            " 66% 5485/8340 [1:26:42<40:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:13,426 >> Initializing global attention on CLS token...\n",
            " 66% 5486/8340 [1:26:43<40:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:14,278 >> Initializing global attention on CLS token...\n",
            " 66% 5487/8340 [1:26:43<40:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:15,130 >> Initializing global attention on CLS token...\n",
            " 66% 5488/8340 [1:26:44<40:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:15,998 >> Initializing global attention on CLS token...\n",
            " 66% 5489/8340 [1:26:45<40:48,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:16,853 >> Initializing global attention on CLS token...\n",
            " 66% 5490/8340 [1:26:46<40:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:17,714 >> Initializing global attention on CLS token...\n",
            " 66% 5491/8340 [1:26:47<40:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:18,564 >> Initializing global attention on CLS token...\n",
            " 66% 5492/8340 [1:26:48<40:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:19,415 >> Initializing global attention on CLS token...\n",
            " 66% 5493/8340 [1:26:49<40:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:20,271 >> Initializing global attention on CLS token...\n",
            " 66% 5494/8340 [1:26:49<40:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:21,123 >> Initializing global attention on CLS token...\n",
            " 66% 5495/8340 [1:26:50<40:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:21,985 >> Initializing global attention on CLS token...\n",
            " 66% 5496/8340 [1:26:51<40:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:22,850 >> Initializing global attention on CLS token...\n",
            " 66% 5497/8340 [1:26:52<40:50,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:23,718 >> Initializing global attention on CLS token...\n",
            " 66% 5498/8340 [1:26:53<40:50,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:24,586 >> Initializing global attention on CLS token...\n",
            " 66% 5499/8340 [1:26:54<40:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:25,442 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.0585, 'learning_rate': 1.7056354916067148e-05, 'epoch': 6.59}\n",
            " 66% 5500/8340 [1:26:55<42:38,  1.11it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:26,454 >> Initializing global attention on CLS token...\n",
            " 66% 5501/8340 [1:26:56<42:20,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:27,309 >> Initializing global attention on CLS token...\n",
            " 66% 5502/8340 [1:26:56<41:45,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:28,173 >> Initializing global attention on CLS token...\n",
            " 66% 5503/8340 [1:26:57<41:25,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:29,034 >> Initializing global attention on CLS token...\n",
            " 66% 5504/8340 [1:26:58<41:15,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:29,890 >> Initializing global attention on CLS token...\n",
            " 66% 5505/8340 [1:26:59<40:58,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:30,745 >> Initializing global attention on CLS token...\n",
            " 66% 5506/8340 [1:27:00<40:50,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:31,603 >> Initializing global attention on CLS token...\n",
            " 66% 5507/8340 [1:27:01<40:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:32,474 >> Initializing global attention on CLS token...\n",
            " 66% 5508/8340 [1:27:02<40:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:33,332 >> Initializing global attention on CLS token...\n",
            " 66% 5509/8340 [1:27:02<40:40,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:34,189 >> Initializing global attention on CLS token...\n",
            " 66% 5510/8340 [1:27:03<40:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:35,042 >> Initializing global attention on CLS token...\n",
            " 66% 5511/8340 [1:27:04<40:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:35,897 >> Initializing global attention on CLS token...\n",
            " 66% 5512/8340 [1:27:05<40:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:36,753 >> Initializing global attention on CLS token...\n",
            " 66% 5513/8340 [1:27:06<40:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:37,607 >> Initializing global attention on CLS token...\n",
            " 66% 5514/8340 [1:27:07<40:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:38,460 >> Initializing global attention on CLS token...\n",
            " 66% 5515/8340 [1:27:08<40:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:39,317 >> Initializing global attention on CLS token...\n",
            " 66% 5516/8340 [1:27:08<40:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:40,183 >> Initializing global attention on CLS token...\n",
            " 66% 5517/8340 [1:27:09<40:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:41,029 >> Initializing global attention on CLS token...\n",
            " 66% 5518/8340 [1:27:10<40:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:41,884 >> Initializing global attention on CLS token...\n",
            " 66% 5519/8340 [1:27:11<40:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:42,737 >> Initializing global attention on CLS token...\n",
            " 66% 5520/8340 [1:27:12<40:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:43,597 >> Initializing global attention on CLS token...\n",
            " 66% 5521/8340 [1:27:13<40:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:44,451 >> Initializing global attention on CLS token...\n",
            " 66% 5522/8340 [1:27:14<40:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:45,305 >> Initializing global attention on CLS token...\n",
            " 66% 5523/8340 [1:27:14<40:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:46,161 >> Initializing global attention on CLS token...\n",
            " 66% 5524/8340 [1:27:15<40:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:47,014 >> Initializing global attention on CLS token...\n",
            " 66% 5525/8340 [1:27:16<40:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:47,870 >> Initializing global attention on CLS token...\n",
            " 66% 5526/8340 [1:27:17<40:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:48,721 >> Initializing global attention on CLS token...\n",
            " 66% 5527/8340 [1:27:18<40:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:49,573 >> Initializing global attention on CLS token...\n",
            " 66% 5528/8340 [1:27:19<40:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:50,428 >> Initializing global attention on CLS token...\n",
            " 66% 5529/8340 [1:27:20<40:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:51,283 >> Initializing global attention on CLS token...\n",
            " 66% 5530/8340 [1:27:20<40:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:52,139 >> Initializing global attention on CLS token...\n",
            " 66% 5531/8340 [1:27:21<40:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:52,992 >> Initializing global attention on CLS token...\n",
            " 66% 5532/8340 [1:27:22<39:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:53,847 >> Initializing global attention on CLS token...\n",
            " 66% 5533/8340 [1:27:23<39:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:54,701 >> Initializing global attention on CLS token...\n",
            " 66% 5534/8340 [1:27:24<39:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:55,553 >> Initializing global attention on CLS token...\n",
            " 66% 5535/8340 [1:27:25<39:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:56,408 >> Initializing global attention on CLS token...\n",
            " 66% 5536/8340 [1:27:26<39:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:57,267 >> Initializing global attention on CLS token...\n",
            " 66% 5537/8340 [1:27:26<39:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:58,116 >> Initializing global attention on CLS token...\n",
            " 66% 5538/8340 [1:27:27<39:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:58,968 >> Initializing global attention on CLS token...\n",
            " 66% 5539/8340 [1:27:28<39:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:25:59,821 >> Initializing global attention on CLS token...\n",
            " 66% 5540/8340 [1:27:29<39:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:00,700 >> Initializing global attention on CLS token...\n",
            " 66% 5541/8340 [1:27:30<40:18,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:01,564 >> Initializing global attention on CLS token...\n",
            " 66% 5542/8340 [1:27:31<40:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:02,410 >> Initializing global attention on CLS token...\n",
            " 66% 5543/8340 [1:27:32<39:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:03,275 >> Initializing global attention on CLS token...\n",
            " 66% 5544/8340 [1:27:32<40:01,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:04,128 >> Initializing global attention on CLS token...\n",
            " 66% 5545/8340 [1:27:33<39:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:04,979 >> Initializing global attention on CLS token...\n",
            " 66% 5546/8340 [1:27:34<39:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:05,827 >> Initializing global attention on CLS token...\n",
            " 67% 5547/8340 [1:27:35<39:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:06,683 >> Initializing global attention on CLS token...\n",
            " 67% 5548/8340 [1:27:36<39:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:07,531 >> Initializing global attention on CLS token...\n",
            " 67% 5549/8340 [1:27:37<39:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:08,387 >> Initializing global attention on CLS token...\n",
            " 67% 5550/8340 [1:27:37<39:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:09,238 >> Initializing global attention on CLS token...\n",
            " 67% 5551/8340 [1:27:38<39:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:10,094 >> Initializing global attention on CLS token...\n",
            " 67% 5552/8340 [1:27:39<39:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:10,947 >> Initializing global attention on CLS token...\n",
            " 67% 5553/8340 [1:27:40<39:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:11,803 >> Initializing global attention on CLS token...\n",
            " 67% 5554/8340 [1:27:41<39:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:12,656 >> Initializing global attention on CLS token...\n",
            " 67% 5555/8340 [1:27:42<39:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:13,510 >> Initializing global attention on CLS token...\n",
            " 67% 5556/8340 [1:27:43<39:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:14,379 >> Initializing global attention on CLS token...\n",
            " 67% 5557/8340 [1:27:43<39:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:15,232 >> Initializing global attention on CLS token...\n",
            " 67% 5558/8340 [1:27:44<39:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:16,085 >> Initializing global attention on CLS token...\n",
            " 67% 5559/8340 [1:27:45<39:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:16,962 >> Initializing global attention on CLS token...\n",
            " 67% 5560/8340 [1:27:46<40:19,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:17,873 >> Initializing global attention on CLS token...\n",
            " 67% 5561/8340 [1:27:47<40:25,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:18,723 >> Initializing global attention on CLS token...\n",
            " 67% 5562/8340 [1:27:48<40:12,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:19,602 >> Initializing global attention on CLS token...\n",
            " 67% 5563/8340 [1:27:49<40:13,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:20,451 >> Initializing global attention on CLS token...\n",
            " 67% 5564/8340 [1:27:50<40:00,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:21,318 >> Initializing global attention on CLS token...\n",
            " 67% 5565/8340 [1:27:50<40:07,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:22,189 >> Initializing global attention on CLS token...\n",
            " 67% 5566/8340 [1:27:51<39:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:23,035 >> Initializing global attention on CLS token...\n",
            " 67% 5567/8340 [1:27:52<39:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:23,887 >> Initializing global attention on CLS token...\n",
            " 67% 5568/8340 [1:27:53<39:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:24,751 >> Initializing global attention on CLS token...\n",
            " 67% 5569/8340 [1:27:54<39:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:25,606 >> Initializing global attention on CLS token...\n",
            " 67% 5570/8340 [1:27:55<39:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:26,455 >> Initializing global attention on CLS token...\n",
            " 67% 5571/8340 [1:27:56<39:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:27,309 >> Initializing global attention on CLS token...\n",
            " 67% 5572/8340 [1:27:56<39:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:28,160 >> Initializing global attention on CLS token...\n",
            " 67% 5573/8340 [1:27:57<39:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:29,012 >> Initializing global attention on CLS token...\n",
            " 67% 5574/8340 [1:27:58<39:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:29,866 >> Initializing global attention on CLS token...\n",
            " 67% 5575/8340 [1:27:59<39:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:30,724 >> Initializing global attention on CLS token...\n",
            " 67% 5576/8340 [1:28:00<39:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:31,574 >> Initializing global attention on CLS token...\n",
            " 67% 5577/8340 [1:28:01<39:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:32,428 >> Initializing global attention on CLS token...\n",
            " 67% 5578/8340 [1:28:02<39:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:33,281 >> Initializing global attention on CLS token...\n",
            " 67% 5579/8340 [1:28:02<39:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:34,137 >> Initializing global attention on CLS token...\n",
            " 67% 5580/8340 [1:28:03<39:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:34,993 >> Initializing global attention on CLS token...\n",
            " 67% 5581/8340 [1:28:04<39:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:35,850 >> Initializing global attention on CLS token...\n",
            " 67% 5582/8340 [1:28:05<39:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:36,702 >> Initializing global attention on CLS token...\n",
            " 67% 5583/8340 [1:28:06<39:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:37,558 >> Initializing global attention on CLS token...\n",
            " 67% 5584/8340 [1:28:07<39:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:38,412 >> Initializing global attention on CLS token...\n",
            " 67% 5585/8340 [1:28:08<39:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:39,267 >> Initializing global attention on CLS token...\n",
            " 67% 5586/8340 [1:28:08<39:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:40,121 >> Initializing global attention on CLS token...\n",
            " 67% 5587/8340 [1:28:09<39:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:40,990 >> Initializing global attention on CLS token...\n",
            " 67% 5588/8340 [1:28:10<39:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:41,844 >> Initializing global attention on CLS token...\n",
            " 67% 5589/8340 [1:28:11<39:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:42,702 >> Initializing global attention on CLS token...\n",
            " 67% 5590/8340 [1:28:12<39:22,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:43,563 >> Initializing global attention on CLS token...\n",
            " 67% 5591/8340 [1:28:13<39:26,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:44,449 >> Initializing global attention on CLS token...\n",
            " 67% 5592/8340 [1:28:14<39:39,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:45,325 >> Initializing global attention on CLS token...\n",
            " 67% 5593/8340 [1:28:14<40:24,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:46,247 >> Initializing global attention on CLS token...\n",
            " 67% 5594/8340 [1:28:15<40:20,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:47,121 >> Initializing global attention on CLS token...\n",
            " 67% 5595/8340 [1:28:16<40:16,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:47,997 >> Initializing global attention on CLS token...\n",
            " 67% 5596/8340 [1:28:17<40:03,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:48,848 >> Initializing global attention on CLS token...\n",
            " 67% 5597/8340 [1:28:18<39:50,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:49,732 >> Initializing global attention on CLS token...\n",
            " 67% 5598/8340 [1:28:19<40:01,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:50,603 >> Initializing global attention on CLS token...\n",
            " 67% 5599/8340 [1:28:20<39:48,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:51,462 >> Initializing global attention on CLS token...\n",
            " 67% 5600/8340 [1:28:21<39:40,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:52,344 >> Initializing global attention on CLS token...\n",
            " 67% 5601/8340 [1:28:21<39:48,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:53,199 >> Initializing global attention on CLS token...\n",
            " 67% 5602/8340 [1:28:22<39:35,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:54,056 >> Initializing global attention on CLS token...\n",
            " 67% 5603/8340 [1:28:23<39:20,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:54,905 >> Initializing global attention on CLS token...\n",
            " 67% 5604/8340 [1:28:24<39:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:55,759 >> Initializing global attention on CLS token...\n",
            " 67% 5605/8340 [1:28:25<39:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:56,614 >> Initializing global attention on CLS token...\n",
            " 67% 5606/8340 [1:28:26<39:08,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:57,497 >> Initializing global attention on CLS token...\n",
            " 67% 5607/8340 [1:28:27<39:21,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:58,372 >> Initializing global attention on CLS token...\n",
            " 67% 5608/8340 [1:28:27<39:27,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:26:59,245 >> Initializing global attention on CLS token...\n",
            " 67% 5609/8340 [1:28:28<39:42,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:00,132 >> Initializing global attention on CLS token...\n",
            " 67% 5610/8340 [1:28:29<40:23,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:01,032 >> Initializing global attention on CLS token...\n",
            " 67% 5611/8340 [1:28:30<39:55,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:01,889 >> Initializing global attention on CLS token...\n",
            " 67% 5612/8340 [1:28:31<39:36,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:02,745 >> Initializing global attention on CLS token...\n",
            " 67% 5613/8340 [1:28:32<39:22,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:03,598 >> Initializing global attention on CLS token...\n",
            " 67% 5614/8340 [1:28:33<39:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:04,475 >> Initializing global attention on CLS token...\n",
            " 67% 5615/8340 [1:28:34<39:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:05,322 >> Initializing global attention on CLS token...\n",
            " 67% 5616/8340 [1:28:34<39:07,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:06,177 >> Initializing global attention on CLS token...\n",
            " 67% 5617/8340 [1:28:35<39:06,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:07,038 >> Initializing global attention on CLS token...\n",
            " 67% 5618/8340 [1:28:36<38:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:07,888 >> Initializing global attention on CLS token...\n",
            " 67% 5619/8340 [1:28:37<38:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:08,741 >> Initializing global attention on CLS token...\n",
            " 67% 5620/8340 [1:28:38<38:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:09,594 >> Initializing global attention on CLS token...\n",
            " 67% 5621/8340 [1:28:39<38:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:10,452 >> Initializing global attention on CLS token...\n",
            " 67% 5622/8340 [1:28:40<38:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:11,303 >> Initializing global attention on CLS token...\n",
            " 67% 5623/8340 [1:28:40<38:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:12,159 >> Initializing global attention on CLS token...\n",
            " 67% 5624/8340 [1:28:41<38:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:13,016 >> Initializing global attention on CLS token...\n",
            " 67% 5625/8340 [1:28:42<38:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:13,866 >> Initializing global attention on CLS token...\n",
            " 67% 5626/8340 [1:28:43<38:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:14,723 >> Initializing global attention on CLS token...\n",
            " 67% 5627/8340 [1:28:44<38:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:15,598 >> Initializing global attention on CLS token...\n",
            " 67% 5628/8340 [1:28:45<38:54,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:16,454 >> Initializing global attention on CLS token...\n",
            " 67% 5629/8340 [1:28:46<38:48,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:17,314 >> Initializing global attention on CLS token...\n",
            " 68% 5630/8340 [1:28:46<38:46,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:18,165 >> Initializing global attention on CLS token...\n",
            " 68% 5631/8340 [1:28:47<38:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:19,011 >> Initializing global attention on CLS token...\n",
            " 68% 5632/8340 [1:28:48<38:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:19,868 >> Initializing global attention on CLS token...\n",
            " 68% 5633/8340 [1:28:49<38:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:20,723 >> Initializing global attention on CLS token...\n",
            " 68% 5634/8340 [1:28:50<38:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:21,596 >> Initializing global attention on CLS token...\n",
            " 68% 5635/8340 [1:28:51<38:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:22,468 >> Initializing global attention on CLS token...\n",
            " 68% 5636/8340 [1:28:52<38:55,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:23,322 >> Initializing global attention on CLS token...\n",
            " 68% 5637/8340 [1:28:52<38:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:24,176 >> Initializing global attention on CLS token...\n",
            " 68% 5638/8340 [1:28:53<38:39,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:25,028 >> Initializing global attention on CLS token...\n",
            " 68% 5639/8340 [1:28:54<38:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:25,883 >> Initializing global attention on CLS token...\n",
            " 68% 5640/8340 [1:28:55<38:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:26,739 >> Initializing global attention on CLS token...\n",
            " 68% 5641/8340 [1:28:56<38:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:27,594 >> Initializing global attention on CLS token...\n",
            " 68% 5642/8340 [1:28:57<38:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:28,471 >> Initializing global attention on CLS token...\n",
            " 68% 5643/8340 [1:28:58<38:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:29,349 >> Initializing global attention on CLS token...\n",
            " 68% 5644/8340 [1:28:58<38:55,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:30,217 >> Initializing global attention on CLS token...\n",
            " 68% 5645/8340 [1:28:59<39:05,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:31,102 >> Initializing global attention on CLS token...\n",
            " 68% 5646/8340 [1:29:00<39:11,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:31,962 >> Initializing global attention on CLS token...\n",
            " 68% 5647/8340 [1:29:01<38:59,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:32,822 >> Initializing global attention on CLS token...\n",
            " 68% 5648/8340 [1:29:02<38:49,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:33,677 >> Initializing global attention on CLS token...\n",
            " 68% 5649/8340 [1:29:03<38:44,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:34,537 >> Initializing global attention on CLS token...\n",
            " 68% 5650/8340 [1:29:04<38:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:35,388 >> Initializing global attention on CLS token...\n",
            " 68% 5651/8340 [1:29:04<38:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:36,238 >> Initializing global attention on CLS token...\n",
            " 68% 5652/8340 [1:29:05<38:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:37,089 >> Initializing global attention on CLS token...\n",
            " 68% 5653/8340 [1:29:06<38:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:37,947 >> Initializing global attention on CLS token...\n",
            " 68% 5654/8340 [1:29:07<38:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:38,794 >> Initializing global attention on CLS token...\n",
            " 68% 5655/8340 [1:29:08<38:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:39,647 >> Initializing global attention on CLS token...\n",
            " 68% 5656/8340 [1:29:09<38:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:40,501 >> Initializing global attention on CLS token...\n",
            " 68% 5657/8340 [1:29:10<38:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:41,357 >> Initializing global attention on CLS token...\n",
            " 68% 5658/8340 [1:29:10<38:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:42,212 >> Initializing global attention on CLS token...\n",
            " 68% 5659/8340 [1:29:11<38:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:43,077 >> Initializing global attention on CLS token...\n",
            " 68% 5660/8340 [1:29:12<38:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:43,932 >> Initializing global attention on CLS token...\n",
            " 68% 5661/8340 [1:29:13<38:23,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:44,826 >> Initializing global attention on CLS token...\n",
            " 68% 5662/8340 [1:29:14<39:12,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:45,718 >> Initializing global attention on CLS token...\n",
            " 68% 5663/8340 [1:29:15<38:43,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:46,560 >> Initializing global attention on CLS token...\n",
            " 68% 5664/8340 [1:29:16<38:29,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:47,413 >> Initializing global attention on CLS token...\n",
            " 68% 5665/8340 [1:29:17<38:20,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:48,265 >> Initializing global attention on CLS token...\n",
            " 68% 5666/8340 [1:29:17<38:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:49,138 >> Initializing global attention on CLS token...\n",
            " 68% 5667/8340 [1:29:18<38:25,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:49,993 >> Initializing global attention on CLS token...\n",
            " 68% 5668/8340 [1:29:19<38:17,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:50,846 >> Initializing global attention on CLS token...\n",
            " 68% 5669/8340 [1:29:20<38:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:51,698 >> Initializing global attention on CLS token...\n",
            " 68% 5670/8340 [1:29:21<38:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:52,556 >> Initializing global attention on CLS token...\n",
            " 68% 5671/8340 [1:29:22<38:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:53,418 >> Initializing global attention on CLS token...\n",
            " 68% 5672/8340 [1:29:23<38:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:54,266 >> Initializing global attention on CLS token...\n",
            " 68% 5673/8340 [1:29:23<38:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:55,119 >> Initializing global attention on CLS token...\n",
            " 68% 5674/8340 [1:29:24<38:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:55,977 >> Initializing global attention on CLS token...\n",
            " 68% 5675/8340 [1:29:25<37:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:56,831 >> Initializing global attention on CLS token...\n",
            " 68% 5676/8340 [1:29:26<38:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:57,690 >> Initializing global attention on CLS token...\n",
            " 68% 5677/8340 [1:29:27<37:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:58,537 >> Initializing global attention on CLS token...\n",
            " 68% 5678/8340 [1:29:28<37:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:27:59,400 >> Initializing global attention on CLS token...\n",
            " 68% 5679/8340 [1:29:28<37:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:00,260 >> Initializing global attention on CLS token...\n",
            " 68% 5680/8340 [1:29:29<37:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:01,111 >> Initializing global attention on CLS token...\n",
            " 68% 5681/8340 [1:29:30<37:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:01,962 >> Initializing global attention on CLS token...\n",
            " 68% 5682/8340 [1:29:31<37:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:02,827 >> Initializing global attention on CLS token...\n",
            " 68% 5683/8340 [1:29:32<37:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:03,683 >> Initializing global attention on CLS token...\n",
            " 68% 5684/8340 [1:29:33<37:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:04,537 >> Initializing global attention on CLS token...\n",
            " 68% 5685/8340 [1:29:34<37:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:05,400 >> Initializing global attention on CLS token...\n",
            " 68% 5686/8340 [1:29:34<37:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:06,254 >> Initializing global attention on CLS token...\n",
            " 68% 5687/8340 [1:29:35<37:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:07,110 >> Initializing global attention on CLS token...\n",
            " 68% 5688/8340 [1:29:36<37:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:07,963 >> Initializing global attention on CLS token...\n",
            " 68% 5689/8340 [1:29:37<37:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:08,818 >> Initializing global attention on CLS token...\n",
            " 68% 5690/8340 [1:29:38<37:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:09,674 >> Initializing global attention on CLS token...\n",
            " 68% 5691/8340 [1:29:39<37:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:10,524 >> Initializing global attention on CLS token...\n",
            " 68% 5692/8340 [1:29:40<37:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:11,377 >> Initializing global attention on CLS token...\n",
            " 68% 5693/8340 [1:29:40<37:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:12,230 >> Initializing global attention on CLS token...\n",
            " 68% 5694/8340 [1:29:41<37:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:13,082 >> Initializing global attention on CLS token...\n",
            " 68% 5695/8340 [1:29:42<37:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:13,938 >> Initializing global attention on CLS token...\n",
            " 68% 5696/8340 [1:29:43<37:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:14,792 >> Initializing global attention on CLS token...\n",
            " 68% 5697/8340 [1:29:44<37:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:15,644 >> Initializing global attention on CLS token...\n",
            " 68% 5698/8340 [1:29:45<37:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:16,500 >> Initializing global attention on CLS token...\n",
            " 68% 5699/8340 [1:29:46<37:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:17,355 >> Initializing global attention on CLS token...\n",
            " 68% 5700/8340 [1:29:46<37:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:18,211 >> Initializing global attention on CLS token...\n",
            " 68% 5701/8340 [1:29:47<37:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:19,068 >> Initializing global attention on CLS token...\n",
            " 68% 5702/8340 [1:29:48<37:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:19,917 >> Initializing global attention on CLS token...\n",
            " 68% 5703/8340 [1:29:49<37:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:20,775 >> Initializing global attention on CLS token...\n",
            " 68% 5704/8340 [1:29:50<37:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:21,624 >> Initializing global attention on CLS token...\n",
            " 68% 5705/8340 [1:29:51<37:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:22,476 >> Initializing global attention on CLS token...\n",
            " 68% 5706/8340 [1:29:52<37:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:23,328 >> Initializing global attention on CLS token...\n",
            " 68% 5707/8340 [1:29:52<37:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:24,185 >> Initializing global attention on CLS token...\n",
            " 68% 5708/8340 [1:29:53<37:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:25,037 >> Initializing global attention on CLS token...\n",
            " 68% 5709/8340 [1:29:54<37:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:25,888 >> Initializing global attention on CLS token...\n",
            " 68% 5710/8340 [1:29:55<37:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:26,745 >> Initializing global attention on CLS token...\n",
            " 68% 5711/8340 [1:29:56<37:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:27,599 >> Initializing global attention on CLS token...\n",
            " 68% 5712/8340 [1:29:57<37:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:28,456 >> Initializing global attention on CLS token...\n",
            " 69% 5713/8340 [1:29:58<37:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:29,304 >> Initializing global attention on CLS token...\n",
            " 69% 5714/8340 [1:29:58<37:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:30,159 >> Initializing global attention on CLS token...\n",
            " 69% 5715/8340 [1:29:59<37:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:31,012 >> Initializing global attention on CLS token...\n",
            " 69% 5716/8340 [1:30:00<37:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:31,867 >> Initializing global attention on CLS token...\n",
            " 69% 5717/8340 [1:30:01<37:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:32,720 >> Initializing global attention on CLS token...\n",
            " 69% 5718/8340 [1:30:02<37:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:33,571 >> Initializing global attention on CLS token...\n",
            " 69% 5719/8340 [1:30:03<37:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:34,428 >> Initializing global attention on CLS token...\n",
            " 69% 5720/8340 [1:30:04<37:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:35,284 >> Initializing global attention on CLS token...\n",
            " 69% 5721/8340 [1:30:04<37:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:36,133 >> Initializing global attention on CLS token...\n",
            " 69% 5722/8340 [1:30:05<37:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:36,989 >> Initializing global attention on CLS token...\n",
            " 69% 5723/8340 [1:30:06<37:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:37,846 >> Initializing global attention on CLS token...\n",
            " 69% 5724/8340 [1:30:07<37:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:38,702 >> Initializing global attention on CLS token...\n",
            " 69% 5725/8340 [1:30:08<37:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:39,555 >> Initializing global attention on CLS token...\n",
            " 69% 5726/8340 [1:30:09<37:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:40,408 >> Initializing global attention on CLS token...\n",
            " 69% 5727/8340 [1:30:10<37:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:41,261 >> Initializing global attention on CLS token...\n",
            " 69% 5728/8340 [1:30:10<37:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:42,114 >> Initializing global attention on CLS token...\n",
            " 69% 5729/8340 [1:30:11<37:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:42,968 >> Initializing global attention on CLS token...\n",
            " 69% 5730/8340 [1:30:12<37:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:43,818 >> Initializing global attention on CLS token...\n",
            " 69% 5731/8340 [1:30:13<37:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:44,673 >> Initializing global attention on CLS token...\n",
            " 69% 5732/8340 [1:30:14<37:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:45,531 >> Initializing global attention on CLS token...\n",
            " 69% 5733/8340 [1:30:15<37:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:46,384 >> Initializing global attention on CLS token...\n",
            " 69% 5734/8340 [1:30:15<37:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:47,239 >> Initializing global attention on CLS token...\n",
            " 69% 5735/8340 [1:30:16<37:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:48,094 >> Initializing global attention on CLS token...\n",
            " 69% 5736/8340 [1:30:17<37:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:48,949 >> Initializing global attention on CLS token...\n",
            " 69% 5737/8340 [1:30:18<37:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:49,800 >> Initializing global attention on CLS token...\n",
            " 69% 5738/8340 [1:30:19<37:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:50,653 >> Initializing global attention on CLS token...\n",
            " 69% 5739/8340 [1:30:20<37:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:51,509 >> Initializing global attention on CLS token...\n",
            " 69% 5740/8340 [1:30:21<36:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:52,363 >> Initializing global attention on CLS token...\n",
            " 69% 5741/8340 [1:30:21<36:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:53,216 >> Initializing global attention on CLS token...\n",
            " 69% 5742/8340 [1:30:22<36:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:54,068 >> Initializing global attention on CLS token...\n",
            " 69% 5743/8340 [1:30:23<36:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:54,913 >> Initializing global attention on CLS token...\n",
            " 69% 5744/8340 [1:30:24<36:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:55,767 >> Initializing global attention on CLS token...\n",
            " 69% 5745/8340 [1:30:25<36:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:56,620 >> Initializing global attention on CLS token...\n",
            " 69% 5746/8340 [1:30:26<36:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:57,473 >> Initializing global attention on CLS token...\n",
            " 69% 5747/8340 [1:30:27<36:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:58,330 >> Initializing global attention on CLS token...\n",
            " 69% 5748/8340 [1:30:27<36:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:28:59,188 >> Initializing global attention on CLS token...\n",
            " 69% 5749/8340 [1:30:28<36:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:00,042 >> Initializing global attention on CLS token...\n",
            " 69% 5750/8340 [1:30:29<36:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:00,898 >> Initializing global attention on CLS token...\n",
            " 69% 5751/8340 [1:30:30<36:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:01,752 >> Initializing global attention on CLS token...\n",
            " 69% 5752/8340 [1:30:31<36:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:02,609 >> Initializing global attention on CLS token...\n",
            " 69% 5753/8340 [1:30:32<36:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:03,465 >> Initializing global attention on CLS token...\n",
            " 69% 5754/8340 [1:30:33<36:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:04,318 >> Initializing global attention on CLS token...\n",
            " 69% 5755/8340 [1:30:33<36:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:05,173 >> Initializing global attention on CLS token...\n",
            " 69% 5756/8340 [1:30:34<36:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:06,026 >> Initializing global attention on CLS token...\n",
            " 69% 5757/8340 [1:30:35<36:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:06,886 >> Initializing global attention on CLS token...\n",
            " 69% 5758/8340 [1:30:36<36:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:07,737 >> Initializing global attention on CLS token...\n",
            " 69% 5759/8340 [1:30:37<36:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:08,594 >> Initializing global attention on CLS token...\n",
            " 69% 5760/8340 [1:30:38<36:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:09,449 >> Initializing global attention on CLS token...\n",
            " 69% 5761/8340 [1:30:39<36:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:10,303 >> Initializing global attention on CLS token...\n",
            " 69% 5762/8340 [1:30:39<36:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:11,157 >> Initializing global attention on CLS token...\n",
            " 69% 5763/8340 [1:30:40<36:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:12,013 >> Initializing global attention on CLS token...\n",
            " 69% 5764/8340 [1:30:41<36:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:12,865 >> Initializing global attention on CLS token...\n",
            " 69% 5765/8340 [1:30:42<36:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:13,717 >> Initializing global attention on CLS token...\n",
            " 69% 5766/8340 [1:30:43<36:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:14,570 >> Initializing global attention on CLS token...\n",
            " 69% 5767/8340 [1:30:44<36:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:15,427 >> Initializing global attention on CLS token...\n",
            " 69% 5768/8340 [1:30:45<36:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:16,278 >> Initializing global attention on CLS token...\n",
            " 69% 5769/8340 [1:30:45<36:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:17,131 >> Initializing global attention on CLS token...\n",
            " 69% 5770/8340 [1:30:46<36:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:17,984 >> Initializing global attention on CLS token...\n",
            " 69% 5771/8340 [1:30:47<36:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:18,837 >> Initializing global attention on CLS token...\n",
            " 69% 5772/8340 [1:30:48<36:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:19,692 >> Initializing global attention on CLS token...\n",
            " 69% 5773/8340 [1:30:49<36:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:20,547 >> Initializing global attention on CLS token...\n",
            " 69% 5774/8340 [1:30:50<36:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:21,401 >> Initializing global attention on CLS token...\n",
            " 69% 5775/8340 [1:30:50<36:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:22,255 >> Initializing global attention on CLS token...\n",
            " 69% 5776/8340 [1:30:51<36:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:23,111 >> Initializing global attention on CLS token...\n",
            " 69% 5777/8340 [1:30:52<36:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:23,964 >> Initializing global attention on CLS token...\n",
            " 69% 5778/8340 [1:30:53<36:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:24,817 >> Initializing global attention on CLS token...\n",
            " 69% 5779/8340 [1:30:54<36:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:25,670 >> Initializing global attention on CLS token...\n",
            " 69% 5780/8340 [1:30:55<36:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:26,525 >> Initializing global attention on CLS token...\n",
            " 69% 5781/8340 [1:30:56<36:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:27,380 >> Initializing global attention on CLS token...\n",
            " 69% 5782/8340 [1:30:56<36:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:28,235 >> Initializing global attention on CLS token...\n",
            " 69% 5783/8340 [1:30:57<36:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:29,092 >> Initializing global attention on CLS token...\n",
            " 69% 5784/8340 [1:30:58<36:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:29,957 >> Initializing global attention on CLS token...\n",
            " 69% 5785/8340 [1:30:59<36:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:30,816 >> Initializing global attention on CLS token...\n",
            " 69% 5786/8340 [1:31:00<36:34,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:31,681 >> Initializing global attention on CLS token...\n",
            " 69% 5787/8340 [1:31:01<36:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:32,528 >> Initializing global attention on CLS token...\n",
            " 69% 5788/8340 [1:31:02<36:33,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:33,410 >> Initializing global attention on CLS token...\n",
            " 69% 5789/8340 [1:31:02<36:36,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:34,255 >> Initializing global attention on CLS token...\n",
            " 69% 5790/8340 [1:31:03<36:30,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:35,111 >> Initializing global attention on CLS token...\n",
            " 69% 5791/8340 [1:31:04<36:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:35,967 >> Initializing global attention on CLS token...\n",
            " 69% 5792/8340 [1:31:05<36:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:36,818 >> Initializing global attention on CLS token...\n",
            " 69% 5793/8340 [1:31:06<36:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:37,670 >> Initializing global attention on CLS token...\n",
            " 69% 5794/8340 [1:31:07<36:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:38,523 >> Initializing global attention on CLS token...\n",
            " 69% 5795/8340 [1:31:08<36:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:39,376 >> Initializing global attention on CLS token...\n",
            " 69% 5796/8340 [1:31:08<36:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:40,232 >> Initializing global attention on CLS token...\n",
            " 70% 5797/8340 [1:31:09<36:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:41,088 >> Initializing global attention on CLS token...\n",
            " 70% 5798/8340 [1:31:10<36:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:41,937 >> Initializing global attention on CLS token...\n",
            " 70% 5799/8340 [1:31:11<36:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:42,793 >> Initializing global attention on CLS token...\n",
            " 70% 5800/8340 [1:31:12<36:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:43,651 >> Initializing global attention on CLS token...\n",
            " 70% 5801/8340 [1:31:13<36:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:44,497 >> Initializing global attention on CLS token...\n",
            " 70% 5802/8340 [1:31:14<36:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:45,352 >> Initializing global attention on CLS token...\n",
            " 70% 5803/8340 [1:31:14<36:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:46,206 >> Initializing global attention on CLS token...\n",
            " 70% 5804/8340 [1:31:15<36:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:47,059 >> Initializing global attention on CLS token...\n",
            " 70% 5805/8340 [1:31:16<36:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:47,914 >> Initializing global attention on CLS token...\n",
            " 70% 5806/8340 [1:31:17<36:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:48,769 >> Initializing global attention on CLS token...\n",
            " 70% 5807/8340 [1:31:18<36:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:49,622 >> Initializing global attention on CLS token...\n",
            " 70% 5808/8340 [1:31:19<36:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:50,477 >> Initializing global attention on CLS token...\n",
            " 70% 5809/8340 [1:31:20<36:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:51,332 >> Initializing global attention on CLS token...\n",
            " 70% 5810/8340 [1:31:20<35:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:52,183 >> Initializing global attention on CLS token...\n",
            " 70% 5811/8340 [1:31:21<35:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:53,038 >> Initializing global attention on CLS token...\n",
            " 70% 5812/8340 [1:31:22<35:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:53,891 >> Initializing global attention on CLS token...\n",
            " 70% 5813/8340 [1:31:23<35:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:54,749 >> Initializing global attention on CLS token...\n",
            " 70% 5814/8340 [1:31:24<35:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:55,599 >> Initializing global attention on CLS token...\n",
            " 70% 5815/8340 [1:31:25<35:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:56,454 >> Initializing global attention on CLS token...\n",
            " 70% 5816/8340 [1:31:26<35:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:57,308 >> Initializing global attention on CLS token...\n",
            " 70% 5817/8340 [1:31:26<35:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:58,164 >> Initializing global attention on CLS token...\n",
            " 70% 5818/8340 [1:31:27<35:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:59,018 >> Initializing global attention on CLS token...\n",
            " 70% 5819/8340 [1:31:28<35:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:29:59,873 >> Initializing global attention on CLS token...\n",
            " 70% 5820/8340 [1:31:29<35:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:00,727 >> Initializing global attention on CLS token...\n",
            " 70% 5821/8340 [1:31:30<35:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:01,581 >> Initializing global attention on CLS token...\n",
            " 70% 5822/8340 [1:31:31<35:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:02,436 >> Initializing global attention on CLS token...\n",
            " 70% 5823/8340 [1:31:32<35:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:03,293 >> Initializing global attention on CLS token...\n",
            " 70% 5824/8340 [1:31:32<35:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:04,149 >> Initializing global attention on CLS token...\n",
            " 70% 5825/8340 [1:31:33<35:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:05,004 >> Initializing global attention on CLS token...\n",
            " 70% 5826/8340 [1:31:34<35:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:05,859 >> Initializing global attention on CLS token...\n",
            " 70% 5827/8340 [1:31:35<35:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:06,715 >> Initializing global attention on CLS token...\n",
            " 70% 5828/8340 [1:31:36<35:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:07,570 >> Initializing global attention on CLS token...\n",
            " 70% 5829/8340 [1:31:37<35:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:08,422 >> Initializing global attention on CLS token...\n",
            " 70% 5830/8340 [1:31:38<35:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:09,272 >> Initializing global attention on CLS token...\n",
            " 70% 5831/8340 [1:31:38<35:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:10,125 >> Initializing global attention on CLS token...\n",
            " 70% 5832/8340 [1:31:39<35:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:10,974 >> Initializing global attention on CLS token...\n",
            " 70% 5833/8340 [1:31:40<35:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:11,829 >> Initializing global attention on CLS token...\n",
            " 70% 5834/8340 [1:31:41<35:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:12,684 >> Initializing global attention on CLS token...\n",
            " 70% 5835/8340 [1:31:42<35:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:13,541 >> Initializing global attention on CLS token...\n",
            " 70% 5836/8340 [1:31:43<35:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:14,395 >> Initializing global attention on CLS token...\n",
            " 70% 5837/8340 [1:31:43<35:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:30:15,233 >> Initializing global attention on CLS token...\n",
            " 70% 5838/8340 [1:31:44<28:54,  1.44it/s][INFO|trainer.py:726] 2022-11-21 17:30:15,539 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2022-11-21 17:30:15,541 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-11-21 17:30:15,542 >>   Num examples = 1400\n",
            "[INFO|trainer.py:2912] 2022-11-21 17:30:15,542 >>   Batch size = 6\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:15,570 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 0/234 [00:00<?, ?it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:15,834 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 2/234 [00:00<00:30,  7.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:16,109 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 3/234 [00:00<00:44,  5.17it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:16,377 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 4/234 [00:00<00:50,  4.53it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:16,640 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 5/234 [00:01<00:54,  4.23it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:16,915 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 6/234 [00:01<00:56,  4.04it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:17,177 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 7/234 [00:01<00:57,  3.97it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:17,438 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 8/234 [00:01<00:57,  3.91it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:17,702 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 9/234 [00:02<00:58,  3.87it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:17,964 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 10/234 [00:02<00:58,  3.84it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:18,238 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 11/234 [00:02<00:58,  3.81it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:18,501 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 12/234 [00:02<00:58,  3.81it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:18,762 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 13/234 [00:03<00:58,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:19,032 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 14/234 [00:03<00:58,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:19,299 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 15/234 [00:03<00:58,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:19,566 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 16/234 [00:03<00:57,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:19,827 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 17/234 [00:04<00:57,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:20,089 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 18/234 [00:04<00:56,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:20,364 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 19/234 [00:04<00:57,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:20,628 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 20/234 [00:05<00:57,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:20,891 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 21/234 [00:05<00:56,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:21,159 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 22/234 [00:05<00:56,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:21,424 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 23/234 [00:05<00:55,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:21,686 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 24/234 [00:06<00:55,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:21,947 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 25/234 [00:06<00:55,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:22,213 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 26/234 [00:06<00:55,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:22,481 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 27/234 [00:06<00:54,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:22,742 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 28/234 [00:07<00:54,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:23,005 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 29/234 [00:07<00:54,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:23,278 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 30/234 [00:07<00:54,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:23,540 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 31/234 [00:07<00:53,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:23,809 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 32/234 [00:08<00:53,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:24,079 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 33/234 [00:08<00:53,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:24,346 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 34/234 [00:08<00:53,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:24,611 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 35/234 [00:09<00:52,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:24,874 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 36/234 [00:09<00:52,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:25,135 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 37/234 [00:09<00:52,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:25,405 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 38/234 [00:09<00:52,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:25,677 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 39/234 [00:10<00:52,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:25,937 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 40/234 [00:10<00:51,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:26,197 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 41/234 [00:10<00:50,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:26,459 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 42/234 [00:10<00:50,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:26,726 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 43/234 [00:11<00:50,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:27,000 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 44/234 [00:11<00:50,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:27,263 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 45/234 [00:11<00:50,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:27,549 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 46/234 [00:11<00:51,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:27,812 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 47/234 [00:12<00:50,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:28,073 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 48/234 [00:12<00:49,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:28,337 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 49/234 [00:12<00:49,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:28,604 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 50/234 [00:13<00:49,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:28,870 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 51/234 [00:13<00:48,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:29,135 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 52/234 [00:13<00:48,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:29,402 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 53/234 [00:13<00:48,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:29,671 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 54/234 [00:14<00:48,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:29,934 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 55/234 [00:14<00:47,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:30,199 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 56/234 [00:14<00:47,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:30,463 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 57/234 [00:14<00:47,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:30,741 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 58/234 [00:15<00:47,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:31,007 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 59/234 [00:15<00:47,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:31,277 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 60/234 [00:15<00:46,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:31,547 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 61/234 [00:15<00:46,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:31,808 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 62/234 [00:16<00:45,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:32,067 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 63/234 [00:16<00:45,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:32,332 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 64/234 [00:16<00:45,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:32,597 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 65/234 [00:17<00:44,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:32,867 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 66/234 [00:17<00:44,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:33,137 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 67/234 [00:17<00:44,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:33,402 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 68/234 [00:17<00:44,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:33,672 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 69/234 [00:18<00:44,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:33,942 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 70/234 [00:18<00:43,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:34,203 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 71/234 [00:18<00:43,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:34,465 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 72/234 [00:18<00:43,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:34,733 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 73/234 [00:19<00:42,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:34,997 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 74/234 [00:19<00:42,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:35,273 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 75/234 [00:19<00:42,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:35,538 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 76/234 [00:19<00:42,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:35,816 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 77/234 [00:20<00:42,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:36,079 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 78/234 [00:20<00:41,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:36,338 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 79/234 [00:20<00:40,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:36,599 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 80/234 [00:21<00:40,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:36,867 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 81/234 [00:21<00:40,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:37,130 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 82/234 [00:21<00:40,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:37,392 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 83/234 [00:21<00:39,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:37,652 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 84/234 [00:22<00:39,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:37,919 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 85/234 [00:22<00:39,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:38,183 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 86/234 [00:22<00:39,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:38,445 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 87/234 [00:22<00:38,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:38,708 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 88/234 [00:23<00:38,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:38,971 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 89/234 [00:23<00:38,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:39,237 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 90/234 [00:23<00:37,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:39,508 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 91/234 [00:23<00:37,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:39,768 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 92/234 [00:24<00:37,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:40,027 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 93/234 [00:24<00:37,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:40,294 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 94/234 [00:24<00:36,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:40,555 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 95/234 [00:24<00:36,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:40,826 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 96/234 [00:25<00:36,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:41,087 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 97/234 [00:25<00:36,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:41,349 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 98/234 [00:25<00:35,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:41,620 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 99/234 [00:26<00:36,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:41,884 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 100/234 [00:26<00:35,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:42,145 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 101/234 [00:26<00:35,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:42,405 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 102/234 [00:26<00:34,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:42,675 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 103/234 [00:27<00:34,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:42,943 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 104/234 [00:27<00:34,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:43,209 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 105/234 [00:27<00:34,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:43,482 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 106/234 [00:27<00:34,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:43,758 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 107/234 [00:28<00:34,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:44,022 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 108/234 [00:28<00:33,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:44,288 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 109/234 [00:28<00:33,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:44,556 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 110/234 [00:28<00:33,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:44,823 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 111/234 [00:29<00:32,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:45,092 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 112/234 [00:29<00:32,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:45,356 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 113/234 [00:29<00:32,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:45,619 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 114/234 [00:30<00:31,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:45,883 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 115/234 [00:30<00:31,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:46,145 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 116/234 [00:30<00:31,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:46,413 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 117/234 [00:30<00:30,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:46,681 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 118/234 [00:31<00:30,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:46,945 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 119/234 [00:31<00:30,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:47,206 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 120/234 [00:31<00:30,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:47,467 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 121/234 [00:31<00:29,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:47,733 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 122/234 [00:32<00:29,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:47,993 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 123/234 [00:32<00:29,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:48,257 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 124/234 [00:32<00:28,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:48,522 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 125/234 [00:32<00:28,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:48,796 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 126/234 [00:33<00:28,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:49,062 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 127/234 [00:33<00:28,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:49,330 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 128/234 [00:33<00:28,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:49,600 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 129/234 [00:34<00:28,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:49,862 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 130/234 [00:34<00:27,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:50,124 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 131/234 [00:34<00:27,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:50,393 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 132/234 [00:34<00:27,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:50,655 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 133/234 [00:35<00:26,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:50,921 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 134/234 [00:35<00:26,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:51,182 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 135/234 [00:35<00:26,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:51,447 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 136/234 [00:35<00:25,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:51,712 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 137/234 [00:36<00:25,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:51,975 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 138/234 [00:36<00:25,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:52,243 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 139/234 [00:36<00:25,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:52,503 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 140/234 [00:36<00:24,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:52,770 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 141/234 [00:37<00:24,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:53,048 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 142/234 [00:37<00:24,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:53,314 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 143/234 [00:37<00:24,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:53,582 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 144/234 [00:38<00:24,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:53,849 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 145/234 [00:38<00:23,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:54,111 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 146/234 [00:38<00:23,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:54,373 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 147/234 [00:38<00:23,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:54,637 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 148/234 [00:39<00:22,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:54,904 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 149/234 [00:39<00:22,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:55,165 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 150/234 [00:39<00:22,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:55,426 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 151/234 [00:39<00:21,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:55,692 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 152/234 [00:40<00:21,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:55,967 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 153/234 [00:40<00:21,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:56,229 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 154/234 [00:40<00:21,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:56,486 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 155/234 [00:40<00:20,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:56,755 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 156/234 [00:41<00:20,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:57,017 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 157/234 [00:41<00:20,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:57,287 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 158/234 [00:41<00:20,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:57,549 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 159/234 [00:41<00:19,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:57,813 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 160/234 [00:42<00:19,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:58,076 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 161/234 [00:42<00:19,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:58,334 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 162/234 [00:42<00:18,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:58,598 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 163/234 [00:43<00:18,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:58,863 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 164/234 [00:43<00:18,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:59,135 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 165/234 [00:43<00:18,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:59,394 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 166/234 [00:43<00:17,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:59,654 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 167/234 [00:44<00:17,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:30:59,924 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 168/234 [00:44<00:17,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:00,192 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 169/234 [00:44<00:17,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:00,453 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 170/234 [00:44<00:16,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:00,713 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 171/234 [00:45<00:16,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:00,976 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 172/234 [00:45<00:16,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:01,247 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 173/234 [00:45<00:16,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:01,514 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 174/234 [00:45<00:15,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:01,793 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 175/234 [00:46<00:15,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:02,071 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 176/234 [00:46<00:15,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:02,353 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 177/234 [00:46<00:15,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:02,635 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 178/234 [00:47<00:15,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:02,905 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 179/234 [00:47<00:15,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:03,188 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 180/234 [00:47<00:14,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:03,472 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 181/234 [00:47<00:14,  3.58it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:03,756 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 182/234 [00:48<00:14,  3.56it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:04,039 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 183/234 [00:48<00:14,  3.55it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:04,308 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 184/234 [00:48<00:13,  3.60it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:04,583 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 185/234 [00:49<00:13,  3.62it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:04,848 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 186/234 [00:49<00:13,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:05,112 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 187/234 [00:49<00:12,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:05,382 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 188/234 [00:49<00:12,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:05,652 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 189/234 [00:50<00:12,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:05,916 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 190/234 [00:50<00:11,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:06,179 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 191/234 [00:50<00:11,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:06,446 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 192/234 [00:50<00:11,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:06,708 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 193/234 [00:51<00:10,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:06,991 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 194/234 [00:51<00:10,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:07,256 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 195/234 [00:51<00:10,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:07,520 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 196/234 [00:51<00:10,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:07,795 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 197/234 [00:52<00:09,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:08,052 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 198/234 [00:52<00:09,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:08,317 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 199/234 [00:52<00:09,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:08,581 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 200/234 [00:53<00:09,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:08,850 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 201/234 [00:53<00:08,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:09,122 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 202/234 [00:53<00:08,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:09,389 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 203/234 [00:53<00:08,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:09,654 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 204/234 [00:54<00:08,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:09,921 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 205/234 [00:54<00:07,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:10,194 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 206/234 [00:54<00:07,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:10,455 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 207/234 [00:54<00:07,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:10,718 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 208/234 [00:55<00:06,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:10,979 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 209/234 [00:55<00:06,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:11,248 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 210/234 [00:55<00:06,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:11,512 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 211/234 [00:55<00:06,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:11,775 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 212/234 [00:56<00:05,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:12,044 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 213/234 [00:56<00:05,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:12,314 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 214/234 [00:56<00:05,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:12,576 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 215/234 [00:57<00:05,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:12,841 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 216/234 [00:57<00:04,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:13,104 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 217/234 [00:57<00:04,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:13,373 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 218/234 [00:57<00:04,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:13,642 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 219/234 [00:58<00:04,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:13,908 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 220/234 [00:58<00:03,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:14,183 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 221/234 [00:58<00:03,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:14,445 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 222/234 [00:58<00:03,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:14,706 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 223/234 [00:59<00:02,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:14,975 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 224/234 [00:59<00:02,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:15,245 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 225/234 [00:59<00:02,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:15,506 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 226/234 [00:59<00:02,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:15,764 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 227/234 [01:00<00:01,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:16,025 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 228/234 [01:00<00:01,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:16,299 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 229/234 [01:00<00:01,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:16,560 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 230/234 [01:00<00:01,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:16,819 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 231/234 [01:01<00:00,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:17,079 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 232/234 [01:01<00:00,  3.81it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:17,340 >> Initializing global attention on CLS token...\n",
            "\n",
            "100% 233/234 [01:01<00:00,  3.83it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:17,579 >> Initializing global attention on CLS token...\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 2.1312224864959717, 'eval_f1-micro': 0.7207142857142858, 'eval_f1-macro': 0.5874284585882461, 'eval_accuracy': 0.7207142857142858, 'eval_runtime': 64.2196, 'eval_samples_per_second': 21.8, 'eval_steps_per_second': 3.644, 'epoch': 7.0}\n",
            " 70% 5838/8340 [1:32:48<28:54,  1.44it/s]\n",
            "100% 234/234 [01:03<00:00,  3.83it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-11-21 17:31:19,764 >> Saving model checkpoint to logs/output_1/checkpoint-5838\n",
            "[INFO|configuration_utils.py:447] 2022-11-21 17:31:19,765 >> Configuration saved in logs/output_1/checkpoint-5838/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-21 17:31:20,091 >> Model weights saved in logs/output_1/checkpoint-5838/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-21 17:31:20,091 >> tokenizer config file saved in logs/output_1/checkpoint-5838/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-21 17:31:20,092 >> Special tokens file saved in logs/output_1/checkpoint-5838/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-21 17:31:23,099 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-21 17:31:23,100 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-21 17:31:39,774 >> Initializing global attention on CLS token...\n",
            " 70% 5839/8340 [1:33:09<18:04:34, 26.02s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:40,680 >> Initializing global attention on CLS token...\n",
            " 70% 5840/8340 [1:33:10<12:49:30, 18.47s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:41,529 >> Initializing global attention on CLS token...\n",
            " 70% 5841/8340 [1:33:11<9:08:56, 13.18s/it] [INFO|modeling_longformer.py:1932] 2022-11-21 17:31:42,372 >> Initializing global attention on CLS token...\n",
            " 70% 5842/8340 [1:33:11<6:34:45,  9.48s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:43,225 >> Initializing global attention on CLS token...\n",
            " 70% 5843/8340 [1:33:12<4:46:54,  6.89s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:44,088 >> Initializing global attention on CLS token...\n",
            " 70% 5844/8340 [1:33:13<3:31:24,  5.08s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:44,933 >> Initializing global attention on CLS token...\n",
            " 70% 5845/8340 [1:33:14<2:38:33,  3.81s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:45,787 >> Initializing global attention on CLS token...\n",
            " 70% 5846/8340 [1:33:15<2:01:37,  2.93s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:46,643 >> Initializing global attention on CLS token...\n",
            " 70% 5847/8340 [1:33:16<1:35:47,  2.31s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:47,506 >> Initializing global attention on CLS token...\n",
            " 70% 5848/8340 [1:33:17<1:17:45,  1.87s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:48,368 >> Initializing global attention on CLS token...\n",
            " 70% 5849/8340 [1:33:17<1:05:11,  1.57s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:49,230 >> Initializing global attention on CLS token...\n",
            " 70% 5850/8340 [1:33:18<56:14,  1.36s/it]  [INFO|modeling_longformer.py:1932] 2022-11-21 17:31:50,085 >> Initializing global attention on CLS token...\n",
            " 70% 5851/8340 [1:33:19<49:58,  1.20s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:50,936 >> Initializing global attention on CLS token...\n",
            " 70% 5852/8340 [1:33:20<45:36,  1.10s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:51,793 >> Initializing global attention on CLS token...\n",
            " 70% 5853/8340 [1:33:21<42:34,  1.03s/it][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:52,648 >> Initializing global attention on CLS token...\n",
            " 70% 5854/8340 [1:33:22<40:22,  1.03it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:53,499 >> Initializing global attention on CLS token...\n",
            " 70% 5855/8340 [1:33:23<38:50,  1.07it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:54,351 >> Initializing global attention on CLS token...\n",
            " 70% 5856/8340 [1:33:23<37:50,  1.09it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:55,211 >> Initializing global attention on CLS token...\n",
            " 70% 5857/8340 [1:33:24<37:04,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:56,063 >> Initializing global attention on CLS token...\n",
            " 70% 5858/8340 [1:33:25<36:35,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:56,922 >> Initializing global attention on CLS token...\n",
            " 70% 5859/8340 [1:33:26<36:16,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:57,784 >> Initializing global attention on CLS token...\n",
            " 70% 5860/8340 [1:33:27<36:06,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:58,649 >> Initializing global attention on CLS token...\n",
            " 70% 5861/8340 [1:33:28<35:57,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:31:59,509 >> Initializing global attention on CLS token...\n",
            " 70% 5862/8340 [1:33:29<35:50,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:00,371 >> Initializing global attention on CLS token...\n",
            " 70% 5863/8340 [1:33:29<35:45,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:01,234 >> Initializing global attention on CLS token...\n",
            " 70% 5864/8340 [1:33:30<35:42,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:02,097 >> Initializing global attention on CLS token...\n",
            " 70% 5865/8340 [1:33:31<35:43,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:02,967 >> Initializing global attention on CLS token...\n",
            " 70% 5866/8340 [1:33:32<35:44,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:03,838 >> Initializing global attention on CLS token...\n",
            " 70% 5867/8340 [1:33:33<35:40,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:04,694 >> Initializing global attention on CLS token...\n",
            " 70% 5868/8340 [1:33:34<35:36,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:05,558 >> Initializing global attention on CLS token...\n",
            " 70% 5869/8340 [1:33:35<35:38,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:06,425 >> Initializing global attention on CLS token...\n",
            " 70% 5870/8340 [1:33:36<35:42,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:07,296 >> Initializing global attention on CLS token...\n",
            " 70% 5871/8340 [1:33:36<35:41,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:08,167 >> Initializing global attention on CLS token...\n",
            " 70% 5872/8340 [1:33:37<35:41,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:09,032 >> Initializing global attention on CLS token...\n",
            " 70% 5873/8340 [1:33:38<35:44,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:09,906 >> Initializing global attention on CLS token...\n",
            " 70% 5874/8340 [1:33:39<35:45,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:10,778 >> Initializing global attention on CLS token...\n",
            " 70% 5875/8340 [1:33:40<35:48,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:11,654 >> Initializing global attention on CLS token...\n",
            " 70% 5876/8340 [1:33:41<35:50,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:12,529 >> Initializing global attention on CLS token...\n",
            " 70% 5877/8340 [1:33:42<35:47,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:13,398 >> Initializing global attention on CLS token...\n",
            " 70% 5878/8340 [1:33:42<35:36,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:14,256 >> Initializing global attention on CLS token...\n",
            " 70% 5879/8340 [1:33:43<35:36,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:15,128 >> Initializing global attention on CLS token...\n",
            " 71% 5880/8340 [1:33:44<35:32,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:15,988 >> Initializing global attention on CLS token...\n",
            " 71% 5881/8340 [1:33:45<35:29,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:16,853 >> Initializing global attention on CLS token...\n",
            " 71% 5882/8340 [1:33:46<35:29,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:17,721 >> Initializing global attention on CLS token...\n",
            " 71% 5883/8340 [1:33:47<35:27,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:18,586 >> Initializing global attention on CLS token...\n",
            " 71% 5884/8340 [1:33:48<35:22,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:19,445 >> Initializing global attention on CLS token...\n",
            " 71% 5885/8340 [1:33:49<35:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:20,301 >> Initializing global attention on CLS token...\n",
            " 71% 5886/8340 [1:33:49<35:10,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:21,161 >> Initializing global attention on CLS token...\n",
            " 71% 5887/8340 [1:33:50<35:09,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:22,019 >> Initializing global attention on CLS token...\n",
            " 71% 5888/8340 [1:33:51<35:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:22,883 >> Initializing global attention on CLS token...\n",
            " 71% 5889/8340 [1:33:52<35:09,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:23,743 >> Initializing global attention on CLS token...\n",
            " 71% 5890/8340 [1:33:53<35:03,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:24,595 >> Initializing global attention on CLS token...\n",
            " 71% 5891/8340 [1:33:54<34:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:25,449 >> Initializing global attention on CLS token...\n",
            " 71% 5892/8340 [1:33:55<34:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:26,303 >> Initializing global attention on CLS token...\n",
            " 71% 5893/8340 [1:33:55<34:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:27,163 >> Initializing global attention on CLS token...\n",
            " 71% 5894/8340 [1:33:56<34:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:28,017 >> Initializing global attention on CLS token...\n",
            " 71% 5895/8340 [1:33:57<34:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:28,880 >> Initializing global attention on CLS token...\n",
            " 71% 5896/8340 [1:33:58<34:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:29,727 >> Initializing global attention on CLS token...\n",
            " 71% 5897/8340 [1:33:59<34:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:30,583 >> Initializing global attention on CLS token...\n",
            " 71% 5898/8340 [1:34:00<34:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:31,439 >> Initializing global attention on CLS token...\n",
            " 71% 5899/8340 [1:34:01<34:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:32,293 >> Initializing global attention on CLS token...\n",
            " 71% 5900/8340 [1:34:01<34:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:33,149 >> Initializing global attention on CLS token...\n",
            " 71% 5901/8340 [1:34:02<34:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:34,007 >> Initializing global attention on CLS token...\n",
            " 71% 5902/8340 [1:34:03<34:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:34,858 >> Initializing global attention on CLS token...\n",
            " 71% 5903/8340 [1:34:04<34:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:35,707 >> Initializing global attention on CLS token...\n",
            " 71% 5904/8340 [1:34:05<34:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:36,563 >> Initializing global attention on CLS token...\n",
            " 71% 5905/8340 [1:34:06<34:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:37,416 >> Initializing global attention on CLS token...\n",
            " 71% 5906/8340 [1:34:07<34:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:38,269 >> Initializing global attention on CLS token...\n",
            " 71% 5907/8340 [1:34:07<34:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:39,124 >> Initializing global attention on CLS token...\n",
            " 71% 5908/8340 [1:34:08<34:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:39,980 >> Initializing global attention on CLS token...\n",
            " 71% 5909/8340 [1:34:09<34:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:40,835 >> Initializing global attention on CLS token...\n",
            " 71% 5910/8340 [1:34:10<34:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:41,687 >> Initializing global attention on CLS token...\n",
            " 71% 5911/8340 [1:34:11<34:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:42,540 >> Initializing global attention on CLS token...\n",
            " 71% 5912/8340 [1:34:12<34:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:43,395 >> Initializing global attention on CLS token...\n",
            " 71% 5913/8340 [1:34:12<34:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:44,244 >> Initializing global attention on CLS token...\n",
            " 71% 5914/8340 [1:34:13<34:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:45,102 >> Initializing global attention on CLS token...\n",
            " 71% 5915/8340 [1:34:14<34:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:45,949 >> Initializing global attention on CLS token...\n",
            " 71% 5916/8340 [1:34:15<34:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:46,806 >> Initializing global attention on CLS token...\n",
            " 71% 5917/8340 [1:34:16<34:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:47,657 >> Initializing global attention on CLS token...\n",
            " 71% 5918/8340 [1:34:17<34:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:48,509 >> Initializing global attention on CLS token...\n",
            " 71% 5919/8340 [1:34:18<34:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:49,367 >> Initializing global attention on CLS token...\n",
            " 71% 5920/8340 [1:34:18<34:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:50,218 >> Initializing global attention on CLS token...\n",
            " 71% 5921/8340 [1:34:19<34:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:51,074 >> Initializing global attention on CLS token...\n",
            " 71% 5922/8340 [1:34:20<34:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:51,926 >> Initializing global attention on CLS token...\n",
            " 71% 5923/8340 [1:34:21<34:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:52,778 >> Initializing global attention on CLS token...\n",
            " 71% 5924/8340 [1:34:22<34:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:53,635 >> Initializing global attention on CLS token...\n",
            " 71% 5925/8340 [1:34:23<34:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:54,485 >> Initializing global attention on CLS token...\n",
            " 71% 5926/8340 [1:34:24<34:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:55,342 >> Initializing global attention on CLS token...\n",
            " 71% 5927/8340 [1:34:24<34:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:56,192 >> Initializing global attention on CLS token...\n",
            " 71% 5928/8340 [1:34:25<34:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:57,051 >> Initializing global attention on CLS token...\n",
            " 71% 5929/8340 [1:34:26<34:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:57,904 >> Initializing global attention on CLS token...\n",
            " 71% 5930/8340 [1:34:27<34:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:58,759 >> Initializing global attention on CLS token...\n",
            " 71% 5931/8340 [1:34:28<34:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:32:59,610 >> Initializing global attention on CLS token...\n",
            " 71% 5932/8340 [1:34:29<34:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:00,466 >> Initializing global attention on CLS token...\n",
            " 71% 5933/8340 [1:34:30<34:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:01,317 >> Initializing global attention on CLS token...\n",
            " 71% 5934/8340 [1:34:30<34:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:02,174 >> Initializing global attention on CLS token...\n",
            " 71% 5935/8340 [1:34:31<34:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:03,027 >> Initializing global attention on CLS token...\n",
            " 71% 5936/8340 [1:34:32<34:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:03,885 >> Initializing global attention on CLS token...\n",
            " 71% 5937/8340 [1:34:33<34:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:04,737 >> Initializing global attention on CLS token...\n",
            " 71% 5938/8340 [1:34:34<34:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:05,593 >> Initializing global attention on CLS token...\n",
            " 71% 5939/8340 [1:34:35<34:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:06,447 >> Initializing global attention on CLS token...\n",
            " 71% 5940/8340 [1:34:36<34:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:07,302 >> Initializing global attention on CLS token...\n",
            " 71% 5941/8340 [1:34:36<34:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:08,155 >> Initializing global attention on CLS token...\n",
            " 71% 5942/8340 [1:34:37<34:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:09,012 >> Initializing global attention on CLS token...\n",
            " 71% 5943/8340 [1:34:38<34:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:09,861 >> Initializing global attention on CLS token...\n",
            " 71% 5944/8340 [1:34:39<34:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:10,718 >> Initializing global attention on CLS token...\n",
            " 71% 5945/8340 [1:34:40<34:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:11,570 >> Initializing global attention on CLS token...\n",
            " 71% 5946/8340 [1:34:41<34:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:12,424 >> Initializing global attention on CLS token...\n",
            " 71% 5947/8340 [1:34:42<34:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:13,280 >> Initializing global attention on CLS token...\n",
            " 71% 5948/8340 [1:34:42<34:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:14,136 >> Initializing global attention on CLS token...\n",
            " 71% 5949/8340 [1:34:43<34:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:14,992 >> Initializing global attention on CLS token...\n",
            " 71% 5950/8340 [1:34:44<34:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:15,849 >> Initializing global attention on CLS token...\n",
            " 71% 5951/8340 [1:34:45<34:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:16,700 >> Initializing global attention on CLS token...\n",
            " 71% 5952/8340 [1:34:46<33:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:17,553 >> Initializing global attention on CLS token...\n",
            " 71% 5953/8340 [1:34:47<33:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:18,408 >> Initializing global attention on CLS token...\n",
            " 71% 5954/8340 [1:34:48<33:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:19,265 >> Initializing global attention on CLS token...\n",
            " 71% 5955/8340 [1:34:48<33:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:20,117 >> Initializing global attention on CLS token...\n",
            " 71% 5956/8340 [1:34:49<33:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:20,970 >> Initializing global attention on CLS token...\n",
            " 71% 5957/8340 [1:34:50<33:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:21,826 >> Initializing global attention on CLS token...\n",
            " 71% 5958/8340 [1:34:51<33:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:22,677 >> Initializing global attention on CLS token...\n",
            " 71% 5959/8340 [1:34:52<33:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:23,534 >> Initializing global attention on CLS token...\n",
            " 71% 5960/8340 [1:34:53<33:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:24,388 >> Initializing global attention on CLS token...\n",
            " 71% 5961/8340 [1:34:53<33:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:25,243 >> Initializing global attention on CLS token...\n",
            " 71% 5962/8340 [1:34:54<33:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:26,099 >> Initializing global attention on CLS token...\n",
            " 71% 5963/8340 [1:34:55<33:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:26,953 >> Initializing global attention on CLS token...\n",
            " 72% 5964/8340 [1:34:56<33:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:27,806 >> Initializing global attention on CLS token...\n",
            " 72% 5965/8340 [1:34:57<33:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:28,661 >> Initializing global attention on CLS token...\n",
            " 72% 5966/8340 [1:34:58<33:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:29,520 >> Initializing global attention on CLS token...\n",
            " 72% 5967/8340 [1:34:59<33:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:30,377 >> Initializing global attention on CLS token...\n",
            " 72% 5968/8340 [1:34:59<33:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:31,226 >> Initializing global attention on CLS token...\n",
            " 72% 5969/8340 [1:35:00<33:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:32,081 >> Initializing global attention on CLS token...\n",
            " 72% 5970/8340 [1:35:01<33:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:32,937 >> Initializing global attention on CLS token...\n",
            " 72% 5971/8340 [1:35:02<33:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:33,796 >> Initializing global attention on CLS token...\n",
            " 72% 5972/8340 [1:35:03<33:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:34,649 >> Initializing global attention on CLS token...\n",
            " 72% 5973/8340 [1:35:04<33:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:35,502 >> Initializing global attention on CLS token...\n",
            " 72% 5974/8340 [1:35:05<33:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:36,360 >> Initializing global attention on CLS token...\n",
            " 72% 5975/8340 [1:35:05<33:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:37,214 >> Initializing global attention on CLS token...\n",
            " 72% 5976/8340 [1:35:06<33:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:38,068 >> Initializing global attention on CLS token...\n",
            " 72% 5977/8340 [1:35:07<33:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:38,923 >> Initializing global attention on CLS token...\n",
            " 72% 5978/8340 [1:35:08<33:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:39,779 >> Initializing global attention on CLS token...\n",
            " 72% 5979/8340 [1:35:09<33:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:40,633 >> Initializing global attention on CLS token...\n",
            " 72% 5980/8340 [1:35:10<33:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:41,489 >> Initializing global attention on CLS token...\n",
            " 72% 5981/8340 [1:35:11<33:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:42,345 >> Initializing global attention on CLS token...\n",
            " 72% 5982/8340 [1:35:11<33:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:43,198 >> Initializing global attention on CLS token...\n",
            " 72% 5983/8340 [1:35:12<33:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:44,054 >> Initializing global attention on CLS token...\n",
            " 72% 5984/8340 [1:35:13<33:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:44,911 >> Initializing global attention on CLS token...\n",
            " 72% 5985/8340 [1:35:14<33:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:45,780 >> Initializing global attention on CLS token...\n",
            " 72% 5986/8340 [1:35:15<33:47,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:46,646 >> Initializing global attention on CLS token...\n",
            " 72% 5987/8340 [1:35:16<33:51,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:47,512 >> Initializing global attention on CLS token...\n",
            " 72% 5988/8340 [1:35:17<33:45,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:48,366 >> Initializing global attention on CLS token...\n",
            " 72% 5989/8340 [1:35:17<33:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:49,217 >> Initializing global attention on CLS token...\n",
            " 72% 5990/8340 [1:35:18<33:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:50,071 >> Initializing global attention on CLS token...\n",
            " 72% 5991/8340 [1:35:19<33:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:50,927 >> Initializing global attention on CLS token...\n",
            " 72% 5992/8340 [1:35:20<33:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:51,780 >> Initializing global attention on CLS token...\n",
            " 72% 5993/8340 [1:35:21<33:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:52,633 >> Initializing global attention on CLS token...\n",
            " 72% 5994/8340 [1:35:22<33:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:53,491 >> Initializing global attention on CLS token...\n",
            " 72% 5995/8340 [1:35:23<33:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:54,340 >> Initializing global attention on CLS token...\n",
            " 72% 5996/8340 [1:35:23<33:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:55,198 >> Initializing global attention on CLS token...\n",
            " 72% 5997/8340 [1:35:24<33:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:56,054 >> Initializing global attention on CLS token...\n",
            " 72% 5998/8340 [1:35:25<33:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:56,908 >> Initializing global attention on CLS token...\n",
            " 72% 5999/8340 [1:35:26<33:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:57,764 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.047, 'learning_rate': 1.4058752997601917e-05, 'epoch': 7.19}\n",
            " 72% 6000/8340 [1:35:27<34:54,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:58,763 >> Initializing global attention on CLS token...\n",
            " 72% 6001/8340 [1:35:28<34:34,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:33:59,619 >> Initializing global attention on CLS token...\n",
            " 72% 6002/8340 [1:35:29<34:13,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:00,475 >> Initializing global attention on CLS token...\n",
            " 72% 6003/8340 [1:35:30<33:56,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:01,331 >> Initializing global attention on CLS token...\n",
            " 72% 6004/8340 [1:35:30<33:48,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:02,193 >> Initializing global attention on CLS token...\n",
            " 72% 6005/8340 [1:35:31<33:37,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:03,046 >> Initializing global attention on CLS token...\n",
            " 72% 6006/8340 [1:35:32<33:29,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:03,902 >> Initializing global attention on CLS token...\n",
            " 72% 6007/8340 [1:35:33<33:22,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:04,754 >> Initializing global attention on CLS token...\n",
            " 72% 6008/8340 [1:35:34<33:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:05,609 >> Initializing global attention on CLS token...\n",
            " 72% 6009/8340 [1:35:35<33:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:06,461 >> Initializing global attention on CLS token...\n",
            " 72% 6010/8340 [1:35:36<33:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:07,319 >> Initializing global attention on CLS token...\n",
            " 72% 6011/8340 [1:35:36<33:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:08,170 >> Initializing global attention on CLS token...\n",
            " 72% 6012/8340 [1:35:37<33:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:09,024 >> Initializing global attention on CLS token...\n",
            " 72% 6013/8340 [1:35:38<33:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:09,877 >> Initializing global attention on CLS token...\n",
            " 72% 6014/8340 [1:35:39<33:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:10,730 >> Initializing global attention on CLS token...\n",
            " 72% 6015/8340 [1:35:40<33:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:11,582 >> Initializing global attention on CLS token...\n",
            " 72% 6016/8340 [1:35:41<33:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:12,438 >> Initializing global attention on CLS token...\n",
            " 72% 6017/8340 [1:35:42<33:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:13,288 >> Initializing global attention on CLS token...\n",
            " 72% 6018/8340 [1:35:42<33:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:14,138 >> Initializing global attention on CLS token...\n",
            " 72% 6019/8340 [1:35:43<33:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:14,994 >> Initializing global attention on CLS token...\n",
            " 72% 6020/8340 [1:35:44<32:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:15,847 >> Initializing global attention on CLS token...\n",
            " 72% 6021/8340 [1:35:45<33:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:16,704 >> Initializing global attention on CLS token...\n",
            " 72% 6022/8340 [1:35:46<33:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:17,560 >> Initializing global attention on CLS token...\n",
            " 72% 6023/8340 [1:35:47<32:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:18,414 >> Initializing global attention on CLS token...\n",
            " 72% 6024/8340 [1:35:48<32:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:19,264 >> Initializing global attention on CLS token...\n",
            " 72% 6025/8340 [1:35:48<32:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:20,120 >> Initializing global attention on CLS token...\n",
            " 72% 6026/8340 [1:35:49<32:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:20,973 >> Initializing global attention on CLS token...\n",
            " 72% 6027/8340 [1:35:50<32:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:21,827 >> Initializing global attention on CLS token...\n",
            " 72% 6028/8340 [1:35:51<32:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:22,680 >> Initializing global attention on CLS token...\n",
            " 72% 6029/8340 [1:35:52<32:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:23,535 >> Initializing global attention on CLS token...\n",
            " 72% 6030/8340 [1:35:53<32:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:24,390 >> Initializing global attention on CLS token...\n",
            " 72% 6031/8340 [1:35:53<32:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:25,243 >> Initializing global attention on CLS token...\n",
            " 72% 6032/8340 [1:35:54<32:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:26,098 >> Initializing global attention on CLS token...\n",
            " 72% 6033/8340 [1:35:55<32:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:26,953 >> Initializing global attention on CLS token...\n",
            " 72% 6034/8340 [1:35:56<32:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:27,807 >> Initializing global attention on CLS token...\n",
            " 72% 6035/8340 [1:35:57<32:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:28,661 >> Initializing global attention on CLS token...\n",
            " 72% 6036/8340 [1:35:58<32:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:29,516 >> Initializing global attention on CLS token...\n",
            " 72% 6037/8340 [1:35:59<32:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:30,374 >> Initializing global attention on CLS token...\n",
            " 72% 6038/8340 [1:35:59<32:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:31,226 >> Initializing global attention on CLS token...\n",
            " 72% 6039/8340 [1:36:00<32:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:32,085 >> Initializing global attention on CLS token...\n",
            " 72% 6040/8340 [1:36:01<32:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:32,936 >> Initializing global attention on CLS token...\n",
            " 72% 6041/8340 [1:36:02<32:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:33,789 >> Initializing global attention on CLS token...\n",
            " 72% 6042/8340 [1:36:03<32:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:34,643 >> Initializing global attention on CLS token...\n",
            " 72% 6043/8340 [1:36:04<32:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:35,501 >> Initializing global attention on CLS token...\n",
            " 72% 6044/8340 [1:36:05<32:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:36,359 >> Initializing global attention on CLS token...\n",
            " 72% 6045/8340 [1:36:05<32:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:37,212 >> Initializing global attention on CLS token...\n",
            " 72% 6046/8340 [1:36:06<32:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:38,068 >> Initializing global attention on CLS token...\n",
            " 73% 6047/8340 [1:36:07<32:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:38,927 >> Initializing global attention on CLS token...\n",
            " 73% 6048/8340 [1:36:08<32:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:39,778 >> Initializing global attention on CLS token...\n",
            " 73% 6049/8340 [1:36:09<32:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:40,633 >> Initializing global attention on CLS token...\n",
            " 73% 6050/8340 [1:36:10<32:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:41,490 >> Initializing global attention on CLS token...\n",
            " 73% 6051/8340 [1:36:11<32:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:42,347 >> Initializing global attention on CLS token...\n",
            " 73% 6052/8340 [1:36:11<32:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:43,201 >> Initializing global attention on CLS token...\n",
            " 73% 6053/8340 [1:36:12<32:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:44,056 >> Initializing global attention on CLS token...\n",
            " 73% 6054/8340 [1:36:13<32:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:44,913 >> Initializing global attention on CLS token...\n",
            " 73% 6055/8340 [1:36:14<32:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:45,768 >> Initializing global attention on CLS token...\n",
            " 73% 6056/8340 [1:36:15<32:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:46,624 >> Initializing global attention on CLS token...\n",
            " 73% 6057/8340 [1:36:16<32:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:47,478 >> Initializing global attention on CLS token...\n",
            " 73% 6058/8340 [1:36:17<32:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:48,333 >> Initializing global attention on CLS token...\n",
            " 73% 6059/8340 [1:36:17<32:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:49,188 >> Initializing global attention on CLS token...\n",
            " 73% 6060/8340 [1:36:18<32:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:50,048 >> Initializing global attention on CLS token...\n",
            " 73% 6061/8340 [1:36:19<32:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:50,899 >> Initializing global attention on CLS token...\n",
            " 73% 6062/8340 [1:36:20<32:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:51,752 >> Initializing global attention on CLS token...\n",
            " 73% 6063/8340 [1:36:21<32:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:52,612 >> Initializing global attention on CLS token...\n",
            " 73% 6064/8340 [1:36:22<32:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:53,465 >> Initializing global attention on CLS token...\n",
            " 73% 6065/8340 [1:36:23<32:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:54,312 >> Initializing global attention on CLS token...\n",
            " 73% 6066/8340 [1:36:23<32:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:55,166 >> Initializing global attention on CLS token...\n",
            " 73% 6067/8340 [1:36:24<32:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:56,025 >> Initializing global attention on CLS token...\n",
            " 73% 6068/8340 [1:36:25<32:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:56,877 >> Initializing global attention on CLS token...\n",
            " 73% 6069/8340 [1:36:26<32:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:57,726 >> Initializing global attention on CLS token...\n",
            " 73% 6070/8340 [1:36:27<32:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:58,580 >> Initializing global attention on CLS token...\n",
            " 73% 6071/8340 [1:36:28<32:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:34:59,434 >> Initializing global attention on CLS token...\n",
            " 73% 6072/8340 [1:36:29<32:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:00,288 >> Initializing global attention on CLS token...\n",
            " 73% 6073/8340 [1:36:29<32:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:01,145 >> Initializing global attention on CLS token...\n",
            " 73% 6074/8340 [1:36:30<32:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:01,999 >> Initializing global attention on CLS token...\n",
            " 73% 6075/8340 [1:36:31<32:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:02,850 >> Initializing global attention on CLS token...\n",
            " 73% 6076/8340 [1:36:32<32:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:03,706 >> Initializing global attention on CLS token...\n",
            " 73% 6077/8340 [1:36:33<32:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:04,557 >> Initializing global attention on CLS token...\n",
            " 73% 6078/8340 [1:36:34<32:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:05,413 >> Initializing global attention on CLS token...\n",
            " 73% 6079/8340 [1:36:35<32:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:06,265 >> Initializing global attention on CLS token...\n",
            " 73% 6080/8340 [1:36:35<32:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:07,120 >> Initializing global attention on CLS token...\n",
            " 73% 6081/8340 [1:36:36<32:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:07,977 >> Initializing global attention on CLS token...\n",
            " 73% 6082/8340 [1:36:37<32:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:08,829 >> Initializing global attention on CLS token...\n",
            " 73% 6083/8340 [1:36:38<32:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:09,681 >> Initializing global attention on CLS token...\n",
            " 73% 6084/8340 [1:36:39<32:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:10,534 >> Initializing global attention on CLS token...\n",
            " 73% 6085/8340 [1:36:40<32:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:11,389 >> Initializing global attention on CLS token...\n",
            " 73% 6086/8340 [1:36:40<32:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:12,242 >> Initializing global attention on CLS token...\n",
            " 73% 6087/8340 [1:36:41<32:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:13,099 >> Initializing global attention on CLS token...\n",
            " 73% 6088/8340 [1:36:42<32:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:13,952 >> Initializing global attention on CLS token...\n",
            " 73% 6089/8340 [1:36:43<32:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:14,804 >> Initializing global attention on CLS token...\n",
            " 73% 6090/8340 [1:36:44<32:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:15,662 >> Initializing global attention on CLS token...\n",
            " 73% 6091/8340 [1:36:45<32:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:16,514 >> Initializing global attention on CLS token...\n",
            " 73% 6092/8340 [1:36:46<32:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:17,374 >> Initializing global attention on CLS token...\n",
            " 73% 6093/8340 [1:36:46<31:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:18,226 >> Initializing global attention on CLS token...\n",
            " 73% 6094/8340 [1:36:47<31:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:19,078 >> Initializing global attention on CLS token...\n",
            " 73% 6095/8340 [1:36:48<31:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:19,933 >> Initializing global attention on CLS token...\n",
            " 73% 6096/8340 [1:36:49<31:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:20,788 >> Initializing global attention on CLS token...\n",
            " 73% 6097/8340 [1:36:50<31:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:21,641 >> Initializing global attention on CLS token...\n",
            " 73% 6098/8340 [1:36:51<31:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:22,497 >> Initializing global attention on CLS token...\n",
            " 73% 6099/8340 [1:36:52<31:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:23,351 >> Initializing global attention on CLS token...\n",
            " 73% 6100/8340 [1:36:52<31:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:24,206 >> Initializing global attention on CLS token...\n",
            " 73% 6101/8340 [1:36:53<31:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:25,065 >> Initializing global attention on CLS token...\n",
            " 73% 6102/8340 [1:36:54<31:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:25,913 >> Initializing global attention on CLS token...\n",
            " 73% 6103/8340 [1:36:55<31:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:26,768 >> Initializing global attention on CLS token...\n",
            " 73% 6104/8340 [1:36:56<31:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:27,621 >> Initializing global attention on CLS token...\n",
            " 73% 6105/8340 [1:36:57<31:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:28,476 >> Initializing global attention on CLS token...\n",
            " 73% 6106/8340 [1:36:58<31:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:29,328 >> Initializing global attention on CLS token...\n",
            " 73% 6107/8340 [1:36:58<31:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:30,181 >> Initializing global attention on CLS token...\n",
            " 73% 6108/8340 [1:36:59<31:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:31,038 >> Initializing global attention on CLS token...\n",
            " 73% 6109/8340 [1:37:00<31:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:31,888 >> Initializing global attention on CLS token...\n",
            " 73% 6110/8340 [1:37:01<31:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:32,743 >> Initializing global attention on CLS token...\n",
            " 73% 6111/8340 [1:37:02<31:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:33,598 >> Initializing global attention on CLS token...\n",
            " 73% 6112/8340 [1:37:03<31:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:34,449 >> Initializing global attention on CLS token...\n",
            " 73% 6113/8340 [1:37:04<31:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:35,303 >> Initializing global attention on CLS token...\n",
            " 73% 6114/8340 [1:37:04<31:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:36,159 >> Initializing global attention on CLS token...\n",
            " 73% 6115/8340 [1:37:05<31:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:37,018 >> Initializing global attention on CLS token...\n",
            " 73% 6116/8340 [1:37:06<31:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:37,868 >> Initializing global attention on CLS token...\n",
            " 73% 6117/8340 [1:37:07<31:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:38,721 >> Initializing global attention on CLS token...\n",
            " 73% 6118/8340 [1:37:08<31:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:39,578 >> Initializing global attention on CLS token...\n",
            " 73% 6119/8340 [1:37:09<31:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:40,431 >> Initializing global attention on CLS token...\n",
            " 73% 6120/8340 [1:37:10<31:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:41,286 >> Initializing global attention on CLS token...\n",
            " 73% 6121/8340 [1:37:10<31:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:42,141 >> Initializing global attention on CLS token...\n",
            " 73% 6122/8340 [1:37:11<31:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:42,997 >> Initializing global attention on CLS token...\n",
            " 73% 6123/8340 [1:37:12<31:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:43,853 >> Initializing global attention on CLS token...\n",
            " 73% 6124/8340 [1:37:13<31:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:44,706 >> Initializing global attention on CLS token...\n",
            " 73% 6125/8340 [1:37:14<31:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:45,560 >> Initializing global attention on CLS token...\n",
            " 73% 6126/8340 [1:37:15<31:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:46,417 >> Initializing global attention on CLS token...\n",
            " 73% 6127/8340 [1:37:16<31:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:47,275 >> Initializing global attention on CLS token...\n",
            " 73% 6128/8340 [1:37:16<31:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:48,128 >> Initializing global attention on CLS token...\n",
            " 73% 6129/8340 [1:37:17<31:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:48,979 >> Initializing global attention on CLS token...\n",
            " 74% 6130/8340 [1:37:18<31:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:49,835 >> Initializing global attention on CLS token...\n",
            " 74% 6131/8340 [1:37:19<31:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:50,689 >> Initializing global attention on CLS token...\n",
            " 74% 6132/8340 [1:37:20<31:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:51,544 >> Initializing global attention on CLS token...\n",
            " 74% 6133/8340 [1:37:21<31:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:52,402 >> Initializing global attention on CLS token...\n",
            " 74% 6134/8340 [1:37:22<31:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:53,256 >> Initializing global attention on CLS token...\n",
            " 74% 6135/8340 [1:37:22<31:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:54,115 >> Initializing global attention on CLS token...\n",
            " 74% 6136/8340 [1:37:23<31:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:54,973 >> Initializing global attention on CLS token...\n",
            " 74% 6137/8340 [1:37:24<31:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:55,828 >> Initializing global attention on CLS token...\n",
            " 74% 6138/8340 [1:37:25<31:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:56,676 >> Initializing global attention on CLS token...\n",
            " 74% 6139/8340 [1:37:26<31:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:57,531 >> Initializing global attention on CLS token...\n",
            " 74% 6140/8340 [1:37:27<31:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:58,386 >> Initializing global attention on CLS token...\n",
            " 74% 6141/8340 [1:37:27<31:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:35:59,239 >> Initializing global attention on CLS token...\n",
            " 74% 6142/8340 [1:37:28<31:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:00,094 >> Initializing global attention on CLS token...\n",
            " 74% 6143/8340 [1:37:29<31:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:00,950 >> Initializing global attention on CLS token...\n",
            " 74% 6144/8340 [1:37:30<31:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:01,803 >> Initializing global attention on CLS token...\n",
            " 74% 6145/8340 [1:37:31<31:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:02,659 >> Initializing global attention on CLS token...\n",
            " 74% 6146/8340 [1:37:32<31:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:03,512 >> Initializing global attention on CLS token...\n",
            " 74% 6147/8340 [1:37:33<31:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:04,368 >> Initializing global attention on CLS token...\n",
            " 74% 6148/8340 [1:37:33<31:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:05,221 >> Initializing global attention on CLS token...\n",
            " 74% 6149/8340 [1:37:34<31:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:06,075 >> Initializing global attention on CLS token...\n",
            " 74% 6150/8340 [1:37:35<31:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:06,931 >> Initializing global attention on CLS token...\n",
            " 74% 6151/8340 [1:37:36<31:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:07,786 >> Initializing global attention on CLS token...\n",
            " 74% 6152/8340 [1:37:37<31:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:08,639 >> Initializing global attention on CLS token...\n",
            " 74% 6153/8340 [1:37:38<31:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:09,495 >> Initializing global attention on CLS token...\n",
            " 74% 6154/8340 [1:37:39<31:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:10,353 >> Initializing global attention on CLS token...\n",
            " 74% 6155/8340 [1:37:39<31:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:11,203 >> Initializing global attention on CLS token...\n",
            " 74% 6156/8340 [1:37:40<31:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:12,054 >> Initializing global attention on CLS token...\n",
            " 74% 6157/8340 [1:37:41<31:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:12,910 >> Initializing global attention on CLS token...\n",
            " 74% 6158/8340 [1:37:42<31:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:13,765 >> Initializing global attention on CLS token...\n",
            " 74% 6159/8340 [1:37:43<31:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:14,617 >> Initializing global attention on CLS token...\n",
            " 74% 6160/8340 [1:37:44<31:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:15,472 >> Initializing global attention on CLS token...\n",
            " 74% 6161/8340 [1:37:45<31:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:16,325 >> Initializing global attention on CLS token...\n",
            " 74% 6162/8340 [1:37:45<30:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:17,176 >> Initializing global attention on CLS token...\n",
            " 74% 6163/8340 [1:37:46<30:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:18,032 >> Initializing global attention on CLS token...\n",
            " 74% 6164/8340 [1:37:47<30:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:18,887 >> Initializing global attention on CLS token...\n",
            " 74% 6165/8340 [1:37:48<30:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:19,742 >> Initializing global attention on CLS token...\n",
            " 74% 6166/8340 [1:37:49<30:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:20,598 >> Initializing global attention on CLS token...\n",
            " 74% 6167/8340 [1:37:50<30:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:21,450 >> Initializing global attention on CLS token...\n",
            " 74% 6168/8340 [1:37:51<30:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:22,305 >> Initializing global attention on CLS token...\n",
            " 74% 6169/8340 [1:37:51<30:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:23,158 >> Initializing global attention on CLS token...\n",
            " 74% 6170/8340 [1:37:52<30:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:24,009 >> Initializing global attention on CLS token...\n",
            " 74% 6171/8340 [1:37:53<30:45,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:24,854 >> Initializing global attention on CLS token...\n",
            " 74% 6172/8340 [1:37:54<30:44,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:25,706 >> Initializing global attention on CLS token...\n",
            " 74% 6173/8340 [1:37:55<30:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:26,557 >> Initializing global attention on CLS token...\n",
            " 74% 6174/8340 [1:37:56<30:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:27,412 >> Initializing global attention on CLS token...\n",
            " 74% 6175/8340 [1:37:57<30:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:28,266 >> Initializing global attention on CLS token...\n",
            " 74% 6176/8340 [1:37:57<30:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:29,119 >> Initializing global attention on CLS token...\n",
            " 74% 6177/8340 [1:37:58<30:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:29,976 >> Initializing global attention on CLS token...\n",
            " 74% 6178/8340 [1:37:59<30:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:30,827 >> Initializing global attention on CLS token...\n",
            " 74% 6179/8340 [1:38:00<30:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:31,682 >> Initializing global attention on CLS token...\n",
            " 74% 6180/8340 [1:38:01<30:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:32,536 >> Initializing global attention on CLS token...\n",
            " 74% 6181/8340 [1:38:02<30:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:33,387 >> Initializing global attention on CLS token...\n",
            " 74% 6182/8340 [1:38:02<30:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:34,245 >> Initializing global attention on CLS token...\n",
            " 74% 6183/8340 [1:38:03<30:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:35,101 >> Initializing global attention on CLS token...\n",
            " 74% 6184/8340 [1:38:04<30:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:35,952 >> Initializing global attention on CLS token...\n",
            " 74% 6185/8340 [1:38:05<30:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:36,806 >> Initializing global attention on CLS token...\n",
            " 74% 6186/8340 [1:38:06<30:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:37,659 >> Initializing global attention on CLS token...\n",
            " 74% 6187/8340 [1:38:07<30:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:38,513 >> Initializing global attention on CLS token...\n",
            " 74% 6188/8340 [1:38:08<30:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:39,375 >> Initializing global attention on CLS token...\n",
            " 74% 6189/8340 [1:38:08<30:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:40,224 >> Initializing global attention on CLS token...\n",
            " 74% 6190/8340 [1:38:09<30:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:41,075 >> Initializing global attention on CLS token...\n",
            " 74% 6191/8340 [1:38:10<30:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:41,929 >> Initializing global attention on CLS token...\n",
            " 74% 6192/8340 [1:38:11<30:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:42,783 >> Initializing global attention on CLS token...\n",
            " 74% 6193/8340 [1:38:12<30:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:43,636 >> Initializing global attention on CLS token...\n",
            " 74% 6194/8340 [1:38:13<30:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:44,489 >> Initializing global attention on CLS token...\n",
            " 74% 6195/8340 [1:38:14<30:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:45,347 >> Initializing global attention on CLS token...\n",
            " 74% 6196/8340 [1:38:14<30:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:46,201 >> Initializing global attention on CLS token...\n",
            " 74% 6197/8340 [1:38:15<30:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:47,056 >> Initializing global attention on CLS token...\n",
            " 74% 6198/8340 [1:38:16<30:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:47,909 >> Initializing global attention on CLS token...\n",
            " 74% 6199/8340 [1:38:17<30:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:48,765 >> Initializing global attention on CLS token...\n",
            " 74% 6200/8340 [1:38:18<30:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:49,618 >> Initializing global attention on CLS token...\n",
            " 74% 6201/8340 [1:38:19<30:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:50,473 >> Initializing global attention on CLS token...\n",
            " 74% 6202/8340 [1:38:20<30:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:51,324 >> Initializing global attention on CLS token...\n",
            " 74% 6203/8340 [1:38:20<30:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:52,170 >> Initializing global attention on CLS token...\n",
            " 74% 6204/8340 [1:38:21<30:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:53,023 >> Initializing global attention on CLS token...\n",
            " 74% 6205/8340 [1:38:22<30:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:53,878 >> Initializing global attention on CLS token...\n",
            " 74% 6206/8340 [1:38:23<30:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:54,736 >> Initializing global attention on CLS token...\n",
            " 74% 6207/8340 [1:38:24<30:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:55,595 >> Initializing global attention on CLS token...\n",
            " 74% 6208/8340 [1:38:25<30:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:56,451 >> Initializing global attention on CLS token...\n",
            " 74% 6209/8340 [1:38:26<30:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:57,304 >> Initializing global attention on CLS token...\n",
            " 74% 6210/8340 [1:38:26<30:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:58,165 >> Initializing global attention on CLS token...\n",
            " 74% 6211/8340 [1:38:27<30:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:59,018 >> Initializing global attention on CLS token...\n",
            " 74% 6212/8340 [1:38:28<30:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:36:59,870 >> Initializing global attention on CLS token...\n",
            " 74% 6213/8340 [1:38:29<30:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:00,725 >> Initializing global attention on CLS token...\n",
            " 75% 6214/8340 [1:38:30<30:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:01,572 >> Initializing global attention on CLS token...\n",
            " 75% 6215/8340 [1:38:31<30:07,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:02,417 >> Initializing global attention on CLS token...\n",
            " 75% 6216/8340 [1:38:32<30:07,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:03,270 >> Initializing global attention on CLS token...\n",
            " 75% 6217/8340 [1:38:32<30:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:04,124 >> Initializing global attention on CLS token...\n",
            " 75% 6218/8340 [1:38:33<30:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:04,981 >> Initializing global attention on CLS token...\n",
            " 75% 6219/8340 [1:38:34<30:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:05,834 >> Initializing global attention on CLS token...\n",
            " 75% 6220/8340 [1:38:35<30:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:06,687 >> Initializing global attention on CLS token...\n",
            " 75% 6221/8340 [1:38:36<30:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:07,547 >> Initializing global attention on CLS token...\n",
            " 75% 6222/8340 [1:38:37<30:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:08,401 >> Initializing global attention on CLS token...\n",
            " 75% 6223/8340 [1:38:38<30:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:09,257 >> Initializing global attention on CLS token...\n",
            " 75% 6224/8340 [1:38:38<30:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:10,110 >> Initializing global attention on CLS token...\n",
            " 75% 6225/8340 [1:38:39<30:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:10,963 >> Initializing global attention on CLS token...\n",
            " 75% 6226/8340 [1:38:40<30:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:11,820 >> Initializing global attention on CLS token...\n",
            " 75% 6227/8340 [1:38:41<30:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:12,674 >> Initializing global attention on CLS token...\n",
            " 75% 6228/8340 [1:38:42<30:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:13,529 >> Initializing global attention on CLS token...\n",
            " 75% 6229/8340 [1:38:43<30:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:14,381 >> Initializing global attention on CLS token...\n",
            " 75% 6230/8340 [1:38:43<30:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:15,235 >> Initializing global attention on CLS token...\n",
            " 75% 6231/8340 [1:38:44<30:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:16,090 >> Initializing global attention on CLS token...\n",
            " 75% 6232/8340 [1:38:45<30:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:16,963 >> Initializing global attention on CLS token...\n",
            " 75% 6233/8340 [1:38:46<30:16,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:17,847 >> Initializing global attention on CLS token...\n",
            " 75% 6234/8340 [1:38:47<30:27,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:18,723 >> Initializing global attention on CLS token...\n",
            " 75% 6235/8340 [1:38:48<30:41,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:19,619 >> Initializing global attention on CLS token...\n",
            " 75% 6236/8340 [1:38:49<30:58,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:20,587 >> Initializing global attention on CLS token...\n",
            " 75% 6237/8340 [1:38:50<31:47,  1.10it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:21,484 >> Initializing global attention on CLS token...\n",
            " 75% 6238/8340 [1:38:51<31:46,  1.10it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:22,389 >> Initializing global attention on CLS token...\n",
            " 75% 6239/8340 [1:38:52<32:52,  1.07it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:23,393 >> Initializing global attention on CLS token...\n",
            " 75% 6240/8340 [1:38:52<32:06,  1.09it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:24,249 >> Initializing global attention on CLS token...\n",
            " 75% 6241/8340 [1:38:53<31:21,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:25,096 >> Initializing global attention on CLS token...\n",
            " 75% 6242/8340 [1:38:54<30:56,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:25,965 >> Initializing global attention on CLS token...\n",
            " 75% 6243/8340 [1:38:55<30:44,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:26,825 >> Initializing global attention on CLS token...\n",
            " 75% 6244/8340 [1:38:56<30:27,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:27,675 >> Initializing global attention on CLS token...\n",
            " 75% 6245/8340 [1:38:57<30:16,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:28,531 >> Initializing global attention on CLS token...\n",
            " 75% 6246/8340 [1:38:58<30:05,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:29,383 >> Initializing global attention on CLS token...\n",
            " 75% 6247/8340 [1:38:58<30:00,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:30,239 >> Initializing global attention on CLS token...\n",
            " 75% 6248/8340 [1:38:59<29:56,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:31,093 >> Initializing global attention on CLS token...\n",
            " 75% 6249/8340 [1:39:00<29:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:31,946 >> Initializing global attention on CLS token...\n",
            " 75% 6250/8340 [1:39:01<29:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:32,801 >> Initializing global attention on CLS token...\n",
            " 75% 6251/8340 [1:39:02<29:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:33,656 >> Initializing global attention on CLS token...\n",
            " 75% 6252/8340 [1:39:03<29:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:34,512 >> Initializing global attention on CLS token...\n",
            " 75% 6253/8340 [1:39:04<29:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:35,366 >> Initializing global attention on CLS token...\n",
            " 75% 6254/8340 [1:39:04<29:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:36,218 >> Initializing global attention on CLS token...\n",
            " 75% 6255/8340 [1:39:05<29:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:37,076 >> Initializing global attention on CLS token...\n",
            " 75% 6256/8340 [1:39:06<29:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:37,926 >> Initializing global attention on CLS token...\n",
            " 75% 6257/8340 [1:39:07<29:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:38,780 >> Initializing global attention on CLS token...\n",
            " 75% 6258/8340 [1:39:08<29:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:39,635 >> Initializing global attention on CLS token...\n",
            " 75% 6259/8340 [1:39:09<29:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:40,493 >> Initializing global attention on CLS token...\n",
            " 75% 6260/8340 [1:39:10<29:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:41,346 >> Initializing global attention on CLS token...\n",
            " 75% 6261/8340 [1:39:10<29:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:42,201 >> Initializing global attention on CLS token...\n",
            " 75% 6262/8340 [1:39:11<29:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:43,056 >> Initializing global attention on CLS token...\n",
            " 75% 6263/8340 [1:39:12<29:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:43,910 >> Initializing global attention on CLS token...\n",
            " 75% 6264/8340 [1:39:13<29:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:44,762 >> Initializing global attention on CLS token...\n",
            " 75% 6265/8340 [1:39:14<29:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:45,623 >> Initializing global attention on CLS token...\n",
            " 75% 6266/8340 [1:39:15<29:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:46,471 >> Initializing global attention on CLS token...\n",
            " 75% 6267/8340 [1:39:16<29:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:47,324 >> Initializing global attention on CLS token...\n",
            " 75% 6268/8340 [1:39:16<29:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:48,178 >> Initializing global attention on CLS token...\n",
            " 75% 6269/8340 [1:39:17<29:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:49,035 >> Initializing global attention on CLS token...\n",
            " 75% 6270/8340 [1:39:18<29:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:49,889 >> Initializing global attention on CLS token...\n",
            " 75% 6271/8340 [1:39:19<29:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:50,741 >> Initializing global attention on CLS token...\n",
            " 75% 6272/8340 [1:39:20<29:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:51,596 >> Initializing global attention on CLS token...\n",
            " 75% 6273/8340 [1:39:21<29:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:52,451 >> Initializing global attention on CLS token...\n",
            " 75% 6274/8340 [1:39:22<29:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:53,304 >> Initializing global attention on CLS token...\n",
            " 75% 6275/8340 [1:39:22<29:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:54,159 >> Initializing global attention on CLS token...\n",
            " 75% 6276/8340 [1:39:23<29:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:55,014 >> Initializing global attention on CLS token...\n",
            " 75% 6277/8340 [1:39:24<29:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:55,871 >> Initializing global attention on CLS token...\n",
            " 75% 6278/8340 [1:39:25<29:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:56,725 >> Initializing global attention on CLS token...\n",
            " 75% 6279/8340 [1:39:26<29:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:57,580 >> Initializing global attention on CLS token...\n",
            " 75% 6280/8340 [1:39:27<29:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:58,439 >> Initializing global attention on CLS token...\n",
            " 75% 6281/8340 [1:39:28<29:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:37:59,312 >> Initializing global attention on CLS token...\n",
            " 75% 6282/8340 [1:39:28<29:32,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:00,170 >> Initializing global attention on CLS token...\n",
            " 75% 6283/8340 [1:39:29<29:27,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:01,025 >> Initializing global attention on CLS token...\n",
            " 75% 6284/8340 [1:39:30<29:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:01,877 >> Initializing global attention on CLS token...\n",
            " 75% 6285/8340 [1:39:31<29:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:02,730 >> Initializing global attention on CLS token...\n",
            " 75% 6286/8340 [1:39:32<29:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:03,584 >> Initializing global attention on CLS token...\n",
            " 75% 6287/8340 [1:39:33<29:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:04,439 >> Initializing global attention on CLS token...\n",
            " 75% 6288/8340 [1:39:34<29:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:05,291 >> Initializing global attention on CLS token...\n",
            " 75% 6289/8340 [1:39:34<29:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:06,142 >> Initializing global attention on CLS token...\n",
            " 75% 6290/8340 [1:39:35<29:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:06,988 >> Initializing global attention on CLS token...\n",
            " 75% 6291/8340 [1:39:36<29:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:07,844 >> Initializing global attention on CLS token...\n",
            " 75% 6292/8340 [1:39:37<29:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:08,697 >> Initializing global attention on CLS token...\n",
            " 75% 6293/8340 [1:39:38<29:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:09,555 >> Initializing global attention on CLS token...\n",
            " 75% 6294/8340 [1:39:39<29:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:10,410 >> Initializing global attention on CLS token...\n",
            " 75% 6295/8340 [1:39:40<29:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:11,265 >> Initializing global attention on CLS token...\n",
            " 75% 6296/8340 [1:39:40<29:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:12,120 >> Initializing global attention on CLS token...\n",
            " 76% 6297/8340 [1:39:41<29:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:12,977 >> Initializing global attention on CLS token...\n",
            " 76% 6298/8340 [1:39:42<29:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:13,830 >> Initializing global attention on CLS token...\n",
            " 76% 6299/8340 [1:39:43<29:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:14,683 >> Initializing global attention on CLS token...\n",
            " 76% 6300/8340 [1:39:44<29:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:15,536 >> Initializing global attention on CLS token...\n",
            " 76% 6301/8340 [1:39:45<28:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:16,389 >> Initializing global attention on CLS token...\n",
            " 76% 6302/8340 [1:39:45<28:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:17,243 >> Initializing global attention on CLS token...\n",
            " 76% 6303/8340 [1:39:46<28:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:18,098 >> Initializing global attention on CLS token...\n",
            " 76% 6304/8340 [1:39:47<28:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:18,953 >> Initializing global attention on CLS token...\n",
            " 76% 6305/8340 [1:39:48<28:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:19,810 >> Initializing global attention on CLS token...\n",
            " 76% 6306/8340 [1:39:49<28:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:20,662 >> Initializing global attention on CLS token...\n",
            " 76% 6307/8340 [1:39:50<28:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:21,521 >> Initializing global attention on CLS token...\n",
            " 76% 6308/8340 [1:39:51<28:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:22,373 >> Initializing global attention on CLS token...\n",
            " 76% 6309/8340 [1:39:51<28:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:23,225 >> Initializing global attention on CLS token...\n",
            " 76% 6310/8340 [1:39:52<28:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:24,083 >> Initializing global attention on CLS token...\n",
            " 76% 6311/8340 [1:39:53<28:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:24,933 >> Initializing global attention on CLS token...\n",
            " 76% 6312/8340 [1:39:54<28:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:25,788 >> Initializing global attention on CLS token...\n",
            " 76% 6313/8340 [1:39:55<28:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:26,643 >> Initializing global attention on CLS token...\n",
            " 76% 6314/8340 [1:39:56<28:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:27,497 >> Initializing global attention on CLS token...\n",
            " 76% 6315/8340 [1:39:57<28:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:28,353 >> Initializing global attention on CLS token...\n",
            " 76% 6316/8340 [1:39:57<28:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:29,209 >> Initializing global attention on CLS token...\n",
            " 76% 6317/8340 [1:39:58<28:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:30,058 >> Initializing global attention on CLS token...\n",
            " 76% 6318/8340 [1:39:59<28:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:30,913 >> Initializing global attention on CLS token...\n",
            " 76% 6319/8340 [1:40:00<28:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:31,768 >> Initializing global attention on CLS token...\n",
            " 76% 6320/8340 [1:40:01<28:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:32,622 >> Initializing global attention on CLS token...\n",
            " 76% 6321/8340 [1:40:02<28:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:33,477 >> Initializing global attention on CLS token...\n",
            " 76% 6322/8340 [1:40:03<28:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:34,331 >> Initializing global attention on CLS token...\n",
            " 76% 6323/8340 [1:40:03<28:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:35,184 >> Initializing global attention on CLS token...\n",
            " 76% 6324/8340 [1:40:04<28:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:36,044 >> Initializing global attention on CLS token...\n",
            " 76% 6325/8340 [1:40:05<28:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:36,894 >> Initializing global attention on CLS token...\n",
            " 76% 6326/8340 [1:40:06<28:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:37,746 >> Initializing global attention on CLS token...\n",
            " 76% 6327/8340 [1:40:07<28:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:38,602 >> Initializing global attention on CLS token...\n",
            " 76% 6328/8340 [1:40:08<28:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:39,454 >> Initializing global attention on CLS token...\n",
            " 76% 6329/8340 [1:40:09<28:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:40,307 >> Initializing global attention on CLS token...\n",
            " 76% 6330/8340 [1:40:09<28:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:41,162 >> Initializing global attention on CLS token...\n",
            " 76% 6331/8340 [1:40:10<28:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:42,017 >> Initializing global attention on CLS token...\n",
            " 76% 6332/8340 [1:40:11<28:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:42,871 >> Initializing global attention on CLS token...\n",
            " 76% 6333/8340 [1:40:12<28:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:43,726 >> Initializing global attention on CLS token...\n",
            " 76% 6334/8340 [1:40:13<28:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:44,578 >> Initializing global attention on CLS token...\n",
            " 76% 6335/8340 [1:40:14<28:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:45,436 >> Initializing global attention on CLS token...\n",
            " 76% 6336/8340 [1:40:15<28:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:46,288 >> Initializing global attention on CLS token...\n",
            " 76% 6337/8340 [1:40:15<28:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:47,141 >> Initializing global attention on CLS token...\n",
            " 76% 6338/8340 [1:40:16<28:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:47,996 >> Initializing global attention on CLS token...\n",
            " 76% 6339/8340 [1:40:17<28:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:48,850 >> Initializing global attention on CLS token...\n",
            " 76% 6340/8340 [1:40:18<28:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:49,705 >> Initializing global attention on CLS token...\n",
            " 76% 6341/8340 [1:40:19<28:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:50,557 >> Initializing global attention on CLS token...\n",
            " 76% 6342/8340 [1:40:20<28:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:51,412 >> Initializing global attention on CLS token...\n",
            " 76% 6343/8340 [1:40:21<28:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:52,267 >> Initializing global attention on CLS token...\n",
            " 76% 6344/8340 [1:40:21<28:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:53,122 >> Initializing global attention on CLS token...\n",
            " 76% 6345/8340 [1:40:22<28:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:53,974 >> Initializing global attention on CLS token...\n",
            " 76% 6346/8340 [1:40:23<28:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:54,832 >> Initializing global attention on CLS token...\n",
            " 76% 6347/8340 [1:40:24<28:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:55,682 >> Initializing global attention on CLS token...\n",
            " 76% 6348/8340 [1:40:25<28:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:56,533 >> Initializing global attention on CLS token...\n",
            " 76% 6349/8340 [1:40:26<28:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:57,392 >> Initializing global attention on CLS token...\n",
            " 76% 6350/8340 [1:40:26<28:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:58,243 >> Initializing global attention on CLS token...\n",
            " 76% 6351/8340 [1:40:27<28:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:59,097 >> Initializing global attention on CLS token...\n",
            " 76% 6352/8340 [1:40:28<28:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:38:59,949 >> Initializing global attention on CLS token...\n",
            " 76% 6353/8340 [1:40:29<28:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:00,808 >> Initializing global attention on CLS token...\n",
            " 76% 6354/8340 [1:40:30<28:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:01,666 >> Initializing global attention on CLS token...\n",
            " 76% 6355/8340 [1:40:31<28:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:02,520 >> Initializing global attention on CLS token...\n",
            " 76% 6356/8340 [1:40:32<28:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:03,374 >> Initializing global attention on CLS token...\n",
            " 76% 6357/8340 [1:40:32<28:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:04,222 >> Initializing global attention on CLS token...\n",
            " 76% 6358/8340 [1:40:33<28:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:05,081 >> Initializing global attention on CLS token...\n",
            " 76% 6359/8340 [1:40:34<28:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:05,933 >> Initializing global attention on CLS token...\n",
            " 76% 6360/8340 [1:40:35<28:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:06,788 >> Initializing global attention on CLS token...\n",
            " 76% 6361/8340 [1:40:36<28:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:07,645 >> Initializing global attention on CLS token...\n",
            " 76% 6362/8340 [1:40:37<28:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:08,498 >> Initializing global attention on CLS token...\n",
            " 76% 6363/8340 [1:40:38<28:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:09,357 >> Initializing global attention on CLS token...\n",
            " 76% 6364/8340 [1:40:38<28:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:10,210 >> Initializing global attention on CLS token...\n",
            " 76% 6365/8340 [1:40:39<28:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:11,064 >> Initializing global attention on CLS token...\n",
            " 76% 6366/8340 [1:40:40<28:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:11,920 >> Initializing global attention on CLS token...\n",
            " 76% 6367/8340 [1:40:41<28:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:12,767 >> Initializing global attention on CLS token...\n",
            " 76% 6368/8340 [1:40:42<27:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:13,615 >> Initializing global attention on CLS token...\n",
            " 76% 6369/8340 [1:40:43<28:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:14,472 >> Initializing global attention on CLS token...\n",
            " 76% 6370/8340 [1:40:44<28:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:15,328 >> Initializing global attention on CLS token...\n",
            " 76% 6371/8340 [1:40:44<28:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:16,184 >> Initializing global attention on CLS token...\n",
            " 76% 6372/8340 [1:40:45<28:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:17,040 >> Initializing global attention on CLS token...\n",
            " 76% 6373/8340 [1:40:46<28:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:17,893 >> Initializing global attention on CLS token...\n",
            " 76% 6374/8340 [1:40:47<28:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:18,752 >> Initializing global attention on CLS token...\n",
            " 76% 6375/8340 [1:40:48<27:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:19,604 >> Initializing global attention on CLS token...\n",
            " 76% 6376/8340 [1:40:49<27:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:20,462 >> Initializing global attention on CLS token...\n",
            " 76% 6377/8340 [1:40:50<27:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:21,317 >> Initializing global attention on CLS token...\n",
            " 76% 6378/8340 [1:40:50<27:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:22,170 >> Initializing global attention on CLS token...\n",
            " 76% 6379/8340 [1:40:51<27:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:23,023 >> Initializing global attention on CLS token...\n",
            " 76% 6380/8340 [1:40:52<27:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:23,876 >> Initializing global attention on CLS token...\n",
            " 77% 6381/8340 [1:40:53<27:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:24,730 >> Initializing global attention on CLS token...\n",
            " 77% 6382/8340 [1:40:54<27:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:25,584 >> Initializing global attention on CLS token...\n",
            " 77% 6383/8340 [1:40:55<27:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:26,438 >> Initializing global attention on CLS token...\n",
            " 77% 6384/8340 [1:40:56<27:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:27,294 >> Initializing global attention on CLS token...\n",
            " 77% 6385/8340 [1:40:56<27:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:28,146 >> Initializing global attention on CLS token...\n",
            " 77% 6386/8340 [1:40:57<27:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:29,002 >> Initializing global attention on CLS token...\n",
            " 77% 6387/8340 [1:40:58<27:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:29,858 >> Initializing global attention on CLS token...\n",
            " 77% 6388/8340 [1:40:59<27:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:30,710 >> Initializing global attention on CLS token...\n",
            " 77% 6389/8340 [1:41:00<27:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:31,572 >> Initializing global attention on CLS token...\n",
            " 77% 6390/8340 [1:41:01<27:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:32,420 >> Initializing global attention on CLS token...\n",
            " 77% 6391/8340 [1:41:02<27:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:33,274 >> Initializing global attention on CLS token...\n",
            " 77% 6392/8340 [1:41:02<27:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:34,128 >> Initializing global attention on CLS token...\n",
            " 77% 6393/8340 [1:41:03<27:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:34,982 >> Initializing global attention on CLS token...\n",
            " 77% 6394/8340 [1:41:04<27:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:35,833 >> Initializing global attention on CLS token...\n",
            " 77% 6395/8340 [1:41:05<27:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:36,688 >> Initializing global attention on CLS token...\n",
            " 77% 6396/8340 [1:41:06<27:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:37,544 >> Initializing global attention on CLS token...\n",
            " 77% 6397/8340 [1:41:07<27:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:38,397 >> Initializing global attention on CLS token...\n",
            " 77% 6398/8340 [1:41:07<27:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:39,251 >> Initializing global attention on CLS token...\n",
            " 77% 6399/8340 [1:41:08<27:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:40,106 >> Initializing global attention on CLS token...\n",
            " 77% 6400/8340 [1:41:09<27:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:40,958 >> Initializing global attention on CLS token...\n",
            " 77% 6401/8340 [1:41:10<27:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:41,812 >> Initializing global attention on CLS token...\n",
            " 77% 6402/8340 [1:41:11<27:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:42,667 >> Initializing global attention on CLS token...\n",
            " 77% 6403/8340 [1:41:12<27:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:43,520 >> Initializing global attention on CLS token...\n",
            " 77% 6404/8340 [1:41:13<27:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:44,377 >> Initializing global attention on CLS token...\n",
            " 77% 6405/8340 [1:41:13<27:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:45,232 >> Initializing global attention on CLS token...\n",
            " 77% 6406/8340 [1:41:14<27:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:46,083 >> Initializing global attention on CLS token...\n",
            " 77% 6407/8340 [1:41:15<27:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:46,938 >> Initializing global attention on CLS token...\n",
            " 77% 6408/8340 [1:41:16<27:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:47,791 >> Initializing global attention on CLS token...\n",
            " 77% 6409/8340 [1:41:17<27:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:48,646 >> Initializing global attention on CLS token...\n",
            " 77% 6410/8340 [1:41:18<27:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:49,502 >> Initializing global attention on CLS token...\n",
            " 77% 6411/8340 [1:41:19<27:28,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:50,362 >> Initializing global attention on CLS token...\n",
            " 77% 6412/8340 [1:41:19<27:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:51,212 >> Initializing global attention on CLS token...\n",
            " 77% 6413/8340 [1:41:20<27:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:52,062 >> Initializing global attention on CLS token...\n",
            " 77% 6414/8340 [1:41:21<27:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:52,916 >> Initializing global attention on CLS token...\n",
            " 77% 6415/8340 [1:41:22<27:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:53,771 >> Initializing global attention on CLS token...\n",
            " 77% 6416/8340 [1:41:23<27:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:54,626 >> Initializing global attention on CLS token...\n",
            " 77% 6417/8340 [1:41:24<27:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:55,477 >> Initializing global attention on CLS token...\n",
            " 77% 6418/8340 [1:41:25<27:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:56,331 >> Initializing global attention on CLS token...\n",
            " 77% 6419/8340 [1:41:25<27:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:57,194 >> Initializing global attention on CLS token...\n",
            " 77% 6420/8340 [1:41:26<27:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:58,040 >> Initializing global attention on CLS token...\n",
            " 77% 6421/8340 [1:41:27<27:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:58,895 >> Initializing global attention on CLS token...\n",
            " 77% 6422/8340 [1:41:28<27:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:39:59,748 >> Initializing global attention on CLS token...\n",
            " 77% 6423/8340 [1:41:29<27:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:00,602 >> Initializing global attention on CLS token...\n",
            " 77% 6424/8340 [1:41:30<27:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:01,455 >> Initializing global attention on CLS token...\n",
            " 77% 6425/8340 [1:41:31<27:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:02,310 >> Initializing global attention on CLS token...\n",
            " 77% 6426/8340 [1:41:31<27:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:03,166 >> Initializing global attention on CLS token...\n",
            " 77% 6427/8340 [1:41:32<27:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:04,020 >> Initializing global attention on CLS token...\n",
            " 77% 6428/8340 [1:41:33<27:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:04,876 >> Initializing global attention on CLS token...\n",
            " 77% 6429/8340 [1:41:34<27:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:05,730 >> Initializing global attention on CLS token...\n",
            " 77% 6430/8340 [1:41:35<27:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:06,584 >> Initializing global attention on CLS token...\n",
            " 77% 6431/8340 [1:41:36<27:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:07,435 >> Initializing global attention on CLS token...\n",
            " 77% 6432/8340 [1:41:37<27:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:08,289 >> Initializing global attention on CLS token...\n",
            " 77% 6433/8340 [1:41:37<27:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:09,144 >> Initializing global attention on CLS token...\n",
            " 77% 6434/8340 [1:41:38<27:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:09,998 >> Initializing global attention on CLS token...\n",
            " 77% 6435/8340 [1:41:39<27:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:10,853 >> Initializing global attention on CLS token...\n",
            " 77% 6436/8340 [1:41:40<27:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:11,708 >> Initializing global attention on CLS token...\n",
            " 77% 6437/8340 [1:41:41<27:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:12,561 >> Initializing global attention on CLS token...\n",
            " 77% 6438/8340 [1:41:42<27:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:13,417 >> Initializing global attention on CLS token...\n",
            " 77% 6439/8340 [1:41:43<27:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:14,272 >> Initializing global attention on CLS token...\n",
            " 77% 6440/8340 [1:41:43<26:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:15,118 >> Initializing global attention on CLS token...\n",
            " 77% 6441/8340 [1:41:44<26:55,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:15,965 >> Initializing global attention on CLS token...\n",
            " 77% 6442/8340 [1:41:45<26:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:16,820 >> Initializing global attention on CLS token...\n",
            " 77% 6443/8340 [1:41:46<26:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:17,675 >> Initializing global attention on CLS token...\n",
            " 77% 6444/8340 [1:41:47<26:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:18,532 >> Initializing global attention on CLS token...\n",
            " 77% 6445/8340 [1:41:48<26:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:19,384 >> Initializing global attention on CLS token...\n",
            " 77% 6446/8340 [1:41:48<26:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:20,243 >> Initializing global attention on CLS token...\n",
            " 77% 6447/8340 [1:41:49<26:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:21,098 >> Initializing global attention on CLS token...\n",
            " 77% 6448/8340 [1:41:50<26:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:21,953 >> Initializing global attention on CLS token...\n",
            " 77% 6449/8340 [1:41:51<26:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:22,810 >> Initializing global attention on CLS token...\n",
            " 77% 6450/8340 [1:41:52<26:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:23,668 >> Initializing global attention on CLS token...\n",
            " 77% 6451/8340 [1:41:53<26:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:24,523 >> Initializing global attention on CLS token...\n",
            " 77% 6452/8340 [1:41:54<26:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:25,378 >> Initializing global attention on CLS token...\n",
            " 77% 6453/8340 [1:41:54<26:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:26,228 >> Initializing global attention on CLS token...\n",
            " 77% 6454/8340 [1:41:55<26:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:27,084 >> Initializing global attention on CLS token...\n",
            " 77% 6455/8340 [1:41:56<26:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:27,937 >> Initializing global attention on CLS token...\n",
            " 77% 6456/8340 [1:41:57<26:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:28,792 >> Initializing global attention on CLS token...\n",
            " 77% 6457/8340 [1:41:58<26:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:29,647 >> Initializing global attention on CLS token...\n",
            " 77% 6458/8340 [1:41:59<26:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:30,500 >> Initializing global attention on CLS token...\n",
            " 77% 6459/8340 [1:42:00<26:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:31,355 >> Initializing global attention on CLS token...\n",
            " 77% 6460/8340 [1:42:00<26:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:32,209 >> Initializing global attention on CLS token...\n",
            " 77% 6461/8340 [1:42:01<26:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:33,060 >> Initializing global attention on CLS token...\n",
            " 77% 6462/8340 [1:42:02<26:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:33,912 >> Initializing global attention on CLS token...\n",
            " 77% 6463/8340 [1:42:03<26:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:34,767 >> Initializing global attention on CLS token...\n",
            " 78% 6464/8340 [1:42:04<26:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:35,621 >> Initializing global attention on CLS token...\n",
            " 78% 6465/8340 [1:42:05<26:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:36,477 >> Initializing global attention on CLS token...\n",
            " 78% 6466/8340 [1:42:06<26:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:37,331 >> Initializing global attention on CLS token...\n",
            " 78% 6467/8340 [1:42:06<26:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:38,186 >> Initializing global attention on CLS token...\n",
            " 78% 6468/8340 [1:42:07<26:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:39,040 >> Initializing global attention on CLS token...\n",
            " 78% 6469/8340 [1:42:08<26:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:39,895 >> Initializing global attention on CLS token...\n",
            " 78% 6470/8340 [1:42:09<26:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:40,749 >> Initializing global attention on CLS token...\n",
            " 78% 6471/8340 [1:42:10<26:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:41,604 >> Initializing global attention on CLS token...\n",
            " 78% 6472/8340 [1:42:11<26:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:42,457 >> Initializing global attention on CLS token...\n",
            " 78% 6473/8340 [1:42:12<26:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:43,315 >> Initializing global attention on CLS token...\n",
            " 78% 6474/8340 [1:42:12<26:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:44,168 >> Initializing global attention on CLS token...\n",
            " 78% 6475/8340 [1:42:13<26:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:45,021 >> Initializing global attention on CLS token...\n",
            " 78% 6476/8340 [1:42:14<26:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:45,876 >> Initializing global attention on CLS token...\n",
            " 78% 6477/8340 [1:42:15<26:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:46,733 >> Initializing global attention on CLS token...\n",
            " 78% 6478/8340 [1:42:16<26:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:47,588 >> Initializing global attention on CLS token...\n",
            " 78% 6479/8340 [1:42:17<26:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:48,438 >> Initializing global attention on CLS token...\n",
            " 78% 6480/8340 [1:42:18<26:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:49,291 >> Initializing global attention on CLS token...\n",
            " 78% 6481/8340 [1:42:18<26:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:50,145 >> Initializing global attention on CLS token...\n",
            " 78% 6482/8340 [1:42:19<26:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:50,995 >> Initializing global attention on CLS token...\n",
            " 78% 6483/8340 [1:42:20<26:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:51,849 >> Initializing global attention on CLS token...\n",
            " 78% 6484/8340 [1:42:21<26:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:52,703 >> Initializing global attention on CLS token...\n",
            " 78% 6485/8340 [1:42:22<26:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:53,556 >> Initializing global attention on CLS token...\n",
            " 78% 6486/8340 [1:42:23<26:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:54,410 >> Initializing global attention on CLS token...\n",
            " 78% 6487/8340 [1:42:24<26:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:55,263 >> Initializing global attention on CLS token...\n",
            " 78% 6488/8340 [1:42:24<26:21,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:56,116 >> Initializing global attention on CLS token...\n",
            " 78% 6489/8340 [1:42:25<26:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:56,969 >> Initializing global attention on CLS token...\n",
            " 78% 6490/8340 [1:42:26<26:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:57,824 >> Initializing global attention on CLS token...\n",
            " 78% 6491/8340 [1:42:27<26:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:58,677 >> Initializing global attention on CLS token...\n",
            " 78% 6492/8340 [1:42:28<26:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:40:59,532 >> Initializing global attention on CLS token...\n",
            " 78% 6493/8340 [1:42:29<26:17,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:00,385 >> Initializing global attention on CLS token...\n",
            " 78% 6494/8340 [1:42:29<26:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:01,242 >> Initializing global attention on CLS token...\n",
            " 78% 6495/8340 [1:42:30<26:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:02,094 >> Initializing global attention on CLS token...\n",
            " 78% 6496/8340 [1:42:31<26:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:02,947 >> Initializing global attention on CLS token...\n",
            " 78% 6497/8340 [1:42:32<26:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:03,811 >> Initializing global attention on CLS token...\n",
            " 78% 6498/8340 [1:42:33<26:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:04,659 >> Initializing global attention on CLS token...\n",
            " 78% 6499/8340 [1:42:34<26:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:05,515 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.0354, 'learning_rate': 1.1061151079136692e-05, 'epoch': 7.79}\n",
            " 78% 6500/8340 [1:42:35<27:24,  1.12it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:06,500 >> Initializing global attention on CLS token...\n",
            " 78% 6501/8340 [1:42:36<27:00,  1.13it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:07,353 >> Initializing global attention on CLS token...\n",
            " 78% 6502/8340 [1:42:36<26:46,  1.14it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:08,208 >> Initializing global attention on CLS token...\n",
            " 78% 6503/8340 [1:42:37<26:33,  1.15it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:09,062 >> Initializing global attention on CLS token...\n",
            " 78% 6504/8340 [1:42:38<26:24,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:09,912 >> Initializing global attention on CLS token...\n",
            " 78% 6505/8340 [1:42:39<26:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:10,758 >> Initializing global attention on CLS token...\n",
            " 78% 6506/8340 [1:42:40<26:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:11,610 >> Initializing global attention on CLS token...\n",
            " 78% 6507/8340 [1:42:41<26:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:12,461 >> Initializing global attention on CLS token...\n",
            " 78% 6508/8340 [1:42:42<26:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:13,316 >> Initializing global attention on CLS token...\n",
            " 78% 6509/8340 [1:42:42<26:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:14,179 >> Initializing global attention on CLS token...\n",
            " 78% 6510/8340 [1:42:43<26:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:15,027 >> Initializing global attention on CLS token...\n",
            " 78% 6511/8340 [1:42:44<26:02,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:15,885 >> Initializing global attention on CLS token...\n",
            " 78% 6512/8340 [1:42:45<26:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:16,732 >> Initializing global attention on CLS token...\n",
            " 78% 6513/8340 [1:42:46<26:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:17,586 >> Initializing global attention on CLS token...\n",
            " 78% 6514/8340 [1:42:47<25:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:18,440 >> Initializing global attention on CLS token...\n",
            " 78% 6515/8340 [1:42:48<26:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:19,297 >> Initializing global attention on CLS token...\n",
            " 78% 6516/8340 [1:42:48<25:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:20,149 >> Initializing global attention on CLS token...\n",
            " 78% 6517/8340 [1:42:49<25:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:20,999 >> Initializing global attention on CLS token...\n",
            " 78% 6518/8340 [1:42:50<25:48,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:21,841 >> Initializing global attention on CLS token...\n",
            " 78% 6519/8340 [1:42:51<25:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:22,698 >> Initializing global attention on CLS token...\n",
            " 78% 6520/8340 [1:42:52<25:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:23,548 >> Initializing global attention on CLS token...\n",
            " 78% 6521/8340 [1:42:53<25:46,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:24,399 >> Initializing global attention on CLS token...\n",
            " 78% 6522/8340 [1:42:53<25:46,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:25,250 >> Initializing global attention on CLS token...\n",
            " 78% 6523/8340 [1:42:54<25:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:26,101 >> Initializing global attention on CLS token...\n",
            " 78% 6524/8340 [1:42:55<25:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:26,955 >> Initializing global attention on CLS token...\n",
            " 78% 6525/8340 [1:42:56<25:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:27,804 >> Initializing global attention on CLS token...\n",
            " 78% 6526/8340 [1:42:57<25:40,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:28,649 >> Initializing global attention on CLS token...\n",
            " 78% 6527/8340 [1:42:58<25:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:29,504 >> Initializing global attention on CLS token...\n",
            " 78% 6528/8340 [1:42:59<25:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:30,360 >> Initializing global attention on CLS token...\n",
            " 78% 6529/8340 [1:42:59<25:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:31,215 >> Initializing global attention on CLS token...\n",
            " 78% 6530/8340 [1:43:00<25:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:32,073 >> Initializing global attention on CLS token...\n",
            " 78% 6531/8340 [1:43:01<25:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:32,929 >> Initializing global attention on CLS token...\n",
            " 78% 6532/8340 [1:43:02<25:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:33,788 >> Initializing global attention on CLS token...\n",
            " 78% 6533/8340 [1:43:03<25:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:34,636 >> Initializing global attention on CLS token...\n",
            " 78% 6534/8340 [1:43:04<25:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:35,490 >> Initializing global attention on CLS token...\n",
            " 78% 6535/8340 [1:43:05<25:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:36,346 >> Initializing global attention on CLS token...\n",
            " 78% 6536/8340 [1:43:05<25:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:37,201 >> Initializing global attention on CLS token...\n",
            " 78% 6537/8340 [1:43:06<25:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:38,055 >> Initializing global attention on CLS token...\n",
            " 78% 6538/8340 [1:43:07<25:40,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:38,915 >> Initializing global attention on CLS token...\n",
            " 78% 6539/8340 [1:43:08<25:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:39,764 >> Initializing global attention on CLS token...\n",
            " 78% 6540/8340 [1:43:09<25:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:40,619 >> Initializing global attention on CLS token...\n",
            " 78% 6541/8340 [1:43:10<25:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:41,472 >> Initializing global attention on CLS token...\n",
            " 78% 6542/8340 [1:43:11<25:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:42,332 >> Initializing global attention on CLS token...\n",
            " 78% 6543/8340 [1:43:11<25:34,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:43,181 >> Initializing global attention on CLS token...\n",
            " 78% 6544/8340 [1:43:12<25:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:44,034 >> Initializing global attention on CLS token...\n",
            " 78% 6545/8340 [1:43:13<25:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:44,885 >> Initializing global attention on CLS token...\n",
            " 78% 6546/8340 [1:43:14<25:32,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:45,742 >> Initializing global attention on CLS token...\n",
            " 79% 6547/8340 [1:43:15<25:31,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:46,596 >> Initializing global attention on CLS token...\n",
            " 79% 6548/8340 [1:43:16<25:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:47,450 >> Initializing global attention on CLS token...\n",
            " 79% 6549/8340 [1:43:17<25:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:48,305 >> Initializing global attention on CLS token...\n",
            " 79% 6550/8340 [1:43:17<25:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:49,163 >> Initializing global attention on CLS token...\n",
            " 79% 6551/8340 [1:43:18<25:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:50,015 >> Initializing global attention on CLS token...\n",
            " 79% 6552/8340 [1:43:19<25:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:50,868 >> Initializing global attention on CLS token...\n",
            " 79% 6553/8340 [1:43:20<25:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:51,722 >> Initializing global attention on CLS token...\n",
            " 79% 6554/8340 [1:43:21<25:25,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:52,577 >> Initializing global attention on CLS token...\n",
            " 79% 6555/8340 [1:43:22<25:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:53,429 >> Initializing global attention on CLS token...\n",
            " 79% 6556/8340 [1:43:23<25:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:54,283 >> Initializing global attention on CLS token...\n",
            " 79% 6557/8340 [1:43:23<25:23,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:55,144 >> Initializing global attention on CLS token...\n",
            " 79% 6558/8340 [1:43:24<25:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:55,993 >> Initializing global attention on CLS token...\n",
            " 79% 6559/8340 [1:43:25<25:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:56,844 >> Initializing global attention on CLS token...\n",
            " 79% 6560/8340 [1:43:26<25:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:57,700 >> Initializing global attention on CLS token...\n",
            " 79% 6561/8340 [1:43:27<25:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:58,555 >> Initializing global attention on CLS token...\n",
            " 79% 6562/8340 [1:43:28<25:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:41:59,410 >> Initializing global attention on CLS token...\n",
            " 79% 6563/8340 [1:43:29<25:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:00,265 >> Initializing global attention on CLS token...\n",
            " 79% 6564/8340 [1:43:29<25:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:01,122 >> Initializing global attention on CLS token...\n",
            " 79% 6565/8340 [1:43:30<25:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:01,970 >> Initializing global attention on CLS token...\n",
            " 79% 6566/8340 [1:43:31<25:14,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:02,823 >> Initializing global attention on CLS token...\n",
            " 79% 6567/8340 [1:43:32<25:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:03,677 >> Initializing global attention on CLS token...\n",
            " 79% 6568/8340 [1:43:33<25:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:04,532 >> Initializing global attention on CLS token...\n",
            " 79% 6569/8340 [1:43:34<25:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:05,389 >> Initializing global attention on CLS token...\n",
            " 79% 6570/8340 [1:43:34<25:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:06,242 >> Initializing global attention on CLS token...\n",
            " 79% 6571/8340 [1:43:35<25:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:07,095 >> Initializing global attention on CLS token...\n",
            " 79% 6572/8340 [1:43:36<25:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:07,953 >> Initializing global attention on CLS token...\n",
            " 79% 6573/8340 [1:43:37<25:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:08,804 >> Initializing global attention on CLS token...\n",
            " 79% 6574/8340 [1:43:38<25:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:09,660 >> Initializing global attention on CLS token...\n",
            " 79% 6575/8340 [1:43:39<25:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:10,524 >> Initializing global attention on CLS token...\n",
            " 79% 6576/8340 [1:43:40<25:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:11,373 >> Initializing global attention on CLS token...\n",
            " 79% 6577/8340 [1:43:40<25:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:12,224 >> Initializing global attention on CLS token...\n",
            " 79% 6578/8340 [1:43:41<25:05,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:13,078 >> Initializing global attention on CLS token...\n",
            " 79% 6579/8340 [1:43:42<25:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:13,932 >> Initializing global attention on CLS token...\n",
            " 79% 6580/8340 [1:43:43<25:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:14,794 >> Initializing global attention on CLS token...\n",
            " 79% 6581/8340 [1:43:44<25:04,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:15,647 >> Initializing global attention on CLS token...\n",
            " 79% 6582/8340 [1:43:45<25:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:16,499 >> Initializing global attention on CLS token...\n",
            " 79% 6583/8340 [1:43:46<25:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:17,356 >> Initializing global attention on CLS token...\n",
            " 79% 6584/8340 [1:43:46<25:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:18,206 >> Initializing global attention on CLS token...\n",
            " 79% 6585/8340 [1:43:47<24:59,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:19,060 >> Initializing global attention on CLS token...\n",
            " 79% 6586/8340 [1:43:48<24:56,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:19,911 >> Initializing global attention on CLS token...\n",
            " 79% 6587/8340 [1:43:49<24:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:20,763 >> Initializing global attention on CLS token...\n",
            " 79% 6588/8340 [1:43:50<24:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:21,618 >> Initializing global attention on CLS token...\n",
            " 79% 6589/8340 [1:43:51<24:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:22,472 >> Initializing global attention on CLS token...\n",
            " 79% 6590/8340 [1:43:52<24:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:23,328 >> Initializing global attention on CLS token...\n",
            " 79% 6591/8340 [1:43:52<24:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:24,183 >> Initializing global attention on CLS token...\n",
            " 79% 6592/8340 [1:43:53<24:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:25,043 >> Initializing global attention on CLS token...\n",
            " 79% 6593/8340 [1:43:54<24:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:25,894 >> Initializing global attention on CLS token...\n",
            " 79% 6594/8340 [1:43:55<24:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:26,747 >> Initializing global attention on CLS token...\n",
            " 79% 6595/8340 [1:43:56<24:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:27,601 >> Initializing global attention on CLS token...\n",
            " 79% 6596/8340 [1:43:57<24:50,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:28,458 >> Initializing global attention on CLS token...\n",
            " 79% 6597/8340 [1:43:58<24:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:29,313 >> Initializing global attention on CLS token...\n",
            " 79% 6598/8340 [1:43:58<24:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:30,166 >> Initializing global attention on CLS token...\n",
            " 79% 6599/8340 [1:43:59<24:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:31,024 >> Initializing global attention on CLS token...\n",
            " 79% 6600/8340 [1:44:00<24:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:31,877 >> Initializing global attention on CLS token...\n",
            " 79% 6601/8340 [1:44:01<24:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:32,731 >> Initializing global attention on CLS token...\n",
            " 79% 6602/8340 [1:44:02<24:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:33,589 >> Initializing global attention on CLS token...\n",
            " 79% 6603/8340 [1:44:03<24:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:34,442 >> Initializing global attention on CLS token...\n",
            " 79% 6604/8340 [1:44:04<24:44,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:35,298 >> Initializing global attention on CLS token...\n",
            " 79% 6605/8340 [1:44:04<24:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:36,153 >> Initializing global attention on CLS token...\n",
            " 79% 6606/8340 [1:44:05<24:43,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:37,010 >> Initializing global attention on CLS token...\n",
            " 79% 6607/8340 [1:44:06<24:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:37,863 >> Initializing global attention on CLS token...\n",
            " 79% 6608/8340 [1:44:07<24:41,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:38,719 >> Initializing global attention on CLS token...\n",
            " 79% 6609/8340 [1:44:08<24:42,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:39,578 >> Initializing global attention on CLS token...\n",
            " 79% 6610/8340 [1:44:09<24:39,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:40,431 >> Initializing global attention on CLS token...\n",
            " 79% 6611/8340 [1:44:10<24:37,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:41,285 >> Initializing global attention on CLS token...\n",
            " 79% 6612/8340 [1:44:10<24:38,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:42,143 >> Initializing global attention on CLS token...\n",
            " 79% 6613/8340 [1:44:11<24:36,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:42,996 >> Initializing global attention on CLS token...\n",
            " 79% 6614/8340 [1:44:12<24:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:43,851 >> Initializing global attention on CLS token...\n",
            " 79% 6615/8340 [1:44:13<24:35,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:44,711 >> Initializing global attention on CLS token...\n",
            " 79% 6616/8340 [1:44:14<24:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:45,560 >> Initializing global attention on CLS token...\n",
            " 79% 6617/8340 [1:44:15<24:33,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:46,417 >> Initializing global attention on CLS token...\n",
            " 79% 6618/8340 [1:44:16<24:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:47,269 >> Initializing global attention on CLS token...\n",
            " 79% 6619/8340 [1:44:16<24:30,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:48,124 >> Initializing global attention on CLS token...\n",
            " 79% 6620/8340 [1:44:17<24:29,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:48,978 >> Initializing global attention on CLS token...\n",
            " 79% 6621/8340 [1:44:18<24:27,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:49,829 >> Initializing global attention on CLS token...\n",
            " 79% 6622/8340 [1:44:19<24:26,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:50,682 >> Initializing global attention on CLS token...\n",
            " 79% 6623/8340 [1:44:20<24:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:51,532 >> Initializing global attention on CLS token...\n",
            " 79% 6624/8340 [1:44:21<24:24,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:52,389 >> Initializing global attention on CLS token...\n",
            " 79% 6625/8340 [1:44:21<24:22,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:53,239 >> Initializing global attention on CLS token...\n",
            " 79% 6626/8340 [1:44:22<24:16,  1.18it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:54,083 >> Initializing global attention on CLS token...\n",
            " 79% 6627/8340 [1:44:23<24:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:54,938 >> Initializing global attention on CLS token...\n",
            " 79% 6628/8340 [1:44:24<24:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:55,793 >> Initializing global attention on CLS token...\n",
            " 79% 6629/8340 [1:44:25<24:20,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:56,648 >> Initializing global attention on CLS token...\n",
            " 79% 6630/8340 [1:44:26<24:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:57,499 >> Initializing global attention on CLS token...\n",
            " 80% 6631/8340 [1:44:27<24:18,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:58,355 >> Initializing global attention on CLS token...\n",
            " 80% 6632/8340 [1:44:27<24:19,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:42:59,213 >> Initializing global attention on CLS token...\n",
            " 80% 6633/8340 [1:44:28<24:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:00,061 >> Initializing global attention on CLS token...\n",
            " 80% 6634/8340 [1:44:29<24:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:00,917 >> Initializing global attention on CLS token...\n",
            " 80% 6635/8340 [1:44:30<24:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:01,772 >> Initializing global attention on CLS token...\n",
            " 80% 6636/8340 [1:44:31<24:16,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:02,630 >> Initializing global attention on CLS token...\n",
            " 80% 6637/8340 [1:44:32<24:15,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:03,483 >> Initializing global attention on CLS token...\n",
            " 80% 6638/8340 [1:44:33<24:13,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:04,336 >> Initializing global attention on CLS token...\n",
            " 80% 6639/8340 [1:44:33<24:12,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:05,190 >> Initializing global attention on CLS token...\n",
            " 80% 6640/8340 [1:44:34<24:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:06,044 >> Initializing global attention on CLS token...\n",
            " 80% 6641/8340 [1:44:35<24:11,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:06,897 >> Initializing global attention on CLS token...\n",
            " 80% 6642/8340 [1:44:36<24:10,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:07,751 >> Initializing global attention on CLS token...\n",
            " 80% 6643/8340 [1:44:37<24:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:08,606 >> Initializing global attention on CLS token...\n",
            " 80% 6644/8340 [1:44:38<24:08,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:09,460 >> Initializing global attention on CLS token...\n",
            " 80% 6645/8340 [1:44:39<24:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:10,313 >> Initializing global attention on CLS token...\n",
            " 80% 6646/8340 [1:44:39<24:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:11,167 >> Initializing global attention on CLS token...\n",
            " 80% 6647/8340 [1:44:40<24:07,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:12,029 >> Initializing global attention on CLS token...\n",
            " 80% 6648/8340 [1:44:41<24:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:12,894 >> Initializing global attention on CLS token...\n",
            " 80% 6649/8340 [1:44:42<24:12,  1.16it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:13,754 >> Initializing global attention on CLS token...\n",
            " 80% 6650/8340 [1:44:43<24:09,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:14,606 >> Initializing global attention on CLS token...\n",
            " 80% 6651/8340 [1:44:44<24:06,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:15,456 >> Initializing global attention on CLS token...\n",
            " 80% 6652/8340 [1:44:45<24:03,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:16,313 >> Initializing global attention on CLS token...\n",
            " 80% 6653/8340 [1:44:45<24:01,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:17,160 >> Initializing global attention on CLS token...\n",
            " 80% 6654/8340 [1:44:46<24:00,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:18,014 >> Initializing global attention on CLS token...\n",
            " 80% 6655/8340 [1:44:47<23:58,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:18,866 >> Initializing global attention on CLS token...\n",
            " 80% 6656/8340 [1:44:48<23:57,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:19,719 >> Initializing global attention on CLS token...\n",
            " 80% 6657/8340 [1:44:49<23:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:20,571 >> Initializing global attention on CLS token...\n",
            " 80% 6658/8340 [1:44:50<23:55,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:21,426 >> Initializing global attention on CLS token...\n",
            " 80% 6659/8340 [1:44:51<23:54,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:22,280 >> Initializing global attention on CLS token...\n",
            " 80% 6660/8340 [1:44:51<23:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:23,132 >> Initializing global attention on CLS token...\n",
            " 80% 6661/8340 [1:44:52<23:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:23,985 >> Initializing global attention on CLS token...\n",
            " 80% 6662/8340 [1:44:53<23:53,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:24,842 >> Initializing global attention on CLS token...\n",
            " 80% 6663/8340 [1:44:54<23:52,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:25,699 >> Initializing global attention on CLS token...\n",
            " 80% 6664/8340 [1:44:55<23:51,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:26,552 >> Initializing global attention on CLS token...\n",
            " 80% 6665/8340 [1:44:56<23:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:27,402 >> Initializing global attention on CLS token...\n",
            " 80% 6666/8340 [1:44:57<23:49,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:28,258 >> Initializing global attention on CLS token...\n",
            " 80% 6667/8340 [1:44:57<23:48,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:29,111 >> Initializing global attention on CLS token...\n",
            " 80% 6668/8340 [1:44:58<23:47,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:29,965 >> Initializing global attention on CLS token...\n",
            " 80% 6669/8340 [1:44:59<23:45,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:30,818 >> Initializing global attention on CLS token...\n",
            " 80% 6670/8340 [1:45:00<23:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:31,675 >> Initializing global attention on CLS token...\n",
            " 80% 6671/8340 [1:45:01<23:46,  1.17it/s][INFO|modeling_longformer.py:1932] 2022-11-21 17:43:32,512 >> Initializing global attention on CLS token...\n",
            " 80% 6672/8340 [1:45:01<19:15,  1.44it/s][INFO|trainer.py:726] 2022-11-21 17:43:32,818 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2907] 2022-11-21 17:43:32,820 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2909] 2022-11-21 17:43:32,820 >>   Num examples = 1400\n",
            "[INFO|trainer.py:2912] 2022-11-21 17:43:32,821 >>   Batch size = 6\n",
            "[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:32,853 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 0/234 [00:00<?, ?it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:33,123 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 2/234 [00:00<00:32,  7.23it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:33,392 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 3/234 [00:00<00:44,  5.23it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:33,658 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 4/234 [00:00<00:50,  4.58it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:33,921 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 5/234 [00:01<00:53,  4.24it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:34,187 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 6/234 [00:01<00:55,  4.09it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:34,455 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 7/234 [00:01<00:57,  3.98it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:34,725 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 8/234 [00:01<00:58,  3.89it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:34,985 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 9/234 [00:02<00:57,  3.88it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:35,249 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 10/234 [00:02<00:58,  3.85it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:35,508 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 11/234 [00:02<00:58,  3.83it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:35,777 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 12/234 [00:02<00:58,  3.81it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:36,040 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 13/234 [00:03<00:58,  3.81it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:36,303 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 14/234 [00:03<00:57,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:36,566 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 15/234 [00:03<00:57,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:36,832 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 16/234 [00:03<00:57,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:37,097 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 17/234 [00:04<00:57,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:37,361 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 18/234 [00:04<00:57,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:37,622 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 19/234 [00:04<00:56,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:37,892 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 20/234 [00:05<00:56,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:38,154 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 21/234 [00:05<00:56,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:38,432 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 22/234 [00:05<00:57,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:38,711 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 23/234 [00:05<00:56,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:38,972 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 24/234 [00:06<00:56,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:39,234 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 25/234 [00:06<00:55,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:39,499 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 26/234 [00:06<00:55,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:39,766 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 27/234 [00:06<00:54,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:40,027 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 28/234 [00:07<00:54,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:40,289 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 29/234 [00:07<00:54,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:40,554 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 30/234 [00:07<00:54,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:40,823 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 31/234 [00:07<00:53,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:41,093 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 32/234 [00:08<00:53,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:41,356 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 33/234 [00:08<00:53,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:41,617 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 34/234 [00:08<00:52,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:41,881 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 35/234 [00:09<00:52,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:42,143 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 36/234 [00:09<00:52,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:42,406 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 37/234 [00:09<00:52,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:42,670 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 38/234 [00:09<00:51,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:42,936 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 39/234 [00:10<00:51,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:43,211 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 40/234 [00:10<00:51,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:43,473 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 41/234 [00:10<00:51,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:43,742 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 42/234 [00:10<00:51,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:44,013 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 43/234 [00:11<00:51,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:44,276 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 44/234 [00:11<00:50,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:44,542 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 45/234 [00:11<00:50,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:44,809 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 46/234 [00:11<00:50,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:45,078 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 47/234 [00:12<00:49,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:45,340 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 48/234 [00:12<00:49,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:45,602 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 49/234 [00:12<00:49,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:45,871 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 50/234 [00:13<00:48,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:46,151 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 51/234 [00:13<00:49,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:46,415 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 52/234 [00:13<00:48,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:46,683 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 53/234 [00:13<00:48,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:46,945 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 54/234 [00:14<00:48,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:47,211 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 55/234 [00:14<00:47,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:47,479 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 56/234 [00:14<00:47,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:47,751 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 57/234 [00:14<00:47,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:48,012 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 58/234 [00:15<00:46,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:48,285 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 59/234 [00:15<00:47,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:48,552 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 60/234 [00:15<00:46,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:48,816 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 61/234 [00:15<00:46,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:49,099 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 62/234 [00:16<00:46,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:49,369 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 63/234 [00:16<00:46,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:49,630 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 64/234 [00:16<00:45,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:49,896 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 65/234 [00:17<00:45,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:50,162 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 66/234 [00:17<00:44,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:50,425 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 67/234 [00:17<00:44,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:50,688 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 68/234 [00:17<00:43,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:50,950 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 69/234 [00:18<00:43,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:51,215 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 70/234 [00:18<00:43,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:51,482 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 71/234 [00:18<00:43,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:51,744 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 72/234 [00:18<00:42,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:52,010 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 73/234 [00:19<00:42,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:52,281 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 74/234 [00:19<00:42,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:52,541 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 75/234 [00:19<00:42,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:52,813 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 76/234 [00:19<00:42,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:53,076 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 77/234 [00:20<00:41,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:53,337 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 78/234 [00:20<00:41,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:53,601 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 79/234 [00:20<00:40,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:53,860 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 80/234 [00:21<00:40,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:54,126 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 81/234 [00:21<00:40,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:54,391 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 82/234 [00:21<00:40,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:54,658 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 83/234 [00:21<00:39,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:54,918 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 84/234 [00:22<00:39,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:55,179 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 85/234 [00:22<00:39,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:55,449 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 86/234 [00:22<00:39,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:55,719 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 87/234 [00:22<00:39,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:55,985 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 88/234 [00:23<00:38,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:56,253 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 89/234 [00:23<00:38,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:56,530 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 90/234 [00:23<00:38,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:56,801 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 91/234 [00:23<00:38,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:57,074 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 92/234 [00:24<00:38,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:57,338 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 93/234 [00:24<00:37,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:57,599 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 94/234 [00:24<00:37,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:57,874 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 95/234 [00:25<00:37,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:58,145 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 96/234 [00:25<00:37,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:58,409 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 97/234 [00:25<00:36,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:58,673 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 98/234 [00:25<00:36,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:58,938 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 99/234 [00:26<00:35,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:59,200 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 100/234 [00:26<00:35,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:59,463 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 101/234 [00:26<00:35,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:59,727 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 102/234 [00:26<00:34,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:43:59,996 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 103/234 [00:27<00:34,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:00,261 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 104/234 [00:27<00:34,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:00,521 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 105/234 [00:27<00:34,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:00,785 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 106/234 [00:27<00:33,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:01,049 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 107/234 [00:28<00:33,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:01,311 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 108/234 [00:28<00:33,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:01,578 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 109/234 [00:28<00:32,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:01,838 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 110/234 [00:28<00:32,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:02,109 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 111/234 [00:29<00:32,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:02,372 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 112/234 [00:29<00:32,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:02,636 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 113/234 [00:29<00:31,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:02,896 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 114/234 [00:30<00:31,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:03,160 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 115/234 [00:30<00:31,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:03,427 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 116/234 [00:30<00:31,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:03,700 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 117/234 [00:30<00:31,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:03,963 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 118/234 [00:31<00:30,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:04,233 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 119/234 [00:31<00:30,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:04,499 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 120/234 [00:31<00:30,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:04,763 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 121/234 [00:31<00:30,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:05,028 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 122/234 [00:32<00:29,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:05,297 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 123/234 [00:32<00:29,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:05,561 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 124/234 [00:32<00:29,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:05,827 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 125/234 [00:32<00:28,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:06,087 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 126/234 [00:33<00:28,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:06,354 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 127/234 [00:33<00:28,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:06,618 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 128/234 [00:33<00:28,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:06,885 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 129/234 [00:34<00:27,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:07,156 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 130/234 [00:34<00:27,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:07,428 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 131/234 [00:34<00:27,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:07,690 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 132/234 [00:34<00:27,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:07,967 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 133/234 [00:35<00:27,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:08,245 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 134/234 [00:35<00:27,  3.69it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:08,503 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 135/234 [00:35<00:26,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:08,768 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 136/234 [00:35<00:26,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:09,035 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 137/234 [00:36<00:25,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:09,299 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 138/234 [00:36<00:25,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:09,564 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 139/234 [00:36<00:25,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:09,823 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 140/234 [00:36<00:24,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:10,087 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 141/234 [00:37<00:24,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:10,351 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 142/234 [00:37<00:24,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:10,616 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 143/234 [00:37<00:24,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:10,884 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 144/234 [00:38<00:23,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:11,151 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 145/234 [00:38<00:23,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:11,414 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 146/234 [00:38<00:23,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:11,675 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 147/234 [00:38<00:22,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:11,936 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 148/234 [00:39<00:22,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:12,198 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 149/234 [00:39<00:22,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:12,468 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 150/234 [00:39<00:22,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:12,729 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 151/234 [00:39<00:21,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:12,993 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 152/234 [00:40<00:21,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:13,253 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 153/234 [00:40<00:21,  3.80it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:13,518 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 154/234 [00:40<00:21,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:13,785 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 155/234 [00:40<00:20,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:14,045 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 156/234 [00:41<00:20,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:14,309 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 157/234 [00:41<00:20,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:14,573 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 158/234 [00:41<00:20,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:14,845 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 159/234 [00:41<00:19,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:15,123 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 160/234 [00:42<00:20,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:15,395 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 161/234 [00:42<00:19,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:15,657 >> Initializing global attention on CLS token...\n",
            "\n",
            " 69% 162/234 [00:42<00:19,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:15,922 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 163/234 [00:43<00:18,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:16,187 >> Initializing global attention on CLS token...\n",
            "\n",
            " 70% 164/234 [00:43<00:18,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:16,449 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 165/234 [00:43<00:18,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:16,713 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 166/234 [00:43<00:18,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:16,988 >> Initializing global attention on CLS token...\n",
            "\n",
            " 71% 167/234 [00:44<00:18,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:17,274 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 168/234 [00:44<00:18,  3.64it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:17,559 >> Initializing global attention on CLS token...\n",
            "\n",
            " 72% 169/234 [00:44<00:18,  3.61it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:17,841 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 170/234 [00:44<00:17,  3.59it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:18,114 >> Initializing global attention on CLS token...\n",
            "\n",
            " 73% 171/234 [00:45<00:17,  3.63it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:18,383 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 172/234 [00:45<00:16,  3.65it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:18,651 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 173/234 [00:45<00:16,  3.66it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:18,922 >> Initializing global attention on CLS token...\n",
            "\n",
            " 74% 174/234 [00:46<00:16,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:19,189 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 175/234 [00:46<00:15,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:19,458 >> Initializing global attention on CLS token...\n",
            "\n",
            " 75% 176/234 [00:46<00:15,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:19,719 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 177/234 [00:46<00:15,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:19,981 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 178/234 [00:47<00:14,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:20,250 >> Initializing global attention on CLS token...\n",
            "\n",
            " 76% 179/234 [00:47<00:14,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:20,512 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 180/234 [00:47<00:14,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:20,777 >> Initializing global attention on CLS token...\n",
            "\n",
            " 77% 181/234 [00:47<00:14,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:21,039 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 182/234 [00:48<00:13,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:21,305 >> Initializing global attention on CLS token...\n",
            "\n",
            " 78% 183/234 [00:48<00:13,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:21,566 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 184/234 [00:48<00:13,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:21,828 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 185/234 [00:48<00:12,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:22,106 >> Initializing global attention on CLS token...\n",
            "\n",
            " 79% 186/234 [00:49<00:12,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:22,379 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 187/234 [00:49<00:12,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:22,640 >> Initializing global attention on CLS token...\n",
            "\n",
            " 80% 188/234 [00:49<00:12,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:22,923 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 189/234 [00:50<00:12,  3.68it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:23,190 >> Initializing global attention on CLS token...\n",
            "\n",
            " 81% 190/234 [00:50<00:11,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:23,450 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 191/234 [00:50<00:11,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:23,719 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 192/234 [00:50<00:11,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:23,985 >> Initializing global attention on CLS token...\n",
            "\n",
            " 82% 193/234 [00:51<00:10,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:24,247 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 194/234 [00:51<00:10,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:24,508 >> Initializing global attention on CLS token...\n",
            "\n",
            " 83% 195/234 [00:51<00:10,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:24,771 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 196/234 [00:51<00:10,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:25,040 >> Initializing global attention on CLS token...\n",
            "\n",
            " 84% 197/234 [00:52<00:09,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:25,304 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 198/234 [00:52<00:09,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:25,565 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 199/234 [00:52<00:09,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:25,838 >> Initializing global attention on CLS token...\n",
            "\n",
            " 85% 200/234 [00:52<00:09,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:26,108 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 201/234 [00:53<00:08,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:26,372 >> Initializing global attention on CLS token...\n",
            "\n",
            " 86% 202/234 [00:53<00:08,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:26,647 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 203/234 [00:53<00:08,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:26,918 >> Initializing global attention on CLS token...\n",
            "\n",
            " 87% 204/234 [00:54<00:08,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:27,183 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 205/234 [00:54<00:07,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:27,452 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 206/234 [00:54<00:07,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:27,724 >> Initializing global attention on CLS token...\n",
            "\n",
            " 88% 207/234 [00:54<00:07,  3.72it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:27,986 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 208/234 [00:55<00:06,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:28,248 >> Initializing global attention on CLS token...\n",
            "\n",
            " 89% 209/234 [00:55<00:06,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:28,513 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 210/234 [00:55<00:06,  3.76it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:28,781 >> Initializing global attention on CLS token...\n",
            "\n",
            " 90% 211/234 [00:55<00:06,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:29,043 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 212/234 [00:56<00:05,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:29,306 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 213/234 [00:56<00:05,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:29,573 >> Initializing global attention on CLS token...\n",
            "\n",
            " 91% 214/234 [00:56<00:05,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:29,838 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 215/234 [00:56<00:05,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:30,099 >> Initializing global attention on CLS token...\n",
            "\n",
            " 92% 216/234 [00:57<00:04,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:30,378 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 217/234 [00:57<00:04,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:30,645 >> Initializing global attention on CLS token...\n",
            "\n",
            " 93% 218/234 [00:57<00:04,  3.73it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:30,913 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 219/234 [00:58<00:04,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:31,173 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 220/234 [00:58<00:03,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:31,437 >> Initializing global attention on CLS token...\n",
            "\n",
            " 94% 221/234 [00:58<00:03,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:31,703 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 222/234 [00:58<00:03,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:31,969 >> Initializing global attention on CLS token...\n",
            "\n",
            " 95% 223/234 [00:59<00:02,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:32,233 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 224/234 [00:59<00:02,  3.77it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:32,495 >> Initializing global attention on CLS token...\n",
            "\n",
            " 96% 225/234 [00:59<00:02,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:32,757 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 226/234 [00:59<00:02,  3.79it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:33,028 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 227/234 [01:00<00:01,  3.78it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:33,310 >> Initializing global attention on CLS token...\n",
            "\n",
            " 97% 228/234 [01:00<00:01,  3.71it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:33,568 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 229/234 [01:00<00:01,  3.74it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:33,834 >> Initializing global attention on CLS token...\n",
            "\n",
            " 98% 230/234 [01:00<00:01,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:34,115 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 231/234 [01:01<00:00,  3.67it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:34,390 >> Initializing global attention on CLS token...\n",
            "\n",
            " 99% 232/234 [01:01<00:00,  3.70it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:34,647 >> Initializing global attention on CLS token...\n",
            "\n",
            "100% 233/234 [01:01<00:00,  3.75it/s]\u001b[A[INFO|modeling_longformer.py:1932] 2022-11-21 17:44:34,894 >> Initializing global attention on CLS token...\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 2.0457046031951904, 'eval_f1-micro': 0.7250000000000001, 'eval_f1-macro': 0.5991966241516234, 'eval_accuracy': 0.725, 'eval_runtime': 64.2023, 'eval_samples_per_second': 21.806, 'eval_steps_per_second': 3.645, 'epoch': 8.0}\n",
            " 80% 6672/8340 [1:46:05<19:15,  1.44it/s]\n",
            "100% 234/234 [01:03<00:00,  4.61it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2656] 2022-11-21 17:44:37,025 >> Saving model checkpoint to logs/output_1/checkpoint-6672\n",
            "[INFO|configuration_utils.py:447] 2022-11-21 17:44:37,026 >> Configuration saved in logs/output_1/checkpoint-6672/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-21 17:44:37,355 >> Model weights saved in logs/output_1/checkpoint-6672/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-21 17:44:37,356 >> tokenizer config file saved in logs/output_1/checkpoint-6672/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-21 17:44:37,356 >> Special tokens file saved in logs/output_1/checkpoint-6672/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-11-21 17:44:40,335 >> tokenizer config file saved in logs/output_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-11-21 17:44:40,336 >> Special tokens file saved in logs/output_1/special_tokens_map.json\n",
            "[INFO|trainer.py:1852] 2022-11-21 17:44:58,055 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1946] 2022-11-21 17:44:58,055 >> Loading best model from logs/output_1/checkpoint-2502 (score: 0.7414285714285715).\n",
            "{'train_runtime': 6387.005, 'train_samples_per_second': 7.828, 'train_steps_per_second': 1.306, 'train_loss': 0.36003239563614914, 'epoch': 8.0}\n",
            " 80% 6672/8340 [1:46:26<26:36,  1.04it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/IR_LDC/model/SCOTUS/scotus_clean.py\", line 257, in <module>\n",
            "  File \"/content/IR_LDC/model/SCOTUS/scotus_clean.py\", line 249, in main\n",
            "    trainer.evaluate(eval_dataset=eval_dataset)\n",
            "NameError: name 'metrics' is not defined\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n",
            "ERROR:huggingface_hub.repository:Waiting for the following commands to finish before shutting down: [[push command, status code: running, in progress. PID: 4345]].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset=tokenized_data['validation']\n",
        "trainer.evaluate(eval_dataset=eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "zn4ZVQaQDY2O",
        "outputId": "153639f3-c9cf-493d-c644-35e44de8a218"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-07fa75689975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_data' is not defined"
          ]
        }
      ]
    }
  ]
}