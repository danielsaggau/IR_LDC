{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/subnetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Hp_z1k_1D2R3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/danielsaggau/IR_LDC.git"
      ],
      "metadata": {
        "id": "YIRs19YBRXhU",
        "outputId": "c6c46d00-49e0-4409-8d6d-b0ccba9609d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IR_LDC'...\n",
            "remote: Enumerating objects: 1664, done.\u001b[K\n",
            "remote: Counting objects: 100% (660/660), done.\u001b[K\n",
            "remote: Compressing objects: 100% (376/376), done.\u001b[K\n",
            "remote: Total 1664 (delta 465), reused 398 (delta 281), pack-reused 1004\u001b[K\n",
            "Receiving objects: 100% (1664/1664), 4.21 MiB | 13.85 MiB/s, done.\n",
            "Resolving deltas: 100% (1033/1033), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd IR_LDC "
      ],
      "metadata": {
        "id": "_71d6SQ_SL-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "9aD0S2eHVpNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "AOGWmGv1i7D4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub.hf_api import HfFolder\n",
        "HfFolder.save_token('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')"
      ],
      "metadata": {
        "id": "4257eC2gjAGW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('danielsaggau/scotus_max_pool',use_auth_token=True, num_labels=14)\n",
        "tokenizer = AutoTokenizer.from_pretrained('danielsaggau/scotus_max_pool', use_auth_token=True,use_fast=True)"
      ],
      "metadata": {
        "id": "rF3pq55TisrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "wdTqslMejCaq",
        "outputId": "2fb03ac4-cb1b-4746-ad64-56ae653ee27e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerForSequenceClassification(\n",
              "  (longformer): LongformerModel(\n",
              "    (embeddings): LongformerEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
              "      (position_embeddings): Embedding(4098, 512, padding_idx=0)\n",
              "      (token_type_embeddings): Embedding(2, 512)\n",
              "      (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): LongformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): LongformerClassificationHead(\n",
              "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=512, out_features=14, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YaVCPb2BDmRm"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 base_model='danielsaggau/scotus_max_pool',\n",
        "                 fc_dim=512,\n",
        "                 k_subs=10, # hyperparameter too; experiment 5 as well\n",
        "                 layer_sizes=[64, 1], # layersize is a hyperparameter too \n",
        "                 ):\n",
        "        super(Model, self).__init__()\n",
        "        self.model = base_model \n",
        "        dim_mlp = self.model.fc.input\n",
        "        self.model.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.model.fc)\n",
        "        \n",
        "        # k subnetworks for bregman\n",
        "        self.subnets = nn.ModuleList()\n",
        "        \n",
        "        for k_idx in range(k_subs):\n",
        "            fc = nn.Sequential()\n",
        "            \n",
        "            for i, (in_size, out_size) in enumerate(zip([fc_dim] + layer_sizes[:-1], layer_sizes)):\n",
        "                if i + 1 < len(layer_sizes):\n",
        "                    fc.add_module(\n",
        "                        name=\"fc_{:d}_{:d}\".format(k_idx, i),\n",
        "                        module=nn.Linear(in_size, out_size))\n",
        "                        \n",
        "                    fc.add_module(\n",
        "                        name=\"relu_{:d}_{:d}\".format(k_idx, i), \n",
        "                        module=nn.ReLU())\n",
        "                    \n",
        "                    fc.add_module(\n",
        "                        name=\"dp_{:d}_{:d}\".format(k_idx, i),\n",
        "                        module=nn.Dropout(p=dr_rate))\n",
        "\n",
        "                else:\n",
        "                    fc.add_module(\n",
        "                        name=\"output_{:d}\".format(k_idx),\n",
        "                        module=nn.Linear(in_size, out_size))\n",
        "                    \n",
        "                    #fc.add_module(\n",
        "                    #    name=\"output_A_{:d}\".format(k_idx),\n",
        "                    #    module=nn.Sigmoid())\n",
        "                \n",
        "            self.subnets.append(fc)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        fc_out = self.model(x)\n",
        "        \n",
        "        out = []\n",
        "        for subnet in self.subnets:\n",
        "            out.append(subnet(fc_out))\n",
        "        \n",
        "        out = torch.cat(out, -1)\n",
        "        #F.normalize(feature, dim=-1)\n",
        "        return fc_out, out"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxfxzXqMU8T6",
        "outputId": "126d7d70-dfd3-4888-d88f-9d8bf234995a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Model"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}