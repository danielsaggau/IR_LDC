{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+YmBufkcMSRHIi238dFPr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/MSMARCO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRW_ETaiHhfu"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import gzip\n",
        "import os\n",
        "import tarfile\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import IterableDataset\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import pickle\n",
        "import argparse"
      ],
      "metadata": {
        "id": "oVcirTQNHzrt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "#### /print debug information to stdout\n"
      ],
      "metadata": {
        "id": "4CGZW1ECIK2J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'distilbert-base-uncased'"
      ],
      "metadata": {
        "id": "pGj7dELTIGbG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 128         #Increasing the train batch size improves the model performance, but requires more GPU memory\n",
        "max_seq_length = 75          #Max length for passages. Increasing it, requires more GPU memory\n",
        "ce_score_margin = 1           #Margin for the CrossEncoder score between negative and positive passages\n",
        "num_negs_per_system = 2\n",
        "num_epochs = 1\n",
        "\n",
        "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),pooling_mode='mean')\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "model_save_path = 'output/train_bi-encoder-mnrl-{}-margin_{:.1f}-{}'.format(model_name.replace(\"/\", \"-\"), ce_score_margin, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "\n",
        "\n",
        "### Now we read the MS Marco dataset\n",
        "data_folder = 'msmarco-data'\n",
        "\n",
        "#### Read the corpus files, that contain all the passages. Store them in the corpus dict\n",
        "corpus = {}         #dict in the format: passage_id -> passage. Stores all existent passages\n",
        "collection_filepath = os.path.join(data_folder, 'collection.tsv')\n",
        "if not os.path.exists(collection_filepath):\n",
        "    tar_filepath = os.path.join(data_folder, 'collection.tar.gz')\n",
        "    if not os.path.exists(tar_filepath):\n",
        "        logging.info(\"Download collection.tar.gz\")\n",
        "        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz', tar_filepath)\n",
        "\n",
        "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
        "        tar.extractall(path=data_folder)\n",
        "\n",
        "logging.info(\"Read corpus: collection.tsv\")\n",
        "with open(collection_filepath, 'r', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        pid, passage = line.strip().split(\"\\t\")\n",
        "        pid = int(pid)\n",
        "        corpus[pid] = passage\n",
        "\n",
        "\n",
        "### Read the train queries, store in queries dict\n",
        "queries = {}        #dict in the format: query_id -> query. Stores all training queries\n",
        "queries_filepath = os.path.join(data_folder, 'queries.train.tsv')\n",
        "if not os.path.exists(queries_filepath):\n",
        "    tar_filepath = os.path.join(data_folder, 'queries.tar.gz')\n",
        "    if not os.path.exists(tar_filepath):\n",
        "        logging.info(\"Download queries.tar.gz\")\n",
        "        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz', tar_filepath)\n",
        "\n",
        "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
        "        tar.extractall(path=data_folder)\n",
        "\n",
        "\n",
        "with open(queries_filepath, 'r', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        qid, query = line.strip().split(\"\\t\")\n",
        "        qid = int(qid)\n",
        "        queries[qid] = query\n",
        "\n"
      ],
      "metadata": {
        "id": "GZxg7DjBIHmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c475e253-3473-449f-a175-e3fcffbb1600"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a dict (qid, pid) -> ce_score that maps query-ids (qid) and paragraph-ids (pid)\n",
        "# to the CrossEncoder score computed by the cross-encoder/ms-marco-MiniLM-L-6-v2 model\n",
        "ce_scores_file = os.path.join(data_folder, 'cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz')\n",
        "if not os.path.exists(ce_scores_file):\n",
        "    logging.info(\"Download cross-encoder scores file\")\n",
        "    util.http_get('https://huggingface.co/datasets/sentence-transformers/msmarco-hard-negatives/resolve/main/cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz', ce_scores_file)\n",
        "\n",
        "logging.info(\"Load CrossEncoder scores dict\")\n",
        "with gzip.open(ce_scores_file, 'rb') as fIn:\n",
        "    ce_scores = pickle.load(fIn)\n",
        "\n",
        "# As training data we use hard-negatives that have been mined using various systems\n",
        "hard_negatives_filepath = os.path.join(data_folder, 'msmarco-hard-negatives.jsonl.gz')\n",
        "if not os.path.exists(hard_negatives_filepath):\n",
        "    logging.info(\"Download cross-encoder scores file\")\n",
        "    util.http_get('https://huggingface.co/datasets/sentence-transformers/msmarco-hard-negatives/resolve/main/msmarco-hard-negatives.jsonl.gz', hard_negatives_filepath)\n",
        "\n",
        "\n",
        "logging.info(\"Read hard negatives train file\")\n",
        "train_queries = {}\n",
        "negs_to_use = None\n",
        "with gzip.open(hard_negatives_filepath, 'rt') as fIn:\n",
        "    for line in tqdm.tqdm(fIn):\n",
        "        data = json.loads(line)\n",
        "\n",
        "        #Get the positive passage ids\n",
        "        qid = data['qid']\n",
        "        pos_pids = data['pos']\n",
        "\n",
        "        if len(pos_pids) == 0:  #Skip entries without positives passages\n",
        "            continue\n",
        "\n",
        "        pos_min_ce_score = min([ce_scores[qid][pid] for pid in data['pos']])\n",
        "        ce_score_threshold = pos_min_ce_score - ce_score_margin\n",
        "\n",
        "        #Get the hard negatives\n",
        "        neg_pids = set()\n",
        "        if negs_to_use is None:\n",
        "            if args.negs_to_use is not None:    #Use specific system for negatives\n",
        "                negs_to_use = args.negs_to_use.split(\",\")\n",
        "            else:   #Use all systems\n",
        "                negs_to_use = list(data['neg'].keys())\n",
        "            logging.info(\"Using negatives from the following systems: {}\".format(\", \".join(negs_to_use)))\n",
        "\n",
        "        for system_name in negs_to_use:\n",
        "            if system_name not in data['neg']:\n",
        "                continue\n",
        "\n",
        "            system_negs = data['neg'][system_name]\n",
        "            negs_added = 0\n",
        "            for pid in system_negs:\n",
        "                if ce_scores[qid][pid] > ce_score_threshold:\n",
        "                    continue\n",
        "\n",
        "                if pid not in neg_pids:\n",
        "                    neg_pids.add(pid)\n",
        "                    negs_added += 1\n",
        "                    if negs_added >= num_negs_per_system:\n",
        "                        break\n",
        "\n",
        "        if args.use_all_queries or (len(pos_pids) > 0 and len(neg_pids) > 0):\n",
        "            train_queries[data['qid']] = {'qid': data['qid'], 'query': queries[data['qid']], 'pos': pos_pids, 'neg': neg_pids}\n",
        "\n",
        "del ce_scores\n",
        "\n",
        "logging.info(\"Train queries: {}\".format(len(train_queries)))\n"
      ],
      "metadata": {
        "id": "laVp_BweSz-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a custom MSMARCO dataset that returns triplets (query, positive, negative)\n",
        "# on-the-fly based on the information from the mined-hard-negatives jsonl file.\n",
        "class MSMARCODataset(Dataset):\n",
        "    def __init__(self, queries, corpus):\n",
        "        self.queries = queries\n",
        "        self.queries_ids = list(queries.keys())\n",
        "        self.corpus = corpus\n",
        "\n",
        "        for qid in self.queries:\n",
        "            self.queries[qid]['pos'] = list(self.queries[qid]['pos'])\n",
        "            self.queries[qid]['neg'] = list(self.queries[qid]['neg'])\n",
        "            random.shuffle(self.queries[qid]['neg'])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        query = self.queries[self.queries_ids[item]]\n",
        "        query_text = query['query']\n",
        "\n",
        "        pos_id = query['pos'].pop(0)    #Pop positive and add at end\n",
        "        pos_text = self.corpus[pos_id]\n",
        "        query['pos'].append(pos_id)\n",
        "\n",
        "        neg_id = query['neg'].pop(0)    #Pop negative and add at end\n",
        "        neg_text = self.corpus[neg_id]\n",
        "        query['neg'].append(neg_id)\n",
        "\n",
        "        return InputExample(texts=[query_text, pos_text, neg_text])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "# For training the SentenceTransformer model, we need a dataset, a dataloader, and a loss used for training.\n",
        "train_dataset = MSMARCODataset(train_queries, corpus=corpus)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          epochs=num_epochs,\n",
        "          warmup_steps=args.warmup_steps,\n",
        "          use_amp=True,\n",
        "          checkpoint_path=model_save_path,\n",
        "          checkpoint_save_steps=len(train_dataloader),\n",
        "          optimizer_params = {'lr': args.lr},\n",
        "          )\n",
        "\n",
        "# Save the model\n",
        "model.save(model_save_path)"
      ],
      "metadata": {
        "id": "dzILzd0hTLbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}