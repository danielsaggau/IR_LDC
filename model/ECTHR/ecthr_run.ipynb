{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzBkoD0CFWVTXpRlhtKtLC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/ECTHR/ecthr_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "naqIWr56jSUf",
        "outputId": "faab37ca-bfb0-4150-cb95-249b992cd818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IR_LDC'...\n",
            "remote: Enumerating objects: 1970, done.\u001b[K\n",
            "remote: Counting objects: 100% (277/277), done.\u001b[K\n",
            "remote: Compressing objects: 100% (164/164), done.\u001b[K\n",
            "remote: Total 1970 (delta 181), reused 169 (delta 112), pack-reused 1693\u001b[K\n",
            "Receiving objects: 100% (1970/1970), 4.26 MiB | 10.49 MiB/s, done.\n",
            "Resolving deltas: 100% (1254/1254), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/danielsaggau/IR_LDC.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd IR_LDC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rw6QC9ojY5t",
        "outputId": "c200551b-d56f-4850-acea-864e16d98d9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IR_LDC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ipRGPbTR45u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login fd6f7deb3126d40be9abf77ee753bf45f00e2a9a\n",
        "%env WANDB_PROJECT=IR_LDC"
      ],
      "metadata": {
        "id": "XOCuFbIKgDss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IR_LDC/model/ECTHR/bregman_ecthr.py \\\n",
        "    --output_dir logs/output_1 \\\n",
        "    --model_name 'danielsaggau/bregman_ecthrb_k_10_ep1' \\\n",
        "    --load_best_model_at_end \\\n",
        "    --model_type 'max' \\\n",
        "    --overwrite_output_dir \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --per_device_eval_batch_size 6 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --push_to_hub \\\n",
        "    #--fp16 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --metric_for_best_model \"f1-micro\" \\\n",
        "    --greater_is_better 1 \\\n",
        "    --report_to 'wandb' \\\n",
        "    --model_type 'ecthr_breg_test'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQYL6fodppBA",
        "outputId": "5e1ee3d8-5703-411f-f3bd-7c8081c23896"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "Moving 0 files to the new cache system\n",
            "0it [00:00, ?it/s]\n",
            "WARNING:__main__:Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=logs/output_1/runs/Dec06_16-08-46_681f64a07a69,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=logs/output_1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=6,\n",
            "per_device_train_batch_size=6,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=logs/output_1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Reusing dataset lex_glue (/root/.cache/huggingface/datasets/lex_glue/ecthr_b/1.0.0/c3c0bd7433b636dc39ae49a84dc401190c73156617efc415b04e9835a93a7043)\n",
            "100% 3/3 [00:00<00:00, 745.88it/s]\n",
            "INFO:__main__:load ecthr_b regular model\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 16:08:47,338 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 16:08:47,338 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 16:08:47,338 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 16:08:47,339 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-12-06 16:08:47,339 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/tokenizer_config.json\n",
            "Downloading: 100% 1.01k/1.01k [00:00<00:00, 722kB/s]\n",
            "[INFO|configuration_utils.py:653] 2022-12-06 16:08:48,119 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/config.json\n",
            "[INFO|configuration_utils.py:705] 2022-12-06 16:08:48,123 >> Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"danielsaggau/bregman_ecthrb_k_10_ep1\",\n",
            "  \"architectures\": [\n",
            "    \"LongformerModel\"\n",
            "  ],\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"cls_token_id\": 101,\n",
            "  \"eos_token_id\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\"\n",
            "  },\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_max_length\": 4096,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"onnx_export\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"multi_label_classification\",\n",
            "  \"sep_token_id\": 102,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Downloading: 100% 167M/167M [00:04<00:00, 36.7MB/s]\n",
            "[INFO|modeling_utils.py:2156] 2022-12-06 16:08:53,674 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--danielsaggau--bregman_ecthrb_k_10_ep1/snapshots/2d86b1ba461a6a0c5579da67bdac2786ec23ce80/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2606] 2022-12-06 16:08:54,325 >> All model checkpoint weights were used when initializing LongformerForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:2608] 2022-12-06 16:08:54,325 >> Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at danielsaggau/bregman_ecthrb_k_10_ep1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f51d5b09b80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Tokenizing the entire dataset: 100% 9/9 [00:44<00:00,  4.94s/ba]\n",
            "Tokenizing the entire dataset: 100% 1/1 [00:05<00:00,  5.17s/ba]\n",
            "Tokenizing the entire dataset: 100% 1/1 [00:04<00:00,  4.91s/ba]\n",
            "100% 9000/9000 [01:22<00:00, 108.55ex/s]\n",
            "100% 1000/1000 [00:09<00:00, 107.32ex/s]\n",
            "100% 1000/1000 [00:09<00:00, 106.87ex/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Cloning https://huggingface.co/danielsaggau/output_1 into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/danielsaggau/output_1 into local empty directory.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Download file pytorch_model.bin:   0% 14.0k/160M [00:01<3:37:40, 12.8kB/s]\n",
            "Download file training_args.bin:  11% 387/3.30k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Download file pytorch_model.bin:  98% 156M/160M [00:47<00:01, 2.99MB/s]\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:   0% 1.00k/160M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:   1% 1.31M/160M [00:01<02:00, 1.37MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:   6% 10.2M/160M [00:02<00:26, 6.01MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  12% 19.3M/160M [00:03<00:19, 7.66MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  18% 28.4M/160M [00:04<00:16, 8.37MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  23% 37.5M/160M [00:05<00:14, 8.78MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  29% 46.4M/160M [00:06<00:13, 8.99MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  35% 55.5M/160M [00:07<00:11, 9.16MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  40% 64.5M/160M [00:08<00:10, 9.24MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  46% 73.5M/160M [00:09<00:09, 9.30MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  52% 82.5M/160M [00:10<00:08, 9.33MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  57% 91.7M/160M [00:11<00:07, 9.41MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  63% 101M/160M [00:12<00:06, 9.51MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  69% 110M/160M [00:13<00:05, 9.53MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  74% 118M/160M [00:14<00:04, 9.04MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  77% 124M/160M [00:15<00:04, 8.25MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin:  83% 133M/160M [00:16<00:03, 8.67MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Download file pytorch_model.bin: 100% 160M/160M [01:06<00:00, 2.54MB/s]\n",
            "\n",
            "Download file training_args.bin: 100% 3.30k/3.30k [01:05<00:00, 45.9B/s]\u001b[A\n",
            "Download file training_args.bin: 100% 3.30k/3.30k [01:05<00:00, 45.9B/s]\n",
            "\n",
            "\n",
            "Clean file training_args.bin: 100% 3.30k/3.30k [01:05<00:00, 36.1B/s]\u001b[A\u001b[A\n",
            "\n",
            "Clean file training_args.bin: 100% 3.30k/3.30k [01:05<00:00, 36.1B/s]\n",
            "\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin: 100% 160M/160M [00:18<00:00, 11.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Clean file pytorch_model.bin: 100% 160M/160M [00:18<00:00, 9.30MB/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielsaggau\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/IR_LDC/wandb/run-20221206_161245-1538fbv2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mECTHR_frozen\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/danielsaggau/IR_LDC\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/danielsaggau/IR_LDC/runs/1538fbv2\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1607] 2022-12-06 16:12:45,969 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2022-12-06 16:12:45,969 >>   Num examples = 9000\n",
            "[INFO|trainer.py:1609] 2022-12-06 16:12:45,970 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1610] 2022-12-06 16:12:45,970 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:1611] 2022-12-06 16:12:45,970 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "[INFO|trainer.py:1612] 2022-12-06 16:12:45,970 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2022-12-06 16:12:45,970 >>   Total optimization steps = 4500\n",
            "[INFO|integrations.py:680] 2022-12-06 16:12:45,989 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "  0% 0/4500 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-12-06 16:12:46,013 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/IR_LDC/model/ECTHR/bregman_ecthr.py\", line 214, in <module>\n",
            "    main()\n",
            "  File \"/content/IR_LDC/model/ECTHR/bregman_ecthr.py\", line 205, in main\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1500, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1742, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2504, in training_step\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 396, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 173, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/wandb/wandb_torch.py\", line 282, in <lambda>\n",
            "    handle = var.register_hook(lambda grad: _callback(grad, log_track))\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mECTHR_frozen\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/danielsaggau/IR_LDC/runs/1538fbv2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221206_161245-1538fbv2/logs\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset=tokenized_data['validation']\n",
        "trainer.evaluate(eval_dataset=eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "zn4ZVQaQDY2O",
        "outputId": "153639f3-c9cf-493d-c644-35e44de8a218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-07fa75689975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_data' is not defined"
          ]
        }
      ]
    }
  ]
}