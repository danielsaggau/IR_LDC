{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/MIMIC/mimic_script_frozen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpOzUfD53lts"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers transformers datasets wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ7IqoZXAVjg",
        "outputId": "91abbe0e-9423-469f-9c5e-b21f0d9e31be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "br19zMT6AoKn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/danielsaggau/IR_LDC.git"
      ],
      "metadata": {
        "id": "rUv6Xu7wSD_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/mimic.jsonl.zip -d content\n",
        "#with open('content/mimic.jsonl') as f:\n",
        "#    data = [json.loads(line) for line in f]"
      ],
      "metadata": {
        "id": "JQcrNr7mnbcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy(\"/content/drive/MyDrive/mimic.jsonl/mimic.jsonl\", \"/content/IR_LDC/model/MIMIC\")\n",
        "dataset = load_dataset(\"/content/IR_LDC/model/MIMIC/mimic-dataset.py\")"
      ],
      "metadata": {
        "id": "swGB8CPVAzg0",
        "outputId": "2d49ab0f-9c5b-4eed-a922-ecf19676585d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "19afdabb22be4c5482154318632e62ef"
          ]
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset mimic-dataset downloaded and prepared to /root/.cache/huggingface/datasets/mimic-dataset/mimic/1.1.0/90aef5b28cbcdff1c522e7dd5b743336977fe255361a97e4c5035fbd7da6e512. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19afdabb22be4c5482154318632e62ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/mimic.jsonl.zip -d content\n",
        "!python /content/IR_LDC/model/Longformer/Longformer/biobert_to_longformer.py\n",
        "!wandb login fd6f7deb3126d40be9abf77ee753bf45f00e2a9a"
      ],
      "metadata": {
        "id": "nVjkpKI9CgsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IR_LDC/model/MIMIC/mimic_script_classification.py \\\n",
        "    --output_dir logs/output_1 \\\n",
        "    --model_name '/content/models/bio-longformer'\\\n",
        "    --overwrite_output_dir \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --fp16 \\\n",
        "    --freezing \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --per_device_eval_batch_size 6 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --save_strategy \"epoch\" \\\n",
        "    --evaluation_strategy \"epoch\" \\\n",
        "    --push_to_hub \\\n",
        "    --metric_for_best_model \"micro_f1\" \\\n",
        "    --greater_is_better 1 \\\n",
        "    --load_best_model_at_end \\\n",
        "    --report_to 'wandb' \\\n",
        "    --run_name \"mimic_bregman_biobert\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuwDnqLJIA3W",
        "outputId": "5bd0e19b-4886-4e00-f012-757864350e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mDie letzten 5000Â Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n",
            " 17% 17268/100000 [47:42<2:59:07,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:09,849 >> Initializing global attention on CLS token...\n",
            " 17% 17269/100000 [47:43<2:59:08,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:09,981 >> Initializing global attention on CLS token...\n",
            " 17% 17270/100000 [47:43<3:00:11,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:10,113 >> Initializing global attention on CLS token...\n",
            " 17% 17271/100000 [47:43<3:00:03,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:10,239 >> Initializing global attention on CLS token...\n",
            " 17% 17272/100000 [47:43<3:00:03,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:10,376 >> Initializing global attention on CLS token...\n",
            " 17% 17273/100000 [47:43<2:59:55,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:10,500 >> Initializing global attention on CLS token...\n",
            " 17% 17274/100000 [47:43<2:56:05,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:10,621 >> Initializing global attention on CLS token...\n",
            " 17% 17275/100000 [47:43<2:56:32,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:10,757 >> Initializing global attention on CLS token...\n",
            " 17% 17276/100000 [47:43<2:57:39,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:10,881 >> Initializing global attention on CLS token...\n",
            " 17% 17277/100000 [47:44<2:56:49,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:11,012 >> Initializing global attention on CLS token...\n",
            " 17% 17278/100000 [47:44<2:57:29,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:11,138 >> Initializing global attention on CLS token...\n",
            " 17% 17279/100000 [47:44<2:54:55,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:11,266 >> Initializing global attention on CLS token...\n",
            " 17% 17280/100000 [47:44<3:00:17,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:11,400 >> Initializing global attention on CLS token...\n",
            " 17% 17281/100000 [47:44<2:57:42,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:11,529 >> Initializing global attention on CLS token...\n",
            " 17% 17282/100000 [47:44<3:01:14,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:11,663 >> Initializing global attention on CLS token...\n",
            " 17% 17283/100000 [47:44<2:57:32,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:11,785 >> Initializing global attention on CLS token...\n",
            " 17% 17284/100000 [47:44<2:56:46,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:11,916 >> Initializing global attention on CLS token...\n",
            " 17% 17285/100000 [47:45<2:57:46,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:12,042 >> Initializing global attention on CLS token...\n",
            " 17% 17286/100000 [47:45<2:57:10,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:12,175 >> Initializing global attention on CLS token...\n",
            " 17% 17287/100000 [47:45<2:55:49,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:12,295 >> Initializing global attention on CLS token...\n",
            " 17% 17288/100000 [47:45<2:58:16,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:12,434 >> Initializing global attention on CLS token...\n",
            " 17% 17289/100000 [47:45<2:58:42,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:12,559 >> Initializing global attention on CLS token...\n",
            " 17% 17290/100000 [47:45<2:59:14,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:12,690 >> Initializing global attention on CLS token...\n",
            " 17% 17291/100000 [47:45<2:57:14,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:12,819 >> Initializing global attention on CLS token...\n",
            " 17% 17292/100000 [47:46<2:57:53,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:12,945 >> Initializing global attention on CLS token...\n",
            " 17% 17293/100000 [47:46<2:54:56,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:13,067 >> Initializing global attention on CLS token...\n",
            " 17% 17294/100000 [47:46<2:57:02,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:13,200 >> Initializing global attention on CLS token...\n",
            " 17% 17295/100000 [47:46<2:54:13,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:13,321 >> Initializing global attention on CLS token...\n",
            " 17% 17296/100000 [47:46<2:53:10,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:13,444 >> Initializing global attention on CLS token...\n",
            " 17% 17297/100000 [47:46<2:53:26,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:13,575 >> Initializing global attention on CLS token...\n",
            " 17% 17298/100000 [47:46<2:55:35,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:13,702 >> Initializing global attention on CLS token...\n",
            " 17% 17299/100000 [47:46<2:54:44,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:13,831 >> Initializing global attention on CLS token...\n",
            " 17% 17300/100000 [47:47<2:55:52,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:13,957 >> Initializing global attention on CLS token...\n",
            " 17% 17301/100000 [47:47<2:53:14,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:14,078 >> Initializing global attention on CLS token...\n",
            " 17% 17302/100000 [47:47<2:56:03,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:14,214 >> Initializing global attention on CLS token...\n",
            " 17% 17303/100000 [47:47<2:54:35,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:14,335 >> Initializing global attention on CLS token...\n",
            " 17% 17304/100000 [47:47<2:53:06,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:14,458 >> Initializing global attention on CLS token...\n",
            " 17% 17305/100000 [47:47<2:54:20,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:14,591 >> Initializing global attention on CLS token...\n",
            " 17% 17306/100000 [47:47<2:54:58,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:14,715 >> Initializing global attention on CLS token...\n",
            " 17% 17307/100000 [47:47<2:55:18,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:14,855 >> Initializing global attention on CLS token...\n",
            " 17% 17308/100000 [47:48<3:02:35,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:14,988 >> Initializing global attention on CLS token...\n",
            " 17% 17309/100000 [47:48<2:59:33,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:15,117 >> Initializing global attention on CLS token...\n",
            " 17% 17310/100000 [47:48<3:02:18,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:15,250 >> Initializing global attention on CLS token...\n",
            " 17% 17311/100000 [47:48<2:58:55,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:15,373 >> Initializing global attention on CLS token...\n",
            " 17% 17312/100000 [47:48<3:00:03,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:15,511 >> Initializing global attention on CLS token...\n",
            " 17% 17313/100000 [47:48<3:01:55,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:15,646 >> Initializing global attention on CLS token...\n",
            " 17% 17314/100000 [47:48<3:02:27,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:15,775 >> Initializing global attention on CLS token...\n",
            " 17% 17315/100000 [47:48<2:58:08,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:15,897 >> Initializing global attention on CLS token...\n",
            " 17% 17316/100000 [47:49<2:57:11,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:16,023 >> Initializing global attention on CLS token...\n",
            " 17% 17317/100000 [47:49<2:54:14,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:16,145 >> Initializing global attention on CLS token...\n",
            " 17% 17318/100000 [47:49<2:52:43,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:16,267 >> Initializing global attention on CLS token...\n",
            " 17% 17319/100000 [47:49<2:53:46,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:16,401 >> Initializing global attention on CLS token...\n",
            " 17% 17320/100000 [47:49<2:55:51,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:16,526 >> Initializing global attention on CLS token...\n",
            " 17% 17321/100000 [47:49<2:54:48,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:16,655 >> Initializing global attention on CLS token...\n",
            " 17% 17322/100000 [47:49<2:55:22,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:16,779 >> Initializing global attention on CLS token...\n",
            " 17% 17323/100000 [47:49<2:53:01,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:16,901 >> Initializing global attention on CLS token...\n",
            " 17% 17324/100000 [47:50<2:53:26,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:17,028 >> Initializing global attention on CLS token...\n",
            " 17% 17325/100000 [47:50<2:51:18,  8.04it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:17,149 >> Initializing global attention on CLS token...\n",
            " 17% 17326/100000 [47:50<2:51:02,  8.06it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:17,272 >> Initializing global attention on CLS token...\n",
            " 17% 17327/100000 [47:50<2:52:15,  8.00it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:17,404 >> Initializing global attention on CLS token...\n",
            " 17% 17328/100000 [47:50<2:55:24,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:17,532 >> Initializing global attention on CLS token...\n",
            " 17% 17329/100000 [47:50<2:54:46,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:17,662 >> Initializing global attention on CLS token...\n",
            " 17% 17330/100000 [47:50<2:55:41,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:17,787 >> Initializing global attention on CLS token...\n",
            " 17% 17331/100000 [47:50<2:53:33,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:17,909 >> Initializing global attention on CLS token...\n",
            " 17% 17332/100000 [47:51<2:53:03,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:18,034 >> Initializing global attention on CLS token...\n",
            " 17% 17333/100000 [47:51<2:51:38,  8.03it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:18,156 >> Initializing global attention on CLS token...\n",
            " 17% 17334/100000 [47:51<2:50:12,  8.09it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:18,277 >> Initializing global attention on CLS token...\n",
            " 17% 17335/100000 [47:51<2:51:10,  8.05it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:18,408 >> Initializing global attention on CLS token...\n",
            " 17% 17336/100000 [47:51<2:54:45,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:18,541 >> Initializing global attention on CLS token...\n",
            " 17% 17337/100000 [47:51<2:56:40,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:18,672 >> Initializing global attention on CLS token...\n",
            " 17% 17338/100000 [47:51<2:56:45,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:18,796 >> Initializing global attention on CLS token...\n",
            " 17% 17339/100000 [47:51<2:54:30,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:18,919 >> Initializing global attention on CLS token...\n",
            " 17% 17340/100000 [47:52<2:54:11,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:19,045 >> Initializing global attention on CLS token...\n",
            " 17% 17341/100000 [47:52<2:51:52,  8.02it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:19,166 >> Initializing global attention on CLS token...\n",
            " 17% 17342/100000 [47:52<2:51:30,  8.03it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:19,290 >> Initializing global attention on CLS token...\n",
            " 17% 17343/100000 [47:52<2:55:02,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:19,427 >> Initializing global attention on CLS token...\n",
            " 17% 17344/100000 [47:52<2:56:37,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:19,558 >> Initializing global attention on CLS token...\n",
            " 17% 17345/100000 [47:52<2:58:21,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:19,691 >> Initializing global attention on CLS token...\n",
            " 17% 17346/100000 [47:52<2:58:42,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:19,817 >> Initializing global attention on CLS token...\n",
            " 17% 17347/100000 [47:53<2:55:42,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:19,943 >> Initializing global attention on CLS token...\n",
            " 17% 17348/100000 [47:53<2:56:16,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:20,069 >> Initializing global attention on CLS token...\n",
            " 17% 17349/100000 [47:53<2:53:39,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:20,189 >> Initializing global attention on CLS token...\n",
            " 17% 17350/100000 [47:53<2:56:03,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:20,326 >> Initializing global attention on CLS token...\n",
            " 17% 17351/100000 [47:53<2:56:18,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:20,450 >> Initializing global attention on CLS token...\n",
            " 17% 17352/100000 [47:53<2:55:33,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:20,583 >> Initializing global attention on CLS token...\n",
            " 17% 17353/100000 [47:53<3:01:46,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:20,718 >> Initializing global attention on CLS token...\n",
            " 17% 17354/100000 [47:53<2:56:45,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:20,841 >> Initializing global attention on CLS token...\n",
            " 17% 17355/100000 [47:54<3:00:05,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:20,975 >> Initializing global attention on CLS token...\n",
            " 17% 17356/100000 [47:54<2:55:53,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:21,095 >> Initializing global attention on CLS token...\n",
            " 17% 17357/100000 [47:54<2:58:17,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:21,232 >> Initializing global attention on CLS token...\n",
            " 17% 17358/100000 [47:54<2:56:56,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:21,355 >> Initializing global attention on CLS token...\n",
            " 17% 17359/100000 [47:54<2:56:21,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:21,488 >> Initializing global attention on CLS token...\n",
            " 17% 17360/100000 [47:54<2:57:47,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:21,613 >> Initializing global attention on CLS token...\n",
            " 17% 17361/100000 [47:54<2:56:17,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:21,739 >> Initializing global attention on CLS token...\n",
            " 17% 17362/100000 [47:54<2:56:43,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:21,868 >> Initializing global attention on CLS token...\n",
            " 17% 17363/100000 [47:55<2:54:21,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:21,991 >> Initializing global attention on CLS token...\n",
            " 17% 17364/100000 [47:55<2:53:17,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:22,114 >> Initializing global attention on CLS token...\n",
            " 17% 17365/100000 [47:55<2:55:14,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:22,246 >> Initializing global attention on CLS token...\n",
            " 17% 17366/100000 [47:55<2:53:10,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:22,367 >> Initializing global attention on CLS token...\n",
            " 17% 17367/100000 [47:55<2:54:23,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:22,501 >> Initializing global attention on CLS token...\n",
            " 17% 17368/100000 [47:55<2:55:56,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:22,627 >> Initializing global attention on CLS token...\n",
            " 17% 17369/100000 [47:55<2:55:21,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:22,760 >> Initializing global attention on CLS token...\n",
            " 17% 17370/100000 [47:55<2:58:28,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:22,888 >> Initializing global attention on CLS token...\n",
            " 17% 17371/100000 [47:56<2:57:17,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:23,019 >> Initializing global attention on CLS token...\n",
            " 17% 17372/100000 [47:56<2:57:13,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:23,143 >> Initializing global attention on CLS token...\n",
            " 17% 17373/100000 [47:56<2:55:41,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:23,272 >> Initializing global attention on CLS token...\n",
            " 17% 17374/100000 [47:56<2:58:32,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:23,402 >> Initializing global attention on CLS token...\n",
            " 17% 17375/100000 [47:56<2:55:03,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:23,524 >> Initializing global attention on CLS token...\n",
            " 17% 17376/100000 [47:56<2:55:54,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:23,658 >> Initializing global attention on CLS token...\n",
            " 17% 17377/100000 [47:56<2:58:09,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:23,786 >> Initializing global attention on CLS token...\n",
            " 17% 17378/100000 [47:56<2:57:04,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:23,917 >> Initializing global attention on CLS token...\n",
            " 17% 17379/100000 [47:57<2:58:51,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:24,046 >> Initializing global attention on CLS token...\n",
            " 17% 17380/100000 [47:57<2:55:57,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:24,173 >> Initializing global attention on CLS token...\n",
            " 17% 17381/100000 [47:57<2:56:23,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:24,297 >> Initializing global attention on CLS token...\n",
            " 17% 17382/100000 [47:57<2:54:56,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:24,422 >> Initializing global attention on CLS token...\n",
            " 17% 17383/100000 [47:57<2:54:37,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:24,553 >> Initializing global attention on CLS token...\n",
            " 17% 17384/100000 [47:57<2:55:38,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:24,680 >> Initializing global attention on CLS token...\n",
            " 17% 17385/100000 [47:57<2:56:44,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:24,812 >> Initializing global attention on CLS token...\n",
            " 17% 17386/100000 [47:58<2:56:27,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:24,936 >> Initializing global attention on CLS token...\n",
            " 17% 17387/100000 [47:58<2:54:18,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:25,059 >> Initializing global attention on CLS token...\n",
            " 17% 17388/100000 [47:58<2:54:25,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:25,186 >> Initializing global attention on CLS token...\n",
            " 17% 17389/100000 [47:58<2:52:23,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:25,307 >> Initializing global attention on CLS token...\n",
            " 17% 17390/100000 [47:58<2:53:00,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:25,434 >> Initializing global attention on CLS token...\n",
            " 17% 17391/100000 [47:58<2:54:25,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:25,568 >> Initializing global attention on CLS token...\n",
            " 17% 17392/100000 [47:58<2:58:48,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:25,705 >> Initializing global attention on CLS token...\n",
            " 17% 17393/100000 [47:58<3:03:11,  7.52it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:25,841 >> Initializing global attention on CLS token...\n",
            " 17% 17394/100000 [47:59<2:58:24,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:25,962 >> Initializing global attention on CLS token...\n",
            " 17% 17395/100000 [47:59<3:05:59,  7.40it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:26,111 >> Initializing global attention on CLS token...\n",
            " 17% 17396/100000 [47:59<3:01:12,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:26,234 >> Initializing global attention on CLS token...\n",
            " 17% 17397/100000 [47:59<2:56:41,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:26,355 >> Initializing global attention on CLS token...\n",
            " 17% 17398/100000 [47:59<2:59:14,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:26,490 >> Initializing global attention on CLS token...\n",
            " 17% 17399/100000 [47:59<2:58:53,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:26,624 >> Initializing global attention on CLS token...\n",
            " 17% 17400/100000 [47:59<3:00:56,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:26,754 >> Initializing global attention on CLS token...\n",
            " 17% 17401/100000 [47:59<2:57:29,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:26,878 >> Initializing global attention on CLS token...\n",
            " 17% 17402/100000 [48:00<2:57:43,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:27,007 >> Initializing global attention on CLS token...\n",
            " 17% 17403/100000 [48:00<2:58:27,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:27,141 >> Initializing global attention on CLS token...\n",
            " 17% 17404/100000 [48:00<2:59:09,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:27,268 >> Initializing global attention on CLS token...\n",
            " 17% 17405/100000 [48:00<2:57:26,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:27,397 >> Initializing global attention on CLS token...\n",
            " 17% 17406/100000 [48:00<2:58:30,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:27,526 >> Initializing global attention on CLS token...\n",
            " 17% 17407/100000 [48:00<2:55:48,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:27,649 >> Initializing global attention on CLS token...\n",
            " 17% 17408/100000 [48:00<2:57:07,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:27,784 >> Initializing global attention on CLS token...\n",
            " 17% 17409/100000 [48:00<2:58:59,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:27,919 >> Initializing global attention on CLS token...\n",
            " 17% 17410/100000 [48:01<2:59:57,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:28,045 >> Initializing global attention on CLS token...\n",
            " 17% 17411/100000 [48:01<2:56:36,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:28,168 >> Initializing global attention on CLS token...\n",
            " 17% 17412/100000 [48:01<2:57:30,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:28,303 >> Initializing global attention on CLS token...\n",
            " 17% 17413/100000 [48:01<2:58:54,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:28,435 >> Initializing global attention on CLS token...\n",
            " 17% 17414/100000 [48:01<3:00:00,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:28,564 >> Initializing global attention on CLS token...\n",
            " 17% 17415/100000 [48:01<2:56:15,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:28,685 >> Initializing global attention on CLS token...\n",
            " 17% 17416/100000 [48:01<2:57:17,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:28,817 >> Initializing global attention on CLS token...\n",
            " 17% 17417/100000 [48:02<2:56:27,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:28,942 >> Initializing global attention on CLS token...\n",
            " 17% 17418/100000 [48:02<2:55:20,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:29,073 >> Initializing global attention on CLS token...\n",
            " 17% 17419/100000 [48:02<2:56:51,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:29,199 >> Initializing global attention on CLS token...\n",
            " 17% 17420/100000 [48:02<2:55:06,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:29,324 >> Initializing global attention on CLS token...\n",
            " 17% 17421/100000 [48:02<2:57:15,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:29,456 >> Initializing global attention on CLS token...\n",
            " 17% 17422/100000 [48:02<2:54:59,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:29,579 >> Initializing global attention on CLS token...\n",
            " 17% 17423/100000 [48:02<2:56:59,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:29,717 >> Initializing global attention on CLS token...\n",
            " 17% 17424/100000 [48:02<2:56:55,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:29,840 >> Initializing global attention on CLS token...\n",
            " 17% 17425/100000 [48:03<2:59:01,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:29,978 >> Initializing global attention on CLS token...\n",
            " 17% 17426/100000 [48:03<2:59:32,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:30,105 >> Initializing global attention on CLS token...\n",
            " 17% 17427/100000 [48:03<2:55:57,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:30,226 >> Initializing global attention on CLS token...\n",
            " 17% 17428/100000 [48:03<2:57:10,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:30,357 >> Initializing global attention on CLS token...\n",
            " 17% 17429/100000 [48:03<2:57:41,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:30,492 >> Initializing global attention on CLS token...\n",
            " 17% 17430/100000 [48:03<2:59:30,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:30,620 >> Initializing global attention on CLS token...\n",
            " 17% 17431/100000 [48:03<2:56:19,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:30,747 >> Initializing global attention on CLS token...\n",
            " 17% 17432/100000 [48:03<2:56:36,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:30,874 >> Initializing global attention on CLS token...\n",
            " 17% 17433/100000 [48:04<2:55:15,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:30,999 >> Initializing global attention on CLS token...\n",
            " 17% 17434/100000 [48:04<2:55:27,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:31,130 >> Initializing global attention on CLS token...\n",
            " 17% 17435/100000 [48:04<2:56:08,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:31,254 >> Initializing global attention on CLS token...\n",
            " 17% 17436/100000 [48:04<2:55:46,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:31,387 >> Initializing global attention on CLS token...\n",
            " 17% 17437/100000 [48:04<2:58:34,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:31,516 >> Initializing global attention on CLS token...\n",
            " 17% 17438/100000 [48:04<2:58:08,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:31,653 >> Initializing global attention on CLS token...\n",
            " 17% 17439/100000 [48:04<2:59:26,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:31,777 >> Initializing global attention on CLS token...\n",
            " 17% 17440/100000 [48:04<2:57:53,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:31,909 >> Initializing global attention on CLS token...\n",
            " 17% 17441/100000 [48:05<3:02:42,  7.53it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:32,045 >> Initializing global attention on CLS token...\n",
            " 17% 17442/100000 [48:05<3:00:06,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:32,175 >> Initializing global attention on CLS token...\n",
            " 17% 17443/100000 [48:05<2:59:09,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:32,300 >> Initializing global attention on CLS token...\n",
            " 17% 17444/100000 [48:05<2:55:17,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:32,421 >> Initializing global attention on CLS token...\n",
            " 17% 17445/100000 [48:05<2:58:42,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:32,561 >> Initializing global attention on CLS token...\n",
            " 17% 17446/100000 [48:05<2:58:11,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:32,685 >> Initializing global attention on CLS token...\n",
            " 17% 17447/100000 [48:05<2:55:22,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:32,808 >> Initializing global attention on CLS token...\n",
            " 17% 17448/100000 [48:06<2:55:59,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:32,941 >> Initializing global attention on CLS token...\n",
            " 17% 17449/100000 [48:06<2:57:23,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:33,068 >> Initializing global attention on CLS token...\n",
            " 17% 17450/100000 [48:06<2:56:33,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:33,200 >> Initializing global attention on CLS token...\n",
            " 17% 17451/100000 [48:06<2:57:15,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:33,325 >> Initializing global attention on CLS token...\n",
            " 17% 17452/100000 [48:06<2:53:48,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:33,445 >> Initializing global attention on CLS token...\n",
            " 17% 17453/100000 [48:06<2:56:55,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:33,580 >> Initializing global attention on CLS token...\n",
            " 17% 17454/100000 [48:06<2:54:24,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:33,702 >> Initializing global attention on CLS token...\n",
            " 17% 17455/100000 [48:06<2:51:56,  8.00it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:33,823 >> Initializing global attention on CLS token...\n",
            " 17% 17456/100000 [48:07<2:54:14,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:33,959 >> Initializing global attention on CLS token...\n",
            " 17% 17457/100000 [48:07<2:58:13,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:34,094 >> Initializing global attention on CLS token...\n",
            " 17% 17458/100000 [48:07<3:00:10,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:34,224 >> Initializing global attention on CLS token...\n",
            " 17% 17459/100000 [48:07<2:56:47,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:34,347 >> Initializing global attention on CLS token...\n",
            " 17% 17460/100000 [48:07<2:58:07,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:34,479 >> Initializing global attention on CLS token...\n",
            " 17% 17461/100000 [48:07<2:57:50,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:34,612 >> Initializing global attention on CLS token...\n",
            " 17% 17462/100000 [48:07<3:00:05,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:34,742 >> Initializing global attention on CLS token...\n",
            " 17% 17463/100000 [48:07<2:55:59,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:34,863 >> Initializing global attention on CLS token...\n",
            " 17% 17464/100000 [48:08<2:55:40,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:34,990 >> Initializing global attention on CLS token...\n",
            " 17% 17465/100000 [48:08<2:56:02,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:35,124 >> Initializing global attention on CLS token...\n",
            " 17% 17466/100000 [48:08<2:57:44,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:35,251 >> Initializing global attention on CLS token...\n",
            " 17% 17467/100000 [48:08<2:55:13,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:35,377 >> Initializing global attention on CLS token...\n",
            " 17% 17468/100000 [48:08<2:58:15,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:35,509 >> Initializing global attention on CLS token...\n",
            " 17% 17469/100000 [48:08<2:58:25,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:35,644 >> Initializing global attention on CLS token...\n",
            " 17% 17470/100000 [48:08<3:01:23,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:35,776 >> Initializing global attention on CLS token...\n",
            " 17% 17471/100000 [48:08<3:01:09,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:35,908 >> Initializing global attention on CLS token...\n",
            " 17% 17472/100000 [48:09<2:57:09,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:36,029 >> Initializing global attention on CLS token...\n",
            " 17% 17473/100000 [48:09<2:56:53,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:36,157 >> Initializing global attention on CLS token...\n",
            " 17% 17474/100000 [48:09<2:57:05,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:36,290 >> Initializing global attention on CLS token...\n",
            " 17% 17475/100000 [48:09<2:58:39,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:36,423 >> Initializing global attention on CLS token...\n",
            " 17% 17476/100000 [48:09<2:58:10,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:36,547 >> Initializing global attention on CLS token...\n",
            " 17% 17477/100000 [48:09<2:55:59,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:36,672 >> Initializing global attention on CLS token...\n",
            " 17% 17478/100000 [48:09<2:57:31,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:36,808 >> Initializing global attention on CLS token...\n",
            " 17% 17479/100000 [48:10<2:56:35,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:36,930 >> Initializing global attention on CLS token...\n",
            " 17% 17480/100000 [48:10<2:58:01,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:37,066 >> Initializing global attention on CLS token...\n",
            " 17% 17481/100000 [48:10<2:59:10,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:37,194 >> Initializing global attention on CLS token...\n",
            " 17% 17482/100000 [48:10<2:57:35,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:37,325 >> Initializing global attention on CLS token...\n",
            " 17% 17483/100000 [48:10<2:57:06,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:37,449 >> Initializing global attention on CLS token...\n",
            " 17% 17484/100000 [48:10<2:54:17,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:37,571 >> Initializing global attention on CLS token...\n",
            " 17% 17485/100000 [48:10<2:55:55,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:37,708 >> Initializing global attention on CLS token...\n",
            " 17% 17486/100000 [48:10<2:55:30,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:37,828 >> Initializing global attention on CLS token...\n",
            " 17% 17487/100000 [48:11<2:54:29,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:37,959 >> Initializing global attention on CLS token...\n",
            " 17% 17488/100000 [48:11<2:56:07,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:38,085 >> Initializing global attention on CLS token...\n",
            " 17% 17489/100000 [48:11<2:54:34,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:38,212 >> Initializing global attention on CLS token...\n",
            " 17% 17490/100000 [48:11<2:55:52,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:38,339 >> Initializing global attention on CLS token...\n",
            " 17% 17491/100000 [48:11<2:53:04,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:38,460 >> Initializing global attention on CLS token...\n",
            " 17% 17492/100000 [48:11<2:56:22,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:38,600 >> Initializing global attention on CLS token...\n",
            " 17% 17493/100000 [48:11<2:58:49,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:38,729 >> Initializing global attention on CLS token...\n",
            " 17% 17494/100000 [48:11<2:57:45,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:38,862 >> Initializing global attention on CLS token...\n",
            " 17% 17495/100000 [48:12<2:59:24,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:38,989 >> Initializing global attention on CLS token...\n",
            " 17% 17496/100000 [48:12<2:55:19,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:39,109 >> Initializing global attention on CLS token...\n",
            " 17% 17497/100000 [48:12<2:57:19,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:39,243 >> Initializing global attention on CLS token...\n",
            " 17% 17498/100000 [48:12<2:55:41,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:39,367 >> Initializing global attention on CLS token...\n",
            " 17% 17499/100000 [48:12<2:52:59,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:39,488 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.4353, 'learning_rate': 8.2505e-05, 'epoch': 3.5}\n",
            " 18% 17500/100000 [48:12<2:57:46,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:39,634 >> Initializing global attention on CLS token...\n",
            " 18% 17501/100000 [48:12<2:58:39,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:39,757 >> Initializing global attention on CLS token...\n",
            " 18% 17502/100000 [48:12<2:56:15,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:39,884 >> Initializing global attention on CLS token...\n",
            " 18% 17503/100000 [48:13<2:57:54,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:40,016 >> Initializing global attention on CLS token...\n",
            " 18% 17504/100000 [48:13<2:56:48,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:40,140 >> Initializing global attention on CLS token...\n",
            " 18% 17505/100000 [48:13<2:59:08,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:40,279 >> Initializing global attention on CLS token...\n",
            " 18% 17506/100000 [48:13<2:59:25,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:40,405 >> Initializing global attention on CLS token...\n",
            " 18% 17507/100000 [48:13<2:55:56,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:40,532 >> Initializing global attention on CLS token...\n",
            " 18% 17508/100000 [48:13<2:57:24,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:40,659 >> Initializing global attention on CLS token...\n",
            " 18% 17509/100000 [48:13<2:56:35,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:40,790 >> Initializing global attention on CLS token...\n",
            " 18% 17510/100000 [48:13<2:57:19,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:40,917 >> Initializing global attention on CLS token...\n",
            " 18% 17511/100000 [48:14<2:54:53,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:41,044 >> Initializing global attention on CLS token...\n",
            " 18% 17512/100000 [48:14<2:56:37,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:41,170 >> Initializing global attention on CLS token...\n",
            " 18% 17513/100000 [48:14<2:55:20,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:41,296 >> Initializing global attention on CLS token...\n",
            " 18% 17514/100000 [48:14<2:55:40,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:41,429 >> Initializing global attention on CLS token...\n",
            " 18% 17515/100000 [48:14<2:56:52,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:41,555 >> Initializing global attention on CLS token...\n",
            " 18% 17516/100000 [48:14<2:57:16,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:41,689 >> Initializing global attention on CLS token...\n",
            " 18% 17517/100000 [48:14<2:57:43,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:41,815 >> Initializing global attention on CLS token...\n",
            " 18% 17518/100000 [48:15<2:55:33,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:41,938 >> Initializing global attention on CLS token...\n",
            " 18% 17519/100000 [48:15<2:55:46,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:42,067 >> Initializing global attention on CLS token...\n",
            " 18% 17520/100000 [48:15<2:55:47,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:42,199 >> Initializing global attention on CLS token...\n",
            " 18% 17521/100000 [48:15<2:59:24,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:42,331 >> Initializing global attention on CLS token...\n",
            " 18% 17522/100000 [48:15<2:54:59,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:42,451 >> Initializing global attention on CLS token...\n",
            " 18% 17523/100000 [48:15<2:57:12,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:42,586 >> Initializing global attention on CLS token...\n",
            " 18% 17524/100000 [48:15<2:58:37,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:42,720 >> Initializing global attention on CLS token...\n",
            " 18% 17525/100000 [48:15<2:58:39,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:42,846 >> Initializing global attention on CLS token...\n",
            " 18% 17526/100000 [48:16<2:55:00,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:42,968 >> Initializing global attention on CLS token...\n",
            " 18% 17527/100000 [48:16<2:54:27,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:43,093 >> Initializing global attention on CLS token...\n",
            " 18% 17528/100000 [48:16<2:54:36,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:43,225 >> Initializing global attention on CLS token...\n",
            " 18% 17529/100000 [48:16<2:56:18,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:43,352 >> Initializing global attention on CLS token...\n",
            " 18% 17530/100000 [48:16<2:53:42,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:43,474 >> Initializing global attention on CLS token...\n",
            " 18% 17531/100000 [48:16<2:56:31,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:43,607 >> Initializing global attention on CLS token...\n",
            " 18% 17532/100000 [48:16<2:56:30,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:43,740 >> Initializing global attention on CLS token...\n",
            " 18% 17533/100000 [48:16<2:56:50,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:43,864 >> Initializing global attention on CLS token...\n",
            " 18% 17534/100000 [48:17<2:54:08,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:43,987 >> Initializing global attention on CLS token...\n",
            " 18% 17535/100000 [48:17<2:54:27,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:44,115 >> Initializing global attention on CLS token...\n",
            " 18% 17536/100000 [48:17<2:52:03,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:44,235 >> Initializing global attention on CLS token...\n",
            " 18% 17537/100000 [48:17<2:51:32,  8.01it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:44,359 >> Initializing global attention on CLS token...\n",
            " 18% 17538/100000 [48:17<2:52:47,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:44,494 >> Initializing global attention on CLS token...\n",
            " 18% 17539/100000 [48:17<2:53:28,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:44,614 >> Initializing global attention on CLS token...\n",
            " 18% 17540/100000 [48:17<2:56:15,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:44,753 >> Initializing global attention on CLS token...\n",
            " 18% 17541/100000 [48:17<2:57:59,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:44,880 >> Initializing global attention on CLS token...\n",
            " 18% 17542/100000 [48:18<2:56:12,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:45,010 >> Initializing global attention on CLS token...\n",
            " 18% 17543/100000 [48:18<2:56:34,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:45,135 >> Initializing global attention on CLS token...\n",
            " 18% 17544/100000 [48:18<2:53:20,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:45,256 >> Initializing global attention on CLS token...\n",
            " 18% 17545/100000 [48:18<3:01:03,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:45,405 >> Initializing global attention on CLS token...\n",
            " 18% 17546/100000 [48:18<3:01:07,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:45,537 >> Initializing global attention on CLS token...\n",
            " 18% 17547/100000 [48:18<3:02:52,  7.51it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:45,670 >> Initializing global attention on CLS token...\n",
            " 18% 17548/100000 [48:18<2:59:12,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:45,792 >> Initializing global attention on CLS token...\n",
            " 18% 17549/100000 [48:19<2:58:08,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:45,920 >> Initializing global attention on CLS token...\n",
            " 18% 17550/100000 [48:19<2:54:12,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:46,039 >> Initializing global attention on CLS token...\n",
            " 18% 17551/100000 [48:19<2:51:34,  8.01it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:46,160 >> Initializing global attention on CLS token...\n",
            " 18% 17552/100000 [48:19<2:51:41,  8.00it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:46,289 >> Initializing global attention on CLS token...\n",
            " 18% 17553/100000 [48:19<2:53:45,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:46,420 >> Initializing global attention on CLS token...\n",
            " 18% 17554/100000 [48:19<2:55:51,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:46,552 >> Initializing global attention on CLS token...\n",
            " 18% 17555/100000 [48:19<2:57:29,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:46,679 >> Initializing global attention on CLS token...\n",
            " 18% 17556/100000 [48:19<2:54:33,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:46,801 >> Initializing global attention on CLS token...\n",
            " 18% 17557/100000 [48:20<2:55:53,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:46,931 >> Initializing global attention on CLS token...\n",
            " 18% 17558/100000 [48:20<2:53:29,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:47,053 >> Initializing global attention on CLS token...\n",
            " 18% 17559/100000 [48:20<2:52:57,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:47,183 >> Initializing global attention on CLS token...\n",
            " 18% 17560/100000 [48:20<2:56:33,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:47,317 >> Initializing global attention on CLS token...\n",
            " 18% 17561/100000 [48:20<2:56:53,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:47,447 >> Initializing global attention on CLS token...\n",
            " 18% 17562/100000 [48:20<2:58:36,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:47,579 >> Initializing global attention on CLS token...\n",
            " 18% 17563/100000 [48:20<2:58:31,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:47,707 >> Initializing global attention on CLS token...\n",
            " 18% 17564/100000 [48:20<2:58:04,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:47,839 >> Initializing global attention on CLS token...\n",
            " 18% 17565/100000 [48:21<2:58:09,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:47,964 >> Initializing global attention on CLS token...\n",
            " 18% 17566/100000 [48:21<2:54:48,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:48,085 >> Initializing global attention on CLS token...\n",
            " 18% 17567/100000 [48:21<2:57:08,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:48,223 >> Initializing global attention on CLS token...\n",
            " 18% 17568/100000 [48:21<2:56:00,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:48,344 >> Initializing global attention on CLS token...\n",
            " 18% 17569/100000 [48:21<2:59:06,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:48,487 >> Initializing global attention on CLS token...\n",
            " 18% 17570/100000 [48:21<3:00:42,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:48,615 >> Initializing global attention on CLS token...\n",
            " 18% 17571/100000 [48:21<2:59:27,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:48,747 >> Initializing global attention on CLS token...\n",
            " 18% 17572/100000 [48:21<3:01:34,  7.57it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:48,879 >> Initializing global attention on CLS token...\n",
            " 18% 17573/100000 [48:22<2:57:18,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:49,000 >> Initializing global attention on CLS token...\n",
            " 18% 17574/100000 [48:22<2:56:53,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:49,134 >> Initializing global attention on CLS token...\n",
            " 18% 17575/100000 [48:22<2:56:18,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:49,256 >> Initializing global attention on CLS token...\n",
            " 18% 17576/100000 [48:22<2:58:34,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:49,395 >> Initializing global attention on CLS token...\n",
            " 18% 17577/100000 [48:22<3:00:00,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:49,523 >> Initializing global attention on CLS token...\n",
            " 18% 17578/100000 [48:22<2:58:20,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:49,654 >> Initializing global attention on CLS token...\n",
            " 18% 17579/100000 [48:22<2:58:57,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:49,781 >> Initializing global attention on CLS token...\n",
            " 18% 17580/100000 [48:22<2:55:35,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:49,904 >> Initializing global attention on CLS token...\n",
            " 18% 17581/100000 [48:23<2:55:45,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:50,035 >> Initializing global attention on CLS token...\n",
            " 18% 17582/100000 [48:23<2:54:15,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:50,156 >> Initializing global attention on CLS token...\n",
            " 18% 17583/100000 [48:23<2:54:24,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:50,287 >> Initializing global attention on CLS token...\n",
            " 18% 17584/100000 [48:23<2:54:38,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:50,411 >> Initializing global attention on CLS token...\n",
            " 18% 17585/100000 [48:23<2:59:10,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:50,554 >> Initializing global attention on CLS token...\n",
            " 18% 17586/100000 [48:23<2:59:18,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:50,685 >> Initializing global attention on CLS token...\n",
            " 18% 17587/100000 [48:23<2:59:44,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:50,814 >> Initializing global attention on CLS token...\n",
            " 18% 17588/100000 [48:24<2:57:08,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:50,936 >> Initializing global attention on CLS token...\n",
            " 18% 17589/100000 [48:24<2:56:32,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:51,067 >> Initializing global attention on CLS token...\n",
            " 18% 17590/100000 [48:24<2:55:22,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:51,189 >> Initializing global attention on CLS token...\n",
            " 18% 17591/100000 [48:24<2:52:46,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:51,310 >> Initializing global attention on CLS token...\n",
            " 18% 17592/100000 [48:24<2:55:48,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:51,450 >> Initializing global attention on CLS token...\n",
            " 18% 17593/100000 [48:24<2:58:00,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:51,577 >> Initializing global attention on CLS token...\n",
            " 18% 17594/100000 [48:24<2:58:20,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:51,712 >> Initializing global attention on CLS token...\n",
            " 18% 17595/100000 [48:24<3:00:28,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:51,842 >> Initializing global attention on CLS token...\n",
            " 18% 17596/100000 [48:25<2:59:49,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:51,973 >> Initializing global attention on CLS token...\n",
            " 18% 17597/100000 [48:25<2:55:52,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:52,094 >> Initializing global attention on CLS token...\n",
            " 18% 17598/100000 [48:25<2:53:14,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:52,215 >> Initializing global attention on CLS token...\n",
            " 18% 17599/100000 [48:25<2:53:37,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:52,347 >> Initializing global attention on CLS token...\n",
            " 18% 17600/100000 [48:25<2:53:45,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:52,472 >> Initializing global attention on CLS token...\n",
            " 18% 17601/100000 [48:25<2:59:52,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:52,615 >> Initializing global attention on CLS token...\n",
            " 18% 17602/100000 [48:25<2:58:01,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:52,737 >> Initializing global attention on CLS token...\n",
            " 18% 17603/100000 [48:25<2:58:37,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:52,876 >> Initializing global attention on CLS token...\n",
            " 18% 17604/100000 [48:26<2:58:25,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:52,998 >> Initializing global attention on CLS token...\n",
            " 18% 17605/100000 [48:26<2:54:33,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:53,118 >> Initializing global attention on CLS token...\n",
            " 18% 17606/100000 [48:26<2:54:32,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:53,252 >> Initializing global attention on CLS token...\n",
            " 18% 17607/100000 [48:26<2:55:52,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:53,380 >> Initializing global attention on CLS token...\n",
            " 18% 17608/100000 [48:26<3:01:38,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:53,518 >> Initializing global attention on CLS token...\n",
            " 18% 17609/100000 [48:26<2:58:42,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:53,645 >> Initializing global attention on CLS token...\n",
            " 18% 17610/100000 [48:26<2:59:30,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:53,782 >> Initializing global attention on CLS token...\n",
            " 18% 17611/100000 [48:26<2:58:54,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:53,904 >> Initializing global attention on CLS token...\n",
            " 18% 17612/100000 [48:27<2:56:52,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:54,034 >> Initializing global attention on CLS token...\n",
            " 18% 17613/100000 [48:27<2:57:46,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:54,161 >> Initializing global attention on CLS token...\n",
            " 18% 17614/100000 [48:27<2:58:19,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:54,292 >> Initializing global attention on CLS token...\n",
            " 18% 17615/100000 [48:27<2:55:59,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:54,419 >> Initializing global attention on CLS token...\n",
            " 18% 17616/100000 [48:27<2:56:20,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:54,545 >> Initializing global attention on CLS token...\n",
            " 18% 17617/100000 [48:27<2:55:27,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:54,672 >> Initializing global attention on CLS token...\n",
            " 18% 17618/100000 [48:27<2:57:53,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:54,807 >> Initializing global attention on CLS token...\n",
            " 18% 17619/100000 [48:28<2:56:35,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:54,932 >> Initializing global attention on CLS token...\n",
            " 18% 17620/100000 [48:28<2:54:48,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:55,056 >> Initializing global attention on CLS token...\n",
            " 18% 17621/100000 [48:28<2:55:39,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:55,187 >> Initializing global attention on CLS token...\n",
            " 18% 17622/100000 [48:28<2:53:46,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:55,308 >> Initializing global attention on CLS token...\n",
            " 18% 17623/100000 [48:28<2:55:22,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:55,443 >> Initializing global attention on CLS token...\n",
            " 18% 17624/100000 [48:28<2:56:35,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:55,569 >> Initializing global attention on CLS token...\n",
            " 18% 17625/100000 [48:28<2:55:40,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:55,700 >> Initializing global attention on CLS token...\n",
            " 18% 17626/100000 [48:28<2:56:59,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:55,827 >> Initializing global attention on CLS token...\n",
            " 18% 17627/100000 [48:29<2:54:16,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:55,950 >> Initializing global attention on CLS token...\n",
            " 18% 17628/100000 [48:29<2:54:00,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:56,080 >> Initializing global attention on CLS token...\n",
            " 18% 17629/100000 [48:29<2:54:30,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:56,204 >> Initializing global attention on CLS token...\n",
            " 18% 17630/100000 [48:29<2:59:14,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:56,342 >> Initializing global attention on CLS token...\n",
            " 18% 17631/100000 [48:29<2:55:30,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:56,464 >> Initializing global attention on CLS token...\n",
            " 18% 17632/100000 [48:29<2:55:29,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:56,598 >> Initializing global attention on CLS token...\n",
            " 18% 17633/100000 [48:29<2:57:54,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:56,725 >> Initializing global attention on CLS token...\n",
            " 18% 17634/100000 [48:29<2:57:48,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:56,859 >> Initializing global attention on CLS token...\n",
            " 18% 17635/100000 [48:30<3:00:12,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:56,990 >> Initializing global attention on CLS token...\n",
            " 18% 17636/100000 [48:30<2:57:23,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:57,119 >> Initializing global attention on CLS token...\n",
            " 18% 17637/100000 [48:30<2:58:31,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:57,246 >> Initializing global attention on CLS token...\n",
            " 18% 17638/100000 [48:30<2:54:55,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:57,370 >> Initializing global attention on CLS token...\n",
            " 18% 17639/100000 [48:30<2:56:42,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:57,503 >> Initializing global attention on CLS token...\n",
            " 18% 17640/100000 [48:30<2:56:06,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:57,627 >> Initializing global attention on CLS token...\n",
            " 18% 17641/100000 [48:30<3:01:09,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:57,773 >> Initializing global attention on CLS token...\n",
            " 18% 17642/100000 [48:30<3:00:43,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:57,898 >> Initializing global attention on CLS token...\n",
            " 18% 17643/100000 [48:31<2:56:25,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:58,020 >> Initializing global attention on CLS token...\n",
            " 18% 17644/100000 [48:31<2:58:26,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:58,154 >> Initializing global attention on CLS token...\n",
            " 18% 17645/100000 [48:31<2:55:16,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:58,275 >> Initializing global attention on CLS token...\n",
            " 18% 17646/100000 [48:31<2:55:19,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:58,408 >> Initializing global attention on CLS token...\n",
            " 18% 17647/100000 [48:31<2:56:44,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:58,535 >> Initializing global attention on CLS token...\n",
            " 18% 17648/100000 [48:31<2:56:22,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:58,668 >> Initializing global attention on CLS token...\n",
            " 18% 17649/100000 [48:31<2:57:49,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:58,799 >> Initializing global attention on CLS token...\n",
            " 18% 17650/100000 [48:32<2:57:29,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:58,923 >> Initializing global attention on CLS token...\n",
            " 18% 17651/100000 [48:32<2:58:43,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:59,055 >> Initializing global attention on CLS token...\n",
            " 18% 17652/100000 [48:32<2:54:35,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:59,176 >> Initializing global attention on CLS token...\n",
            " 18% 17653/100000 [48:32<2:52:07,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:59,297 >> Initializing global attention on CLS token...\n",
            " 18% 17654/100000 [48:32<2:54:38,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:59,445 >> Initializing global attention on CLS token...\n",
            " 18% 17655/100000 [48:32<2:58:48,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:59,565 >> Initializing global attention on CLS token...\n",
            " 18% 17656/100000 [48:32<2:56:38,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:59,695 >> Initializing global attention on CLS token...\n",
            " 18% 17657/100000 [48:32<2:57:22,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:59,826 >> Initializing global attention on CLS token...\n",
            " 18% 17658/100000 [48:33<2:58:41,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:15:59,960 >> Initializing global attention on CLS token...\n",
            " 18% 17659/100000 [48:33<2:59:59,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:00,087 >> Initializing global attention on CLS token...\n",
            " 18% 17660/100000 [48:33<2:56:05,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:00,209 >> Initializing global attention on CLS token...\n",
            " 18% 17661/100000 [48:33<2:56:44,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:00,344 >> Initializing global attention on CLS token...\n",
            " 18% 17662/100000 [48:33<2:56:47,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:00,468 >> Initializing global attention on CLS token...\n",
            " 18% 17663/100000 [48:33<2:55:22,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:00,601 >> Initializing global attention on CLS token...\n",
            " 18% 17664/100000 [48:33<2:58:47,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:00,729 >> Initializing global attention on CLS token...\n",
            " 18% 17665/100000 [48:33<2:56:29,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:00,861 >> Initializing global attention on CLS token...\n",
            " 18% 17666/100000 [48:34<3:02:05,  7.54it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:00,996 >> Initializing global attention on CLS token...\n",
            " 18% 17667/100000 [48:34<2:57:40,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:01,118 >> Initializing global attention on CLS token...\n",
            " 18% 17668/100000 [48:34<2:59:57,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:01,259 >> Initializing global attention on CLS token...\n",
            " 18% 17669/100000 [48:34<2:59:15,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:01,383 >> Initializing global attention on CLS token...\n",
            " 18% 17670/100000 [48:34<2:58:36,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:01,516 >> Initializing global attention on CLS token...\n",
            " 18% 17671/100000 [48:34<2:58:46,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:01,643 >> Initializing global attention on CLS token...\n",
            " 18% 17672/100000 [48:34<2:58:53,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:01,777 >> Initializing global attention on CLS token...\n",
            " 18% 17673/100000 [48:34<3:02:12,  7.53it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:01,912 >> Initializing global attention on CLS token...\n",
            " 18% 17674/100000 [48:35<2:57:51,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:02,033 >> Initializing global attention on CLS token...\n",
            " 18% 17675/100000 [48:35<2:57:02,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:02,167 >> Initializing global attention on CLS token...\n",
            " 18% 17676/100000 [48:35<2:56:43,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:02,289 >> Initializing global attention on CLS token...\n",
            " 18% 17677/100000 [48:35<2:53:54,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:02,413 >> Initializing global attention on CLS token...\n",
            " 18% 17678/100000 [48:35<2:56:19,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:02,548 >> Initializing global attention on CLS token...\n",
            " 18% 17679/100000 [48:35<2:55:51,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:02,677 >> Initializing global attention on CLS token...\n",
            " 18% 17680/100000 [48:35<2:57:22,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:02,803 >> Initializing global attention on CLS token...\n",
            " 18% 17681/100000 [48:36<2:56:03,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:02,932 >> Initializing global attention on CLS token...\n",
            " 18% 17682/100000 [48:36<2:58:14,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:03,069 >> Initializing global attention on CLS token...\n",
            " 18% 17683/100000 [48:36<2:57:31,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:03,191 >> Initializing global attention on CLS token...\n",
            " 18% 17684/100000 [48:36<2:57:13,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:03,324 >> Initializing global attention on CLS token...\n",
            " 18% 17685/100000 [48:36<2:58:31,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:03,454 >> Initializing global attention on CLS token...\n",
            " 18% 17686/100000 [48:36<2:56:01,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:03,582 >> Initializing global attention on CLS token...\n",
            " 18% 17687/100000 [48:36<2:59:01,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:03,712 >> Initializing global attention on CLS token...\n",
            " 18% 17688/100000 [48:36<2:55:25,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:03,834 >> Initializing global attention on CLS token...\n",
            " 18% 17689/100000 [48:37<2:57:26,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:03,972 >> Initializing global attention on CLS token...\n",
            " 18% 17690/100000 [48:37<2:58:09,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:04,098 >> Initializing global attention on CLS token...\n",
            " 18% 17691/100000 [48:37<2:57:46,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:04,231 >> Initializing global attention on CLS token...\n",
            " 18% 17692/100000 [48:37<2:57:43,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:04,356 >> Initializing global attention on CLS token...\n",
            " 18% 17693/100000 [48:37<2:54:09,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:04,481 >> Initializing global attention on CLS token...\n",
            " 18% 17694/100000 [48:37<2:57:48,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:04,612 >> Initializing global attention on CLS token...\n",
            " 18% 17695/100000 [48:37<2:54:38,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:04,735 >> Initializing global attention on CLS token...\n",
            " 18% 17696/100000 [48:37<2:55:11,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:04,871 >> Initializing global attention on CLS token...\n",
            " 18% 17697/100000 [48:38<2:58:55,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:05,002 >> Initializing global attention on CLS token...\n",
            " 18% 17698/100000 [48:38<2:58:14,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:05,133 >> Initializing global attention on CLS token...\n",
            " 18% 17699/100000 [48:38<2:58:07,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:05,258 >> Initializing global attention on CLS token...\n",
            " 18% 17700/100000 [48:38<2:54:43,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:05,382 >> Initializing global attention on CLS token...\n",
            " 18% 17701/100000 [48:38<2:59:32,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:05,522 >> Initializing global attention on CLS token...\n",
            " 18% 17702/100000 [48:38<2:56:29,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:05,643 >> Initializing global attention on CLS token...\n",
            " 18% 17703/100000 [48:38<2:57:02,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:05,777 >> Initializing global attention on CLS token...\n",
            " 18% 17704/100000 [48:38<2:56:10,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:05,899 >> Initializing global attention on CLS token...\n",
            " 18% 17705/100000 [48:39<2:57:49,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:06,038 >> Initializing global attention on CLS token...\n",
            " 18% 17706/100000 [48:39<2:59:55,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:06,167 >> Initializing global attention on CLS token...\n",
            " 18% 17707/100000 [48:39<2:55:36,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:06,292 >> Initializing global attention on CLS token...\n",
            " 18% 17708/100000 [48:39<2:58:05,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:06,421 >> Initializing global attention on CLS token...\n",
            " 18% 17709/100000 [48:39<2:54:04,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:06,541 >> Initializing global attention on CLS token...\n",
            " 18% 17710/100000 [48:39<2:54:38,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:06,675 >> Initializing global attention on CLS token...\n",
            " 18% 17711/100000 [48:39<2:56:39,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:06,803 >> Initializing global attention on CLS token...\n",
            " 18% 17712/100000 [48:40<2:55:38,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:06,933 >> Initializing global attention on CLS token...\n",
            " 18% 17713/100000 [48:40<2:57:42,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:07,068 >> Initializing global attention on CLS token...\n",
            " 18% 17714/100000 [48:40<2:57:37,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:07,191 >> Initializing global attention on CLS token...\n",
            " 18% 17715/100000 [48:40<2:55:25,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:07,315 >> Initializing global attention on CLS token...\n",
            " 18% 17716/100000 [48:40<2:52:00,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:07,435 >> Initializing global attention on CLS token...\n",
            " 18% 17717/100000 [48:40<2:50:55,  8.02it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:07,557 >> Initializing global attention on CLS token...\n",
            " 18% 17718/100000 [48:40<2:52:30,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:07,691 >> Initializing global attention on CLS token...\n",
            " 18% 17719/100000 [48:40<2:54:35,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:07,821 >> Initializing global attention on CLS token...\n",
            " 18% 17720/100000 [48:41<2:55:53,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:07,947 >> Initializing global attention on CLS token...\n",
            " 18% 17721/100000 [48:41<2:52:24,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:08,067 >> Initializing global attention on CLS token...\n",
            " 18% 17722/100000 [48:41<2:57:39,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:08,209 >> Initializing global attention on CLS token...\n",
            " 18% 17723/100000 [48:41<2:55:02,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:08,329 >> Initializing global attention on CLS token...\n",
            " 18% 17724/100000 [48:41<2:54:58,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:08,461 >> Initializing global attention on CLS token...\n",
            " 18% 17725/100000 [48:41<2:56:22,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:08,587 >> Initializing global attention on CLS token...\n",
            " 18% 17726/100000 [48:41<2:53:52,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:08,716 >> Initializing global attention on CLS token...\n",
            " 18% 17727/100000 [48:41<2:59:28,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:08,855 >> Initializing global attention on CLS token...\n",
            " 18% 17728/100000 [48:42<2:57:47,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:08,977 >> Initializing global attention on CLS token...\n",
            " 18% 17729/100000 [48:42<2:57:36,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:09,116 >> Initializing global attention on CLS token...\n",
            " 18% 17730/100000 [48:42<3:01:24,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:09,249 >> Initializing global attention on CLS token...\n",
            " 18% 17731/100000 [48:42<3:00:32,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:09,376 >> Initializing global attention on CLS token...\n",
            " 18% 17732/100000 [48:42<2:57:16,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:09,499 >> Initializing global attention on CLS token...\n",
            " 18% 17733/100000 [48:42<2:58:21,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:09,634 >> Initializing global attention on CLS token...\n",
            " 18% 17734/100000 [48:42<2:56:15,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:09,756 >> Initializing global attention on CLS token...\n",
            " 18% 17735/100000 [48:42<2:52:50,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:09,878 >> Initializing global attention on CLS token...\n",
            " 18% 17736/100000 [48:43<2:53:51,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:10,013 >> Initializing global attention on CLS token...\n",
            " 18% 17737/100000 [48:43<2:59:18,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:10,151 >> Initializing global attention on CLS token...\n",
            " 18% 17738/100000 [48:43<3:00:38,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:10,283 >> Initializing global attention on CLS token...\n",
            " 18% 17739/100000 [48:43<3:00:27,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:10,411 >> Initializing global attention on CLS token...\n",
            " 18% 17740/100000 [48:43<2:56:00,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:10,535 >> Initializing global attention on CLS token...\n",
            " 18% 17741/100000 [48:43<2:56:57,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:10,662 >> Initializing global attention on CLS token...\n",
            " 18% 17742/100000 [48:43<2:53:35,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:10,783 >> Initializing global attention on CLS token...\n",
            " 18% 17743/100000 [48:43<2:53:29,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:10,913 >> Initializing global attention on CLS token...\n",
            " 18% 17744/100000 [48:44<2:54:51,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:11,039 >> Initializing global attention on CLS token...\n",
            " 18% 17745/100000 [48:44<2:54:15,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:11,175 >> Initializing global attention on CLS token...\n",
            " 18% 17746/100000 [48:44<3:00:05,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:11,306 >> Initializing global attention on CLS token...\n",
            " 18% 17747/100000 [48:44<2:56:22,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:11,429 >> Initializing global attention on CLS token...\n",
            " 18% 17748/100000 [48:44<2:57:55,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:11,561 >> Initializing global attention on CLS token...\n",
            " 18% 17749/100000 [48:44<2:54:28,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:11,682 >> Initializing global attention on CLS token...\n",
            " 18% 17750/100000 [48:44<2:52:19,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:11,804 >> Initializing global attention on CLS token...\n",
            " 18% 17751/100000 [48:45<2:54:40,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:11,940 >> Initializing global attention on CLS token...\n",
            " 18% 17752/100000 [48:45<2:53:44,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:12,061 >> Initializing global attention on CLS token...\n",
            " 18% 17753/100000 [48:45<2:52:59,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:12,192 >> Initializing global attention on CLS token...\n",
            " 18% 17754/100000 [48:45<3:00:09,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:12,330 >> Initializing global attention on CLS token...\n",
            " 18% 17755/100000 [48:45<2:57:33,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:12,459 >> Initializing global attention on CLS token...\n",
            " 18% 17756/100000 [48:45<2:58:16,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:12,589 >> Initializing global attention on CLS token...\n",
            " 18% 17757/100000 [48:45<2:55:58,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:12,710 >> Initializing global attention on CLS token...\n",
            " 18% 17758/100000 [48:45<2:55:10,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:12,840 >> Initializing global attention on CLS token...\n",
            " 18% 17759/100000 [48:46<2:53:53,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:12,962 >> Initializing global attention on CLS token...\n",
            " 18% 17760/100000 [48:46<2:53:07,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:13,092 >> Initializing global attention on CLS token...\n",
            " 18% 17761/100000 [48:46<2:54:35,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:13,219 >> Initializing global attention on CLS token...\n",
            " 18% 17762/100000 [48:46<2:54:20,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:13,343 >> Initializing global attention on CLS token...\n",
            " 18% 17763/100000 [48:46<2:55:16,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:13,473 >> Initializing global attention on CLS token...\n",
            " 18% 17764/100000 [48:46<2:54:13,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:13,598 >> Initializing global attention on CLS token...\n",
            " 18% 17765/100000 [48:46<2:52:03,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:13,720 >> Initializing global attention on CLS token...\n",
            " 18% 17766/100000 [48:46<2:53:21,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:13,851 >> Initializing global attention on CLS token...\n",
            " 18% 17767/100000 [48:47<2:52:34,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:13,973 >> Initializing global attention on CLS token...\n",
            " 18% 17768/100000 [48:47<2:53:01,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:14,106 >> Initializing global attention on CLS token...\n",
            " 18% 17769/100000 [48:47<2:57:21,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:14,237 >> Initializing global attention on CLS token...\n",
            " 18% 17770/100000 [48:47<2:55:12,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:14,367 >> Initializing global attention on CLS token...\n",
            " 18% 17771/100000 [48:47<2:56:04,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:14,492 >> Initializing global attention on CLS token...\n",
            " 18% 17772/100000 [48:47<2:54:15,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:14,615 >> Initializing global attention on CLS token...\n",
            " 18% 17773/100000 [48:47<2:53:44,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:14,745 >> Initializing global attention on CLS token...\n",
            " 18% 17774/100000 [48:47<2:54:27,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:14,870 >> Initializing global attention on CLS token...\n",
            " 18% 17775/100000 [48:48<2:54:38,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:15,002 >> Initializing global attention on CLS token...\n",
            " 18% 17776/100000 [48:48<2:56:16,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:15,129 >> Initializing global attention on CLS token...\n",
            " 18% 17777/100000 [48:48<2:53:54,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:15,253 >> Initializing global attention on CLS token...\n",
            " 18% 17778/100000 [48:48<2:59:11,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:15,393 >> Initializing global attention on CLS token...\n",
            " 18% 17779/100000 [48:48<2:58:16,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:15,524 >> Initializing global attention on CLS token...\n",
            " 18% 17780/100000 [48:48<2:56:39,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:15,647 >> Initializing global attention on CLS token...\n",
            " 18% 17781/100000 [48:48<2:57:42,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:15,782 >> Initializing global attention on CLS token...\n",
            " 18% 17782/100000 [48:48<2:57:46,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:15,912 >> Initializing global attention on CLS token...\n",
            " 18% 17783/100000 [48:49<2:56:59,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:16,035 >> Initializing global attention on CLS token...\n",
            " 18% 17784/100000 [48:49<2:53:36,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:16,156 >> Initializing global attention on CLS token...\n",
            " 18% 17785/100000 [48:49<2:55:40,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:16,291 >> Initializing global attention on CLS token...\n",
            " 18% 17786/100000 [48:49<2:54:30,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:16,414 >> Initializing global attention on CLS token...\n",
            " 18% 17787/100000 [48:49<2:52:34,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:16,537 >> Initializing global attention on CLS token...\n",
            " 18% 17788/100000 [48:49<2:53:43,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:16,671 >> Initializing global attention on CLS token...\n",
            " 18% 17789/100000 [48:49<2:54:56,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:16,795 >> Initializing global attention on CLS token...\n",
            " 18% 17790/100000 [48:50<2:54:17,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:16,926 >> Initializing global attention on CLS token...\n",
            " 18% 17791/100000 [48:50<2:58:11,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:17,057 >> Initializing global attention on CLS token...\n",
            " 18% 17792/100000 [48:50<2:55:00,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:17,184 >> Initializing global attention on CLS token...\n",
            " 18% 17793/100000 [48:50<2:58:09,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:17,315 >> Initializing global attention on CLS token...\n",
            " 18% 17794/100000 [48:50<2:55:46,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:17,439 >> Initializing global attention on CLS token...\n",
            " 18% 17795/100000 [48:50<2:59:54,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:17,582 >> Initializing global attention on CLS token...\n",
            " 18% 17796/100000 [48:50<2:58:46,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:17,706 >> Initializing global attention on CLS token...\n",
            " 18% 17797/100000 [48:50<2:54:57,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:17,828 >> Initializing global attention on CLS token...\n",
            " 18% 17798/100000 [48:51<2:56:07,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:17,964 >> Initializing global attention on CLS token...\n",
            " 18% 17799/100000 [48:51<2:56:42,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:18,088 >> Initializing global attention on CLS token...\n",
            " 18% 17800/100000 [48:51<2:56:00,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:18,221 >> Initializing global attention on CLS token...\n",
            " 18% 17801/100000 [48:51<2:57:03,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:18,346 >> Initializing global attention on CLS token...\n",
            " 18% 17802/100000 [48:51<2:57:55,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:18,478 >> Initializing global attention on CLS token...\n",
            " 18% 17803/100000 [48:51<2:58:56,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:18,611 >> Initializing global attention on CLS token...\n",
            " 18% 17804/100000 [48:51<2:55:19,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:18,732 >> Initializing global attention on CLS token...\n",
            " 18% 17805/100000 [48:51<2:53:12,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:18,855 >> Initializing global attention on CLS token...\n",
            " 18% 17806/100000 [48:52<2:53:47,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:18,986 >> Initializing global attention on CLS token...\n",
            " 18% 17807/100000 [48:52<2:53:25,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:19,108 >> Initializing global attention on CLS token...\n",
            " 18% 17808/100000 [48:52<2:52:23,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:19,238 >> Initializing global attention on CLS token...\n",
            " 18% 17809/100000 [48:52<2:54:04,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:19,363 >> Initializing global attention on CLS token...\n",
            " 18% 17810/100000 [48:52<2:52:18,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:19,486 >> Initializing global attention on CLS token...\n",
            " 18% 17811/100000 [48:52<2:56:25,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:19,621 >> Initializing global attention on CLS token...\n",
            " 18% 17812/100000 [48:52<2:53:03,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:19,742 >> Initializing global attention on CLS token...\n",
            " 18% 17813/100000 [48:52<2:53:54,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:19,875 >> Initializing global attention on CLS token...\n",
            " 18% 17814/100000 [48:53<2:54:36,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:19,999 >> Initializing global attention on CLS token...\n",
            " 18% 17815/100000 [48:53<2:54:35,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:20,132 >> Initializing global attention on CLS token...\n",
            " 18% 17816/100000 [48:53<2:55:52,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:20,257 >> Initializing global attention on CLS token...\n",
            " 18% 17817/100000 [48:53<2:54:07,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:20,381 >> Initializing global attention on CLS token...\n",
            " 18% 17818/100000 [48:53<3:03:30,  7.46it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:20,533 >> Initializing global attention on CLS token...\n",
            " 18% 17819/100000 [48:53<3:01:33,  7.54it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:20,666 >> Initializing global attention on CLS token...\n",
            " 18% 17820/100000 [48:53<3:01:46,  7.53it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:20,794 >> Initializing global attention on CLS token...\n",
            " 18% 17821/100000 [48:54<2:59:22,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:20,925 >> Initializing global attention on CLS token...\n",
            " 18% 17822/100000 [48:54<2:58:44,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:21,050 >> Initializing global attention on CLS token...\n",
            " 18% 17823/100000 [48:54<2:55:10,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:21,172 >> Initializing global attention on CLS token...\n",
            " 18% 17824/100000 [48:54<2:56:14,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:21,303 >> Initializing global attention on CLS token...\n",
            " 18% 17825/100000 [48:54<2:53:46,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:21,425 >> Initializing global attention on CLS token...\n",
            " 18% 17826/100000 [48:54<2:55:33,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:21,562 >> Initializing global attention on CLS token...\n",
            " 18% 17827/100000 [48:54<2:57:14,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:21,688 >> Initializing global attention on CLS token...\n",
            " 18% 17828/100000 [48:54<2:54:52,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:21,817 >> Initializing global attention on CLS token...\n",
            " 18% 17829/100000 [48:55<2:55:48,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:21,942 >> Initializing global attention on CLS token...\n",
            " 18% 17830/100000 [48:55<2:52:57,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:22,064 >> Initializing global attention on CLS token...\n",
            " 18% 17831/100000 [48:55<2:53:38,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:22,196 >> Initializing global attention on CLS token...\n",
            " 18% 17832/100000 [48:55<2:54:16,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:22,320 >> Initializing global attention on CLS token...\n",
            " 18% 17833/100000 [48:55<2:55:03,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:22,454 >> Initializing global attention on CLS token...\n",
            " 18% 17834/100000 [48:55<3:00:18,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:22,594 >> Initializing global attention on CLS token...\n",
            " 18% 17835/100000 [48:55<2:59:25,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:22,723 >> Initializing global attention on CLS token...\n",
            " 18% 17836/100000 [48:55<2:58:47,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:22,848 >> Initializing global attention on CLS token...\n",
            " 18% 17837/100000 [48:56<2:54:31,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:22,968 >> Initializing global attention on CLS token...\n",
            " 18% 17838/100000 [48:56<2:54:09,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:23,100 >> Initializing global attention on CLS token...\n",
            " 18% 17839/100000 [48:56<2:54:02,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:23,222 >> Initializing global attention on CLS token...\n",
            " 18% 17840/100000 [48:56<2:53:31,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:23,352 >> Initializing global attention on CLS token...\n",
            " 18% 17841/100000 [48:56<2:54:39,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:23,477 >> Initializing global attention on CLS token...\n",
            " 18% 17842/100000 [48:56<2:54:20,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:23,610 >> Initializing global attention on CLS token...\n",
            " 18% 17843/100000 [48:56<2:59:55,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:23,746 >> Initializing global attention on CLS token...\n",
            " 18% 17844/100000 [48:56<2:57:25,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:23,876 >> Initializing global attention on CLS token...\n",
            " 18% 17845/100000 [48:57<2:57:17,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:24,000 >> Initializing global attention on CLS token...\n",
            " 18% 17846/100000 [48:57<2:53:41,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:24,123 >> Initializing global attention on CLS token...\n",
            " 18% 17847/100000 [48:57<2:55:13,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:24,251 >> Initializing global attention on CLS token...\n",
            " 18% 17848/100000 [48:57<2:52:07,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:24,371 >> Initializing global attention on CLS token...\n",
            " 18% 17849/100000 [48:57<2:52:46,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:24,503 >> Initializing global attention on CLS token...\n",
            " 18% 17850/100000 [48:57<2:53:59,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:24,631 >> Initializing global attention on CLS token...\n",
            " 18% 17851/100000 [48:57<2:58:04,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:24,766 >> Initializing global attention on CLS token...\n",
            " 18% 17852/100000 [48:57<2:55:08,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:24,888 >> Initializing global attention on CLS token...\n",
            " 18% 17853/100000 [48:58<2:54:23,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:25,018 >> Initializing global attention on CLS token...\n",
            " 18% 17854/100000 [48:58<2:53:50,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:25,140 >> Initializing global attention on CLS token...\n",
            " 18% 17855/100000 [48:58<2:52:59,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:25,269 >> Initializing global attention on CLS token...\n",
            " 18% 17856/100000 [48:58<2:53:38,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:25,393 >> Initializing global attention on CLS token...\n",
            " 18% 17857/100000 [48:58<2:52:18,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:25,520 >> Initializing global attention on CLS token...\n",
            " 18% 17858/100000 [48:58<2:57:50,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:25,656 >> Initializing global attention on CLS token...\n",
            " 18% 17859/100000 [48:58<2:55:40,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:25,781 >> Initializing global attention on CLS token...\n",
            " 18% 17860/100000 [48:58<2:53:24,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:25,903 >> Initializing global attention on CLS token...\n",
            " 18% 17861/100000 [48:59<2:53:14,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:26,031 >> Initializing global attention on CLS token...\n",
            " 18% 17862/100000 [48:59<2:52:53,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:26,155 >> Initializing global attention on CLS token...\n",
            " 18% 17863/100000 [48:59<2:53:22,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:26,288 >> Initializing global attention on CLS token...\n",
            " 18% 17864/100000 [48:59<2:54:16,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:26,413 >> Initializing global attention on CLS token...\n",
            " 18% 17865/100000 [48:59<2:52:45,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:26,535 >> Initializing global attention on CLS token...\n",
            " 18% 17866/100000 [48:59<2:54:51,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:26,667 >> Initializing global attention on CLS token...\n",
            " 18% 17867/100000 [48:59<2:56:04,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:26,802 >> Initializing global attention on CLS token...\n",
            " 18% 17868/100000 [49:00<2:57:03,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:26,929 >> Initializing global attention on CLS token...\n",
            " 18% 17869/100000 [49:00<2:52:48,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:27,052 >> Initializing global attention on CLS token...\n",
            " 18% 17870/100000 [49:00<2:55:11,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:27,179 >> Initializing global attention on CLS token...\n",
            " 18% 17871/100000 [49:00<2:51:49,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:27,299 >> Initializing global attention on CLS token...\n",
            " 18% 17872/100000 [49:00<2:51:43,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:27,429 >> Initializing global attention on CLS token...\n",
            " 18% 17873/100000 [49:00<2:55:07,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:27,558 >> Initializing global attention on CLS token...\n",
            " 18% 17874/100000 [49:00<2:55:48,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:27,692 >> Initializing global attention on CLS token...\n",
            " 18% 17875/100000 [49:00<2:58:18,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:27,822 >> Initializing global attention on CLS token...\n",
            " 18% 17876/100000 [49:01<2:54:35,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:27,943 >> Initializing global attention on CLS token...\n",
            " 18% 17877/100000 [49:01<2:54:14,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:28,072 >> Initializing global attention on CLS token...\n",
            " 18% 17878/100000 [49:01<2:54:27,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:28,204 >> Initializing global attention on CLS token...\n",
            " 18% 17879/100000 [49:01<2:56:23,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:28,331 >> Initializing global attention on CLS token...\n",
            " 18% 17880/100000 [49:01<2:52:59,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:28,451 >> Initializing global attention on CLS token...\n",
            " 18% 17881/100000 [49:01<2:54:51,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:28,585 >> Initializing global attention on CLS token...\n",
            " 18% 17882/100000 [49:01<2:54:42,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:28,709 >> Initializing global attention on CLS token...\n",
            " 18% 17883/100000 [49:01<2:52:48,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:28,832 >> Initializing global attention on CLS token...\n",
            " 18% 17884/100000 [49:02<2:53:25,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:28,965 >> Initializing global attention on CLS token...\n",
            " 18% 17885/100000 [49:02<2:53:45,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:29,087 >> Initializing global attention on CLS token...\n",
            " 18% 17886/100000 [49:02<2:53:41,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:29,219 >> Initializing global attention on CLS token...\n",
            " 18% 17887/100000 [49:02<2:54:25,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:29,343 >> Initializing global attention on CLS token...\n",
            " 18% 17888/100000 [49:02<2:51:12,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:29,463 >> Initializing global attention on CLS token...\n",
            " 18% 17889/100000 [49:02<2:57:54,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:29,609 >> Initializing global attention on CLS token...\n",
            " 18% 17890/100000 [49:02<2:56:04,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:29,730 >> Initializing global attention on CLS token...\n",
            " 18% 17891/100000 [49:02<2:55:11,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:29,856 >> Initializing global attention on CLS token...\n",
            " 18% 17892/100000 [49:03<2:55:30,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:29,986 >> Initializing global attention on CLS token...\n",
            " 18% 17893/100000 [49:03<2:52:39,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:30,107 >> Initializing global attention on CLS token...\n",
            " 18% 17894/100000 [49:03<2:50:32,  8.02it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:30,228 >> Initializing global attention on CLS token...\n",
            " 18% 17895/100000 [49:03<2:52:12,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:30,361 >> Initializing global attention on CLS token...\n",
            " 18% 17896/100000 [49:03<2:54:27,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:30,487 >> Initializing global attention on CLS token...\n",
            " 18% 17897/100000 [49:03<2:55:36,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:30,622 >> Initializing global attention on CLS token...\n",
            " 18% 17898/100000 [49:03<2:55:54,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:30,747 >> Initializing global attention on CLS token...\n",
            " 18% 17899/100000 [49:03<2:54:24,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:30,873 >> Initializing global attention on CLS token...\n",
            " 18% 17900/100000 [49:04<2:54:17,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:30,999 >> Initializing global attention on CLS token...\n",
            " 18% 17901/100000 [49:04<2:52:53,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:31,129 >> Initializing global attention on CLS token...\n",
            " 18% 17902/100000 [49:04<2:54:27,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:31,253 >> Initializing global attention on CLS token...\n",
            " 18% 17903/100000 [49:04<2:50:59,  8.00it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:31,372 >> Initializing global attention on CLS token...\n",
            " 18% 17904/100000 [49:04<2:52:02,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:31,503 >> Initializing global attention on CLS token...\n",
            " 18% 17905/100000 [49:04<2:52:26,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:31,627 >> Initializing global attention on CLS token...\n",
            " 18% 17906/100000 [49:04<2:50:35,  8.02it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:31,748 >> Initializing global attention on CLS token...\n",
            " 18% 17907/100000 [49:04<2:53:52,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:31,887 >> Initializing global attention on CLS token...\n",
            " 18% 17908/100000 [49:05<2:56:22,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:32,014 >> Initializing global attention on CLS token...\n",
            " 18% 17909/100000 [49:05<2:55:30,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:32,145 >> Initializing global attention on CLS token...\n",
            " 18% 17910/100000 [49:05<2:55:54,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:32,271 >> Initializing global attention on CLS token...\n",
            " 18% 17911/100000 [49:05<2:53:22,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:32,399 >> Initializing global attention on CLS token...\n",
            " 18% 17912/100000 [49:05<2:55:16,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:32,524 >> Initializing global attention on CLS token...\n",
            " 18% 17913/100000 [49:05<2:54:14,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:32,649 >> Initializing global attention on CLS token...\n",
            " 18% 17914/100000 [49:05<2:54:41,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:32,783 >> Initializing global attention on CLS token...\n",
            " 18% 17915/100000 [49:05<2:57:09,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:32,918 >> Initializing global attention on CLS token...\n",
            " 18% 17916/100000 [49:06<3:04:03,  7.43it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:33,058 >> Initializing global attention on CLS token...\n",
            " 18% 17917/100000 [49:06<2:59:06,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:33,180 >> Initializing global attention on CLS token...\n",
            " 18% 17918/100000 [49:06<2:58:35,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:33,312 >> Initializing global attention on CLS token...\n",
            " 18% 17919/100000 [49:06<2:57:26,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:33,443 >> Initializing global attention on CLS token...\n",
            " 18% 17920/100000 [49:06<2:59:23,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:33,572 >> Initializing global attention on CLS token...\n",
            " 18% 17921/100000 [49:06<2:55:23,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:33,693 >> Initializing global attention on CLS token...\n",
            " 18% 17922/100000 [49:06<2:55:50,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:33,824 >> Initializing global attention on CLS token...\n",
            " 18% 17923/100000 [49:07<2:52:45,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:33,944 >> Initializing global attention on CLS token...\n",
            " 18% 17924/100000 [49:07<2:51:54,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:34,068 >> Initializing global attention on CLS token...\n",
            " 18% 17925/100000 [49:07<2:52:44,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:34,200 >> Initializing global attention on CLS token...\n",
            " 18% 17926/100000 [49:07<2:53:42,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:34,324 >> Initializing global attention on CLS token...\n",
            " 18% 17927/100000 [49:07<2:54:17,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:34,457 >> Initializing global attention on CLS token...\n",
            " 18% 17928/100000 [49:07<2:55:03,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:34,582 >> Initializing global attention on CLS token...\n",
            " 18% 17929/100000 [49:07<2:52:45,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:34,704 >> Initializing global attention on CLS token...\n",
            " 18% 17930/100000 [49:07<2:57:57,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:34,844 >> Initializing global attention on CLS token...\n",
            " 18% 17931/100000 [49:08<2:57:06,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:34,977 >> Initializing global attention on CLS token...\n",
            " 18% 17932/100000 [49:08<2:58:11,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:35,106 >> Initializing global attention on CLS token...\n",
            " 18% 17933/100000 [49:08<2:55:24,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:35,233 >> Initializing global attention on CLS token...\n",
            " 18% 17934/100000 [49:08<2:56:32,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:35,358 >> Initializing global attention on CLS token...\n",
            " 18% 17935/100000 [49:08<2:56:22,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:35,492 >> Initializing global attention on CLS token...\n",
            " 18% 17936/100000 [49:08<2:57:27,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:35,618 >> Initializing global attention on CLS token...\n",
            " 18% 17937/100000 [49:08<2:56:10,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:35,749 >> Initializing global attention on CLS token...\n",
            " 18% 17938/100000 [49:08<2:57:13,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:35,878 >> Initializing global attention on CLS token...\n",
            " 18% 17939/100000 [49:09<2:54:07,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:35,998 >> Initializing global attention on CLS token...\n",
            " 18% 17940/100000 [49:09<2:56:34,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:36,136 >> Initializing global attention on CLS token...\n",
            " 18% 17941/100000 [49:09<2:58:56,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:36,271 >> Initializing global attention on CLS token...\n",
            " 18% 17942/100000 [49:09<2:59:00,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:36,398 >> Initializing global attention on CLS token...\n",
            " 18% 17943/100000 [49:09<2:55:20,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:36,520 >> Initializing global attention on CLS token...\n",
            " 18% 17944/100000 [49:09<2:59:00,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:36,657 >> Initializing global attention on CLS token...\n",
            " 18% 17945/100000 [49:09<2:55:20,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:36,778 >> Initializing global attention on CLS token...\n",
            " 18% 17946/100000 [49:09<2:52:23,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:36,899 >> Initializing global attention on CLS token...\n",
            " 18% 17947/100000 [49:10<2:52:23,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:37,031 >> Initializing global attention on CLS token...\n",
            " 18% 17948/100000 [49:10<2:54:57,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:37,158 >> Initializing global attention on CLS token...\n",
            " 18% 17949/100000 [49:10<2:53:48,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:37,287 >> Initializing global attention on CLS token...\n",
            " 18% 17950/100000 [49:10<2:54:09,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:37,412 >> Initializing global attention on CLS token...\n",
            " 18% 17951/100000 [49:10<2:51:20,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:37,532 >> Initializing global attention on CLS token...\n",
            " 18% 17952/100000 [49:10<2:56:17,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:37,672 >> Initializing global attention on CLS token...\n",
            " 18% 17953/100000 [49:10<2:54:18,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:37,793 >> Initializing global attention on CLS token...\n",
            " 18% 17954/100000 [49:10<2:52:14,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:37,916 >> Initializing global attention on CLS token...\n",
            " 18% 17955/100000 [49:11<2:53:32,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:38,051 >> Initializing global attention on CLS token...\n",
            " 18% 17956/100000 [49:11<2:56:01,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:38,178 >> Initializing global attention on CLS token...\n",
            " 18% 17957/100000 [49:11<2:54:59,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:38,309 >> Initializing global attention on CLS token...\n",
            " 18% 17958/100000 [49:11<2:55:30,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:38,433 >> Initializing global attention on CLS token...\n",
            " 18% 17959/100000 [49:11<2:52:36,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:38,558 >> Initializing global attention on CLS token...\n",
            " 18% 17960/100000 [49:11<2:57:39,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:38,693 >> Initializing global attention on CLS token...\n",
            " 18% 17961/100000 [49:11<2:53:51,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:38,813 >> Initializing global attention on CLS token...\n",
            " 18% 17962/100000 [49:12<2:54:17,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:38,946 >> Initializing global attention on CLS token...\n",
            " 18% 17963/100000 [49:12<2:54:52,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:39,071 >> Initializing global attention on CLS token...\n",
            " 18% 17964/100000 [49:12<2:55:01,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:39,203 >> Initializing global attention on CLS token...\n",
            " 18% 17965/100000 [49:12<2:54:44,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:39,326 >> Initializing global attention on CLS token...\n",
            " 18% 17966/100000 [49:12<2:53:10,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:39,451 >> Initializing global attention on CLS token...\n",
            " 18% 17967/100000 [49:12<2:54:43,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:39,581 >> Initializing global attention on CLS token...\n",
            " 18% 17968/100000 [49:12<2:54:05,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:39,707 >> Initializing global attention on CLS token...\n",
            " 18% 17969/100000 [49:12<2:51:49,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:39,829 >> Initializing global attention on CLS token...\n",
            " 18% 17970/100000 [49:13<2:53:35,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:39,963 >> Initializing global attention on CLS token...\n",
            " 18% 17971/100000 [49:13<2:54:52,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:40,094 >> Initializing global attention on CLS token...\n",
            " 18% 17972/100000 [49:13<3:02:12,  7.50it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:40,235 >> Initializing global attention on CLS token...\n",
            " 18% 17973/100000 [49:13<2:56:50,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:40,355 >> Initializing global attention on CLS token...\n",
            " 18% 17974/100000 [49:13<2:56:53,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:40,488 >> Initializing global attention on CLS token...\n",
            " 18% 17975/100000 [49:13<2:56:20,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:40,613 >> Initializing global attention on CLS token...\n",
            " 18% 17976/100000 [49:13<2:53:27,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:40,735 >> Initializing global attention on CLS token...\n",
            " 18% 17977/100000 [49:13<2:53:06,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:40,866 >> Initializing global attention on CLS token...\n",
            " 18% 17978/100000 [49:14<2:55:22,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:40,997 >> Initializing global attention on CLS token...\n",
            " 18% 17979/100000 [49:14<2:56:28,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:41,124 >> Initializing global attention on CLS token...\n",
            " 18% 17980/100000 [49:14<2:54:05,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:41,247 >> Initializing global attention on CLS token...\n",
            " 18% 17981/100000 [49:14<2:54:37,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:41,380 >> Initializing global attention on CLS token...\n",
            " 18% 17982/100000 [49:14<2:53:51,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:41,501 >> Initializing global attention on CLS token...\n",
            " 18% 17983/100000 [49:14<2:54:30,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:41,636 >> Initializing global attention on CLS token...\n",
            " 18% 17984/100000 [49:14<2:58:35,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:41,768 >> Initializing global attention on CLS token...\n",
            " 18% 17985/100000 [49:14<2:55:10,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:41,890 >> Initializing global attention on CLS token...\n",
            " 18% 17986/100000 [49:15<2:55:52,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:42,020 >> Initializing global attention on CLS token...\n",
            " 18% 17987/100000 [49:15<2:54:47,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:42,151 >> Initializing global attention on CLS token...\n",
            " 18% 17988/100000 [49:15<2:58:12,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:42,282 >> Initializing global attention on CLS token...\n",
            " 18% 17989/100000 [49:15<2:54:25,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:42,408 >> Initializing global attention on CLS token...\n",
            " 18% 17990/100000 [49:15<2:55:22,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:42,533 >> Initializing global attention on CLS token...\n",
            " 18% 17991/100000 [49:15<2:54:36,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:42,664 >> Initializing global attention on CLS token...\n",
            " 18% 17992/100000 [49:15<2:57:24,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:42,794 >> Initializing global attention on CLS token...\n",
            " 18% 17993/100000 [49:16<2:55:39,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:42,925 >> Initializing global attention on CLS token...\n",
            " 18% 17994/100000 [49:16<2:56:32,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:43,050 >> Initializing global attention on CLS token...\n",
            " 18% 17995/100000 [49:16<2:52:57,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:43,171 >> Initializing global attention on CLS token...\n",
            " 18% 17996/100000 [49:16<2:54:29,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:43,306 >> Initializing global attention on CLS token...\n",
            " 18% 17997/100000 [49:16<2:56:29,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:43,434 >> Initializing global attention on CLS token...\n",
            " 18% 17998/100000 [49:16<2:54:31,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:43,564 >> Initializing global attention on CLS token...\n",
            " 18% 17999/100000 [49:16<2:55:45,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:43,689 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.4331, 'learning_rate': 8.2005e-05, 'epoch': 3.6}\n",
            " 18% 18000/100000 [49:16<2:58:30,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:43,832 >> Initializing global attention on CLS token...\n",
            " 18% 18001/100000 [49:17<2:57:08,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:43,951 >> Initializing global attention on CLS token...\n",
            " 18% 18002/100000 [49:17<2:53:42,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:44,073 >> Initializing global attention on CLS token...\n",
            " 18% 18003/100000 [49:17<2:54:26,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:44,206 >> Initializing global attention on CLS token...\n",
            " 18% 18004/100000 [49:17<2:56:01,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:44,334 >> Initializing global attention on CLS token...\n",
            " 18% 18005/100000 [49:17<2:55:06,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:44,464 >> Initializing global attention on CLS token...\n",
            " 18% 18006/100000 [49:17<2:56:06,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:44,590 >> Initializing global attention on CLS token...\n",
            " 18% 18007/100000 [49:17<2:52:59,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:44,713 >> Initializing global attention on CLS token...\n",
            " 18% 18008/100000 [49:17<2:58:04,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:44,853 >> Initializing global attention on CLS token...\n",
            " 18% 18009/100000 [49:18<2:56:03,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:44,976 >> Initializing global attention on CLS token...\n",
            " 18% 18010/100000 [49:18<2:53:37,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:45,099 >> Initializing global attention on CLS token...\n",
            " 18% 18011/100000 [49:18<2:54:11,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:45,236 >> Initializing global attention on CLS token...\n",
            " 18% 18012/100000 [49:18<2:56:09,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:45,360 >> Initializing global attention on CLS token...\n",
            " 18% 18013/100000 [49:18<2:52:54,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:45,481 >> Initializing global attention on CLS token...\n",
            " 18% 18014/100000 [49:18<2:53:59,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:45,614 >> Initializing global attention on CLS token...\n",
            " 18% 18015/100000 [49:18<2:56:57,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:45,744 >> Initializing global attention on CLS token...\n",
            " 18% 18016/100000 [49:18<2:55:46,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:45,876 >> Initializing global attention on CLS token...\n",
            " 18% 18017/100000 [49:19<2:58:35,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:46,007 >> Initializing global attention on CLS token...\n",
            " 18% 18018/100000 [49:19<2:54:15,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:46,127 >> Initializing global attention on CLS token...\n",
            " 18% 18019/100000 [49:19<2:56:23,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:46,260 >> Initializing global attention on CLS token...\n",
            " 18% 18020/100000 [49:19<2:54:21,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:46,384 >> Initializing global attention on CLS token...\n",
            " 18% 18021/100000 [49:19<2:51:52,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:46,506 >> Initializing global attention on CLS token...\n",
            " 18% 18022/100000 [49:19<2:53:13,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:46,640 >> Initializing global attention on CLS token...\n",
            " 18% 18023/100000 [49:19<2:54:44,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:46,765 >> Initializing global attention on CLS token...\n",
            " 18% 18024/100000 [49:19<2:54:56,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:46,898 >> Initializing global attention on CLS token...\n",
            " 18% 18025/100000 [49:20<2:55:24,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:47,022 >> Initializing global attention on CLS token...\n",
            " 18% 18026/100000 [49:20<2:52:23,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:47,147 >> Initializing global attention on CLS token...\n",
            " 18% 18027/100000 [49:20<2:54:42,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:47,275 >> Initializing global attention on CLS token...\n",
            " 18% 18028/100000 [49:20<2:53:59,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:47,402 >> Initializing global attention on CLS token...\n",
            " 18% 18029/100000 [49:20<2:54:03,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:47,533 >> Initializing global attention on CLS token...\n",
            " 18% 18030/100000 [49:20<2:54:10,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:47,657 >> Initializing global attention on CLS token...\n",
            " 18% 18031/100000 [49:20<2:55:08,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:47,791 >> Initializing global attention on CLS token...\n",
            " 18% 18032/100000 [49:20<2:55:20,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:47,916 >> Initializing global attention on CLS token...\n",
            " 18% 18033/100000 [49:21<2:52:26,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:48,037 >> Initializing global attention on CLS token...\n",
            " 18% 18034/100000 [49:21<2:55:06,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:48,171 >> Initializing global attention on CLS token...\n",
            " 18% 18035/100000 [49:21<2:53:06,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:48,293 >> Initializing global attention on CLS token...\n",
            " 18% 18036/100000 [49:21<2:52:55,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:48,421 >> Initializing global attention on CLS token...\n",
            " 18% 18037/100000 [49:21<2:54:59,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:48,555 >> Initializing global attention on CLS token...\n",
            " 18% 18038/100000 [49:21<2:54:36,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:48,677 >> Initializing global attention on CLS token...\n",
            " 18% 18039/100000 [49:21<2:55:00,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:48,810 >> Initializing global attention on CLS token...\n",
            " 18% 18040/100000 [49:22<2:55:02,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:48,935 >> Initializing global attention on CLS token...\n",
            " 18% 18041/100000 [49:22<2:52:11,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:49,056 >> Initializing global attention on CLS token...\n",
            " 18% 18042/100000 [49:22<2:52:59,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:49,184 >> Initializing global attention on CLS token...\n",
            " 18% 18043/100000 [49:22<2:50:28,  8.01it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:49,305 >> Initializing global attention on CLS token...\n",
            " 18% 18044/100000 [49:22<2:52:51,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:49,444 >> Initializing global attention on CLS token...\n",
            " 18% 18045/100000 [49:22<2:58:38,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:49,576 >> Initializing global attention on CLS token...\n",
            " 18% 18046/100000 [49:22<2:56:28,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:49,706 >> Initializing global attention on CLS token...\n",
            " 18% 18047/100000 [49:22<2:58:15,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:49,836 >> Initializing global attention on CLS token...\n",
            " 18% 18048/100000 [49:23<2:55:05,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:49,958 >> Initializing global attention on CLS token...\n",
            " 18% 18049/100000 [49:23<2:55:33,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:50,087 >> Initializing global attention on CLS token...\n",
            " 18% 18050/100000 [49:23<2:52:39,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:50,209 >> Initializing global attention on CLS token...\n",
            " 18% 18051/100000 [49:23<2:50:29,  8.01it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:50,330 >> Initializing global attention on CLS token...\n",
            " 18% 18052/100000 [49:23<2:51:06,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:50,461 >> Initializing global attention on CLS token...\n",
            " 18% 18053/100000 [49:23<2:53:51,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:50,588 >> Initializing global attention on CLS token...\n",
            " 18% 18054/100000 [49:23<2:54:16,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:50,721 >> Initializing global attention on CLS token...\n",
            " 18% 18055/100000 [49:23<2:56:02,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:50,848 >> Initializing global attention on CLS token...\n",
            " 18% 18056/100000 [49:24<2:52:59,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:50,974 >> Initializing global attention on CLS token...\n",
            " 18% 18057/100000 [49:24<2:54:49,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:51,101 >> Initializing global attention on CLS token...\n",
            " 18% 18058/100000 [49:24<2:51:35,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:51,221 >> Initializing global attention on CLS token...\n",
            " 18% 18059/100000 [49:24<2:51:18,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:51,350 >> Initializing global attention on CLS token...\n",
            " 18% 18060/100000 [49:24<2:52:14,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:51,474 >> Initializing global attention on CLS token...\n",
            " 18% 18061/100000 [49:24<2:52:31,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:51,604 >> Initializing global attention on CLS token...\n",
            " 18% 18062/100000 [49:24<2:53:47,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:51,732 >> Initializing global attention on CLS token...\n",
            " 18% 18063/100000 [49:24<2:52:19,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:51,853 >> Initializing global attention on CLS token...\n",
            " 18% 18064/100000 [49:25<2:54:06,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:51,986 >> Initializing global attention on CLS token...\n",
            " 18% 18065/100000 [49:25<2:53:43,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:52,115 >> Initializing global attention on CLS token...\n",
            " 18% 18066/100000 [49:25<2:54:54,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:52,241 >> Initializing global attention on CLS token...\n",
            " 18% 18067/100000 [49:25<2:52:57,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:52,364 >> Initializing global attention on CLS token...\n",
            " 18% 18068/100000 [49:25<2:54:20,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:52,495 >> Initializing global attention on CLS token...\n",
            " 18% 18069/100000 [49:25<2:54:49,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:52,628 >> Initializing global attention on CLS token...\n",
            " 18% 18070/100000 [49:25<2:56:49,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:52,756 >> Initializing global attention on CLS token...\n",
            " 18% 18071/100000 [49:25<2:54:13,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:52,879 >> Initializing global attention on CLS token...\n",
            " 18% 18072/100000 [49:26<2:55:33,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:53,010 >> Initializing global attention on CLS token...\n",
            " 18% 18073/100000 [49:26<2:52:39,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:53,131 >> Initializing global attention on CLS token...\n",
            " 18% 18074/100000 [49:26<2:50:08,  8.03it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:53,252 >> Initializing global attention on CLS token...\n",
            " 18% 18075/100000 [49:26<2:51:58,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:53,385 >> Initializing global attention on CLS token...\n",
            " 18% 18076/100000 [49:26<2:52:04,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:53,507 >> Initializing global attention on CLS token...\n",
            " 18% 18077/100000 [49:26<2:54:21,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:53,644 >> Initializing global attention on CLS token...\n",
            " 18% 18078/100000 [49:26<2:55:48,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:53,770 >> Initializing global attention on CLS token...\n",
            " 18% 18079/100000 [49:26<2:52:38,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:53,891 >> Initializing global attention on CLS token...\n",
            " 18% 18080/100000 [49:27<2:54:11,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:54,021 >> Initializing global attention on CLS token...\n",
            " 18% 18081/100000 [49:27<2:53:12,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:54,152 >> Initializing global attention on CLS token...\n",
            " 18% 18082/100000 [49:27<2:55:21,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:54,278 >> Initializing global attention on CLS token...\n",
            " 18% 18083/100000 [49:27<2:52:20,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:54,400 >> Initializing global attention on CLS token...\n",
            " 18% 18084/100000 [49:27<2:54:05,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:54,532 >> Initializing global attention on CLS token...\n",
            " 18% 18085/100000 [49:27<2:53:43,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:54,657 >> Initializing global attention on CLS token...\n",
            " 18% 18086/100000 [49:27<2:52:49,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:54,782 >> Initializing global attention on CLS token...\n",
            " 18% 18087/100000 [49:28<3:05:42,  7.35it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:54,944 >> Initializing global attention on CLS token...\n",
            " 18% 18088/100000 [49:28<3:01:51,  7.51it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:55,066 >> Initializing global attention on CLS token...\n",
            " 18% 18089/100000 [49:28<2:57:47,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:55,190 >> Initializing global attention on CLS token...\n",
            " 18% 18090/100000 [49:28<2:57:42,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:55,324 >> Initializing global attention on CLS token...\n",
            " 18% 18091/100000 [49:28<2:58:09,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:55,455 >> Initializing global attention on CLS token...\n",
            " 18% 18092/100000 [49:28<2:56:47,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:55,578 >> Initializing global attention on CLS token...\n",
            " 18% 18093/100000 [49:28<2:54:40,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:55,703 >> Initializing global attention on CLS token...\n",
            " 18% 18094/100000 [49:28<2:54:56,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:55,834 >> Initializing global attention on CLS token...\n",
            " 18% 18095/100000 [49:29<2:53:34,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:55,956 >> Initializing global attention on CLS token...\n",
            " 18% 18096/100000 [49:29<2:50:49,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:56,077 >> Initializing global attention on CLS token...\n",
            " 18% 18097/100000 [49:29<2:51:57,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:56,211 >> Initializing global attention on CLS token...\n",
            " 18% 18098/100000 [49:29<2:53:53,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:56,335 >> Initializing global attention on CLS token...\n",
            " 18% 18099/100000 [49:29<2:52:32,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:56,463 >> Initializing global attention on CLS token...\n",
            " 18% 18100/100000 [49:29<2:53:00,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:56,587 >> Initializing global attention on CLS token...\n",
            " 18% 18101/100000 [49:29<2:52:26,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:56,712 >> Initializing global attention on CLS token...\n",
            " 18% 18102/100000 [49:29<2:53:06,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:56,843 >> Initializing global attention on CLS token...\n",
            " 18% 18103/100000 [49:30<2:53:20,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:56,968 >> Initializing global attention on CLS token...\n",
            " 18% 18104/100000 [49:30<2:50:54,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:57,089 >> Initializing global attention on CLS token...\n",
            " 18% 18105/100000 [49:30<2:51:34,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:57,219 >> Initializing global attention on CLS token...\n",
            " 18% 18106/100000 [49:30<2:52:07,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:57,343 >> Initializing global attention on CLS token...\n",
            " 18% 18107/100000 [49:30<2:53:05,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:57,476 >> Initializing global attention on CLS token...\n",
            " 18% 18108/100000 [49:30<2:54:52,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:57,603 >> Initializing global attention on CLS token...\n",
            " 18% 18109/100000 [49:30<2:53:37,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:57,728 >> Initializing global attention on CLS token...\n",
            " 18% 18110/100000 [49:30<2:59:31,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:57,868 >> Initializing global attention on CLS token...\n",
            " 18% 18111/100000 [49:31<2:56:16,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:57,992 >> Initializing global attention on CLS token...\n",
            " 18% 18112/100000 [49:31<2:55:43,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:58,125 >> Initializing global attention on CLS token...\n",
            " 18% 18113/100000 [49:31<2:56:09,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:58,256 >> Initializing global attention on CLS token...\n",
            " 18% 18114/100000 [49:31<2:59:56,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:58,389 >> Initializing global attention on CLS token...\n",
            " 18% 18115/100000 [49:31<2:56:13,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:58,511 >> Initializing global attention on CLS token...\n",
            " 18% 18116/100000 [49:31<2:56:50,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:58,646 >> Initializing global attention on CLS token...\n",
            " 18% 18117/100000 [49:31<2:57:04,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:58,772 >> Initializing global attention on CLS token...\n",
            " 18% 18118/100000 [49:31<2:54:18,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:58,899 >> Initializing global attention on CLS token...\n",
            " 18% 18119/100000 [49:32<2:57:05,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:59,034 >> Initializing global attention on CLS token...\n",
            " 18% 18120/100000 [49:32<2:58:13,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:59,167 >> Initializing global attention on CLS token...\n",
            " 18% 18121/100000 [49:32<2:58:53,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:59,294 >> Initializing global attention on CLS token...\n",
            " 18% 18122/100000 [49:32<2:55:01,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:59,416 >> Initializing global attention on CLS token...\n",
            " 18% 18123/100000 [49:32<2:57:41,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:59,557 >> Initializing global attention on CLS token...\n",
            " 18% 18124/100000 [49:32<2:57:16,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:59,680 >> Initializing global attention on CLS token...\n",
            " 18% 18125/100000 [49:32<2:54:47,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:59,803 >> Initializing global attention on CLS token...\n",
            " 18% 18126/100000 [49:33<2:55:43,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:16:59,939 >> Initializing global attention on CLS token...\n",
            " 18% 18127/100000 [49:33<2:55:56,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:00,067 >> Initializing global attention on CLS token...\n",
            " 18% 18128/100000 [49:33<2:56:45,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:00,200 >> Initializing global attention on CLS token...\n",
            " 18% 18129/100000 [49:33<2:57:37,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:00,326 >> Initializing global attention on CLS token...\n",
            " 18% 18130/100000 [49:33<2:55:31,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:00,458 >> Initializing global attention on CLS token...\n",
            " 18% 18131/100000 [49:33<2:58:34,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:00,587 >> Initializing global attention on CLS token...\n",
            " 18% 18132/100000 [49:33<2:54:27,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:00,708 >> Initializing global attention on CLS token...\n",
            " 18% 18133/100000 [49:33<2:59:04,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:00,852 >> Initializing global attention on CLS token...\n",
            " 18% 18134/100000 [49:34<2:58:48,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:00,978 >> Initializing global attention on CLS token...\n",
            " 18% 18135/100000 [49:34<2:56:28,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:01,108 >> Initializing global attention on CLS token...\n",
            " 18% 18136/100000 [49:34<2:56:17,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:01,233 >> Initializing global attention on CLS token...\n",
            " 18% 18137/100000 [49:34<2:53:20,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:01,358 >> Initializing global attention on CLS token...\n",
            " 18% 18138/100000 [49:34<2:54:10,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:01,484 >> Initializing global attention on CLS token...\n",
            " 18% 18139/100000 [49:34<2:51:28,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:01,604 >> Initializing global attention on CLS token...\n",
            " 18% 18140/100000 [49:34<2:51:50,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:01,737 >> Initializing global attention on CLS token...\n",
            " 18% 18141/100000 [49:34<2:55:59,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:01,866 >> Initializing global attention on CLS token...\n",
            " 18% 18142/100000 [49:35<2:55:05,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:01,999 >> Initializing global attention on CLS token...\n",
            " 18% 18143/100000 [49:35<2:55:30,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:02,123 >> Initializing global attention on CLS token...\n",
            " 18% 18144/100000 [49:35<2:52:36,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:02,244 >> Initializing global attention on CLS token...\n",
            " 18% 18145/100000 [49:35<2:52:48,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:02,371 >> Initializing global attention on CLS token...\n",
            " 18% 18146/100000 [49:35<2:50:40,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:02,493 >> Initializing global attention on CLS token...\n",
            " 18% 18147/100000 [49:35<2:48:50,  8.08it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:02,613 >> Initializing global attention on CLS token...\n",
            " 18% 18148/100000 [49:35<2:49:36,  8.04it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:02,745 >> Initializing global attention on CLS token...\n",
            " 18% 18149/100000 [49:35<2:54:08,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:02,877 >> Initializing global attention on CLS token...\n",
            " 18% 18150/100000 [49:36<2:54:40,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:03,008 >> Initializing global attention on CLS token...\n",
            " 18% 18151/100000 [49:36<2:55:18,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:03,133 >> Initializing global attention on CLS token...\n",
            " 18% 18152/100000 [49:36<2:52:34,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:03,255 >> Initializing global attention on CLS token...\n",
            " 18% 18153/100000 [49:36<2:55:37,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:03,389 >> Initializing global attention on CLS token...\n",
            " 18% 18154/100000 [49:36<2:52:50,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:03,511 >> Initializing global attention on CLS token...\n",
            " 18% 18155/100000 [49:36<2:51:10,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:03,634 >> Initializing global attention on CLS token...\n",
            " 18% 18156/100000 [49:36<2:51:48,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:03,766 >> Initializing global attention on CLS token...\n",
            " 18% 18157/100000 [49:36<2:53:37,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:03,891 >> Initializing global attention on CLS token...\n",
            " 18% 18158/100000 [49:37<2:53:31,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:04,022 >> Initializing global attention on CLS token...\n",
            " 18% 18159/100000 [49:37<2:54:42,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:04,148 >> Initializing global attention on CLS token...\n",
            " 18% 18160/100000 [49:37<2:51:22,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:04,270 >> Initializing global attention on CLS token...\n",
            " 18% 18161/100000 [49:37<2:52:31,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:04,397 >> Initializing global attention on CLS token...\n",
            " 18% 18162/100000 [49:37<2:50:47,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:04,519 >> Initializing global attention on CLS token...\n",
            " 18% 18163/100000 [49:37<2:50:57,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:04,650 >> Initializing global attention on CLS token...\n",
            " 18% 18164/100000 [49:37<2:53:04,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:04,776 >> Initializing global attention on CLS token...\n",
            " 18% 18165/100000 [49:37<2:55:27,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:04,912 >> Initializing global attention on CLS token...\n",
            " 18% 18166/100000 [49:38<2:57:36,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:05,042 >> Initializing global attention on CLS token...\n",
            " 18% 18167/100000 [49:38<2:54:41,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:05,165 >> Initializing global attention on CLS token...\n",
            " 18% 18168/100000 [49:38<2:55:19,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:05,295 >> Initializing global attention on CLS token...\n",
            " 18% 18169/100000 [49:38<2:54:09,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:05,421 >> Initializing global attention on CLS token...\n",
            " 18% 18170/100000 [49:38<2:52:09,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:05,543 >> Initializing global attention on CLS token...\n",
            " 18% 18171/100000 [49:38<2:53:37,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:05,679 >> Initializing global attention on CLS token...\n",
            " 18% 18172/100000 [49:38<2:58:03,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:05,815 >> Initializing global attention on CLS token...\n",
            " 18% 18173/100000 [49:39<2:59:02,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:05,944 >> Initializing global attention on CLS token...\n",
            " 18% 18174/100000 [49:39<2:55:04,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:06,065 >> Initializing global attention on CLS token...\n",
            " 18% 18175/100000 [49:39<2:54:43,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:06,193 >> Initializing global attention on CLS token...\n",
            " 18% 18176/100000 [49:39<2:51:41,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:06,314 >> Initializing global attention on CLS token...\n",
            " 18% 18177/100000 [49:39<2:50:48,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:06,438 >> Initializing global attention on CLS token...\n",
            " 18% 18178/100000 [49:39<2:52:17,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:06,573 >> Initializing global attention on CLS token...\n",
            " 18% 18179/100000 [49:39<2:54:19,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:06,702 >> Initializing global attention on CLS token...\n",
            " 18% 18180/100000 [49:39<2:55:15,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:06,828 >> Initializing global attention on CLS token...\n",
            " 18% 18181/100000 [49:40<2:54:48,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:06,956 >> Initializing global attention on CLS token...\n",
            " 18% 18182/100000 [49:40<2:55:25,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:07,090 >> Initializing global attention on CLS token...\n",
            " 18% 18183/100000 [49:40<2:56:05,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:07,215 >> Initializing global attention on CLS token...\n",
            " 18% 18184/100000 [49:40<2:54:30,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:07,345 >> Initializing global attention on CLS token...\n",
            " 18% 18185/100000 [49:40<2:54:05,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:07,468 >> Initializing global attention on CLS token...\n",
            " 18% 18186/100000 [49:40<2:55:07,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:07,603 >> Initializing global attention on CLS token...\n",
            " 18% 18187/100000 [49:40<2:54:31,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:07,725 >> Initializing global attention on CLS token...\n",
            " 18% 18188/100000 [49:40<2:56:22,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:07,862 >> Initializing global attention on CLS token...\n",
            " 18% 18189/100000 [49:41<2:59:09,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:07,993 >> Initializing global attention on CLS token...\n",
            " 18% 18190/100000 [49:41<2:56:42,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:08,126 >> Initializing global attention on CLS token...\n",
            " 18% 18191/100000 [49:41<2:57:52,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:08,251 >> Initializing global attention on CLS token...\n",
            " 18% 18192/100000 [49:41<2:54:16,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:08,373 >> Initializing global attention on CLS token...\n",
            " 18% 18193/100000 [49:41<2:53:56,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:08,506 >> Initializing global attention on CLS token...\n",
            " 18% 18194/100000 [49:41<2:54:12,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:08,628 >> Initializing global attention on CLS token...\n",
            " 18% 18195/100000 [49:41<2:56:06,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:08,765 >> Initializing global attention on CLS token...\n",
            " 18% 18196/100000 [49:41<2:56:27,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:08,891 >> Initializing global attention on CLS token...\n",
            " 18% 18197/100000 [49:42<2:58:49,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:09,031 >> Initializing global attention on CLS token...\n",
            " 18% 18198/100000 [49:42<2:58:42,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:09,157 >> Initializing global attention on CLS token...\n",
            " 18% 18199/100000 [49:42<2:54:34,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:09,278 >> Initializing global attention on CLS token...\n",
            " 18% 18200/100000 [49:42<2:54:56,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:09,413 >> Initializing global attention on CLS token...\n",
            " 18% 18201/100000 [49:42<2:56:19,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:09,538 >> Initializing global attention on CLS token...\n",
            " 18% 18202/100000 [49:42<2:54:51,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:09,669 >> Initializing global attention on CLS token...\n",
            " 18% 18203/100000 [49:42<2:55:59,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:09,795 >> Initializing global attention on CLS token...\n",
            " 18% 18204/100000 [49:42<2:52:46,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:09,917 >> Initializing global attention on CLS token...\n",
            " 18% 18205/100000 [49:43<2:58:24,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:10,060 >> Initializing global attention on CLS token...\n",
            " 18% 18206/100000 [49:43<2:57:29,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:10,186 >> Initializing global attention on CLS token...\n",
            " 18% 18207/100000 [49:43<2:56:20,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:10,322 >> Initializing global attention on CLS token...\n",
            " 18% 18208/100000 [49:43<2:57:34,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:10,445 >> Initializing global attention on CLS token...\n",
            " 18% 18209/100000 [49:43<2:55:27,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:10,575 >> Initializing global attention on CLS token...\n",
            " 18% 18210/100000 [49:43<2:56:00,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:10,700 >> Initializing global attention on CLS token...\n",
            " 18% 18211/100000 [49:43<2:52:37,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:10,822 >> Initializing global attention on CLS token...\n",
            " 18% 18212/100000 [49:44<2:52:41,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:10,948 >> Initializing global attention on CLS token...\n",
            " 18% 18213/100000 [49:44<2:53:02,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:11,079 >> Initializing global attention on CLS token...\n",
            " 18% 18214/100000 [49:44<2:54:44,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:11,211 >> Initializing global attention on CLS token...\n",
            " 18% 18215/100000 [49:44<2:54:06,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:11,333 >> Initializing global attention on CLS token...\n",
            " 18% 18216/100000 [49:44<2:52:40,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:11,463 >> Initializing global attention on CLS token...\n",
            " 18% 18217/100000 [49:44<2:56:28,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:11,594 >> Initializing global attention on CLS token...\n",
            " 18% 18218/100000 [49:44<2:53:13,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:11,715 >> Initializing global attention on CLS token...\n",
            " 18% 18219/100000 [49:44<2:56:49,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:11,851 >> Initializing global attention on CLS token...\n",
            " 18% 18220/100000 [49:45<2:53:30,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:11,974 >> Initializing global attention on CLS token...\n",
            " 18% 18221/100000 [49:45<2:54:02,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:12,105 >> Initializing global attention on CLS token...\n",
            " 18% 18222/100000 [49:45<2:55:59,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:12,239 >> Initializing global attention on CLS token...\n",
            " 18% 18223/100000 [49:45<2:56:42,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:12,369 >> Initializing global attention on CLS token...\n",
            " 18% 18224/100000 [49:45<2:56:26,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:12,494 >> Initializing global attention on CLS token...\n",
            " 18% 18225/100000 [49:45<2:53:53,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:12,617 >> Initializing global attention on CLS token...\n",
            " 18% 18226/100000 [49:45<2:55:50,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:12,751 >> Initializing global attention on CLS token...\n",
            " 18% 18227/100000 [49:45<2:52:46,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:12,871 >> Initializing global attention on CLS token...\n",
            " 18% 18228/100000 [49:46<2:51:06,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:12,993 >> Initializing global attention on CLS token...\n",
            " 18% 18229/100000 [49:46<2:56:34,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:13,136 >> Initializing global attention on CLS token...\n",
            " 18% 18230/100000 [49:46<2:57:46,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:13,270 >> Initializing global attention on CLS token...\n",
            " 18% 18231/100000 [49:46<2:58:43,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:13,397 >> Initializing global attention on CLS token...\n",
            " 18% 18232/100000 [49:46<2:54:45,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:13,519 >> Initializing global attention on CLS token...\n",
            " 18% 18233/100000 [49:46<2:55:23,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:13,653 >> Initializing global attention on CLS token...\n",
            " 18% 18234/100000 [49:46<2:54:10,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:13,775 >> Initializing global attention on CLS token...\n",
            " 18% 18235/100000 [49:46<2:54:54,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:13,909 >> Initializing global attention on CLS token...\n",
            " 18% 18236/100000 [49:47<2:55:43,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:14,035 >> Initializing global attention on CLS token...\n",
            " 18% 18237/100000 [49:47<2:54:44,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:14,169 >> Initializing global attention on CLS token...\n",
            " 18% 18238/100000 [49:47<2:59:06,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:14,300 >> Initializing global attention on CLS token...\n",
            " 18% 18239/100000 [49:47<2:54:48,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:14,421 >> Initializing global attention on CLS token...\n",
            " 18% 18240/100000 [49:47<2:55:53,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:14,558 >> Initializing global attention on CLS token...\n",
            " 18% 18241/100000 [49:47<2:55:24,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:14,680 >> Initializing global attention on CLS token...\n",
            " 18% 18242/100000 [49:47<2:54:49,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:14,812 >> Initializing global attention on CLS token...\n",
            " 18% 18243/100000 [49:48<2:55:24,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:14,937 >> Initializing global attention on CLS token...\n",
            " 18% 18244/100000 [49:48<2:52:38,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:15,062 >> Initializing global attention on CLS token...\n",
            " 18% 18245/100000 [49:48<2:55:09,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:15,192 >> Initializing global attention on CLS token...\n",
            " 18% 18246/100000 [49:48<2:52:48,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:15,315 >> Initializing global attention on CLS token...\n",
            " 18% 18247/100000 [49:48<2:53:34,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:15,448 >> Initializing global attention on CLS token...\n",
            " 18% 18248/100000 [49:48<2:54:24,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:15,573 >> Initializing global attention on CLS token...\n",
            " 18% 18249/100000 [49:48<2:54:41,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:15,707 >> Initializing global attention on CLS token...\n",
            " 18% 18250/100000 [49:48<2:57:21,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:15,841 >> Initializing global attention on CLS token...\n",
            " 18% 18251/100000 [49:49<2:56:12,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:15,966 >> Initializing global attention on CLS token...\n",
            " 18% 18252/100000 [49:49<2:56:36,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:16,095 >> Initializing global attention on CLS token...\n",
            " 18% 18253/100000 [49:49<2:53:31,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:16,216 >> Initializing global attention on CLS token...\n",
            " 18% 18254/100000 [49:49<2:56:12,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:16,355 >> Initializing global attention on CLS token...\n",
            " 18% 18255/100000 [49:49<2:57:26,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:16,482 >> Initializing global attention on CLS token...\n",
            " 18% 18256/100000 [49:49<2:55:57,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:16,616 >> Initializing global attention on CLS token...\n",
            " 18% 18257/100000 [49:49<2:57:18,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:16,742 >> Initializing global attention on CLS token...\n",
            " 18% 18258/100000 [49:49<2:56:46,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:16,877 >> Initializing global attention on CLS token...\n",
            " 18% 18259/100000 [49:50<2:55:54,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:16,998 >> Initializing global attention on CLS token...\n",
            " 18% 18260/100000 [49:50<2:54:18,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:17,128 >> Initializing global attention on CLS token...\n",
            " 18% 18261/100000 [49:50<2:54:53,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:17,256 >> Initializing global attention on CLS token...\n",
            " 18% 18262/100000 [49:50<2:54:33,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:17,385 >> Initializing global attention on CLS token...\n",
            " 18% 18263/100000 [49:50<2:55:40,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:17,511 >> Initializing global attention on CLS token...\n",
            " 18% 18264/100000 [49:50<2:52:46,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:17,633 >> Initializing global attention on CLS token...\n",
            " 18% 18265/100000 [49:50<2:53:02,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:17,764 >> Initializing global attention on CLS token...\n",
            " 18% 18266/100000 [49:50<2:53:16,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:17,888 >> Initializing global attention on CLS token...\n",
            " 18% 18267/100000 [49:51<2:52:11,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:18,017 >> Initializing global attention on CLS token...\n",
            " 18% 18268/100000 [49:51<2:55:02,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:18,146 >> Initializing global attention on CLS token...\n",
            " 18% 18269/100000 [49:51<2:52:01,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:18,267 >> Initializing global attention on CLS token...\n",
            " 18% 18270/100000 [49:51<2:54:05,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:18,403 >> Initializing global attention on CLS token...\n",
            " 18% 18271/100000 [49:51<2:54:22,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:18,527 >> Initializing global attention on CLS token...\n",
            " 18% 18272/100000 [49:51<2:52:14,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:18,651 >> Initializing global attention on CLS token...\n",
            " 18% 18273/100000 [49:51<2:52:28,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:18,781 >> Initializing global attention on CLS token...\n",
            " 18% 18274/100000 [49:51<2:53:13,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:18,905 >> Initializing global attention on CLS token...\n",
            " 18% 18275/100000 [49:52<2:52:22,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:19,038 >> Initializing global attention on CLS token...\n",
            " 18% 18276/100000 [49:52<2:54:38,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:19,162 >> Initializing global attention on CLS token...\n",
            " 18% 18277/100000 [49:52<2:51:38,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:19,283 >> Initializing global attention on CLS token...\n",
            " 18% 18278/100000 [49:52<2:55:52,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:19,419 >> Initializing global attention on CLS token...\n",
            " 18% 18279/100000 [49:52<2:51:59,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:19,541 >> Initializing global attention on CLS token...\n",
            " 18% 18280/100000 [49:52<2:50:33,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:19,662 >> Initializing global attention on CLS token...\n",
            " 18% 18281/100000 [49:52<2:51:21,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:19,796 >> Initializing global attention on CLS token...\n",
            " 18% 18282/100000 [49:53<2:53:43,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:19,921 >> Initializing global attention on CLS token...\n",
            " 18% 18283/100000 [49:53<2:54:19,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:20,054 >> Initializing global attention on CLS token...\n",
            " 18% 18284/100000 [49:53<2:54:41,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:20,179 >> Initializing global attention on CLS token...\n",
            " 18% 18285/100000 [49:53<2:55:20,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:20,313 >> Initializing global attention on CLS token...\n",
            " 18% 18286/100000 [49:53<2:57:51,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:20,443 >> Initializing global attention on CLS token...\n",
            " 18% 18287/100000 [49:53<2:54:00,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:20,565 >> Initializing global attention on CLS token...\n",
            " 18% 18288/100000 [49:53<2:53:25,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:20,697 >> Initializing global attention on CLS token...\n",
            " 18% 18289/100000 [49:53<2:54:05,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:20,820 >> Initializing global attention on CLS token...\n",
            " 18% 18290/100000 [49:54<2:53:33,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:20,951 >> Initializing global attention on CLS token...\n",
            " 18% 18291/100000 [49:54<2:55:19,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:21,078 >> Initializing global attention on CLS token...\n",
            " 18% 18292/100000 [49:54<2:52:15,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:21,204 >> Initializing global attention on CLS token...\n",
            " 18% 18293/100000 [49:54<2:56:24,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:21,337 >> Initializing global attention on CLS token...\n",
            " 18% 18294/100000 [49:54<2:56:55,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:21,472 >> Initializing global attention on CLS token...\n",
            " 18% 18295/100000 [49:54<2:57:45,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:21,600 >> Initializing global attention on CLS token...\n",
            " 18% 18296/100000 [49:54<2:58:06,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:21,735 >> Initializing global attention on CLS token...\n",
            " 18% 18297/100000 [49:54<2:58:21,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:21,862 >> Initializing global attention on CLS token...\n",
            " 18% 18298/100000 [49:55<2:54:45,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:21,984 >> Initializing global attention on CLS token...\n",
            " 18% 18299/100000 [49:55<2:56:10,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:22,121 >> Initializing global attention on CLS token...\n",
            " 18% 18300/100000 [49:55<2:55:46,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:22,244 >> Initializing global attention on CLS token...\n",
            " 18% 18301/100000 [49:55<2:54:37,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:22,376 >> Initializing global attention on CLS token...\n",
            " 18% 18302/100000 [49:55<2:58:07,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:22,508 >> Initializing global attention on CLS token...\n",
            " 18% 18303/100000 [49:55<2:59:46,  7.57it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:22,643 >> Initializing global attention on CLS token...\n",
            " 18% 18304/100000 [49:55<2:56:01,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:22,765 >> Initializing global attention on CLS token...\n",
            " 18% 18305/100000 [49:55<2:52:46,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:22,886 >> Initializing global attention on CLS token...\n",
            " 18% 18306/100000 [49:56<2:52:38,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:23,019 >> Initializing global attention on CLS token...\n",
            " 18% 18307/100000 [49:56<2:54:24,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:23,144 >> Initializing global attention on CLS token...\n",
            " 18% 18308/100000 [49:56<2:54:15,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:23,278 >> Initializing global attention on CLS token...\n",
            " 18% 18309/100000 [49:56<2:56:16,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:23,405 >> Initializing global attention on CLS token...\n",
            " 18% 18310/100000 [49:56<2:55:48,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:23,540 >> Initializing global attention on CLS token...\n",
            " 18% 18311/100000 [49:56<3:02:42,  7.45it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:23,679 >> Initializing global attention on CLS token...\n",
            " 18% 18312/100000 [49:56<2:57:06,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:23,799 >> Initializing global attention on CLS token...\n",
            " 18% 18313/100000 [49:57<2:59:49,  7.57it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:23,941 >> Initializing global attention on CLS token...\n",
            " 18% 18314/100000 [49:57<2:57:39,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:24,063 >> Initializing global attention on CLS token...\n",
            " 18% 18315/100000 [49:57<2:54:49,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:24,190 >> Initializing global attention on CLS token...\n",
            " 18% 18316/100000 [49:57<2:57:02,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:24,326 >> Initializing global attention on CLS token...\n",
            " 18% 18317/100000 [49:57<2:56:37,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:24,449 >> Initializing global attention on CLS token...\n",
            " 18% 18318/100000 [49:57<2:57:29,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:24,585 >> Initializing global attention on CLS token...\n",
            " 18% 18319/100000 [49:57<2:58:22,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:24,714 >> Initializing global attention on CLS token...\n",
            " 18% 18320/100000 [49:57<2:54:01,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:24,838 >> Initializing global attention on CLS token...\n",
            " 18% 18321/100000 [49:58<2:56:07,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:24,972 >> Initializing global attention on CLS token...\n",
            " 18% 18322/100000 [49:58<3:03:38,  7.41it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:25,122 >> Initializing global attention on CLS token...\n",
            " 18% 18323/100000 [49:58<3:05:45,  7.33it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:25,257 >> Initializing global attention on CLS token...\n",
            " 18% 18324/100000 [49:58<3:01:58,  7.48it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:25,387 >> Initializing global attention on CLS token...\n",
            " 18% 18325/100000 [49:58<2:59:16,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:25,510 >> Initializing global attention on CLS token...\n",
            " 18% 18326/100000 [49:58<2:57:12,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:25,637 >> Initializing global attention on CLS token...\n",
            " 18% 18327/100000 [49:58<2:58:01,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:25,769 >> Initializing global attention on CLS token...\n",
            " 18% 18328/100000 [49:58<2:54:12,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:25,893 >> Initializing global attention on CLS token...\n",
            " 18% 18329/100000 [49:59<2:52:52,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:26,014 >> Initializing global attention on CLS token...\n",
            " 18% 18330/100000 [49:59<2:52:44,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:26,147 >> Initializing global attention on CLS token...\n",
            " 18% 18331/100000 [49:59<2:52:55,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:26,268 >> Initializing global attention on CLS token...\n",
            " 18% 18332/100000 [49:59<2:52:56,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:26,407 >> Initializing global attention on CLS token...\n",
            " 18% 18333/100000 [49:59<2:57:58,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:26,535 >> Initializing global attention on CLS token...\n",
            " 18% 18334/100000 [49:59<2:59:58,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:26,676 >> Initializing global attention on CLS token...\n",
            " 18% 18335/100000 [49:59<2:58:47,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:26,800 >> Initializing global attention on CLS token...\n",
            " 18% 18336/100000 [50:00<2:54:41,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:26,921 >> Initializing global attention on CLS token...\n",
            " 18% 18337/100000 [50:00<2:55:07,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:27,057 >> Initializing global attention on CLS token...\n",
            " 18% 18338/100000 [50:00<2:56:06,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:27,182 >> Initializing global attention on CLS token...\n",
            " 18% 18339/100000 [50:00<2:55:06,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:27,314 >> Initializing global attention on CLS token...\n",
            " 18% 18340/100000 [50:00<2:57:37,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:27,443 >> Initializing global attention on CLS token...\n",
            " 18% 18341/100000 [50:00<2:54:27,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:27,572 >> Initializing global attention on CLS token...\n",
            " 18% 18342/100000 [50:00<3:00:06,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:27,708 >> Initializing global attention on CLS token...\n",
            " 18% 18343/100000 [50:00<2:55:37,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:27,830 >> Initializing global attention on CLS token...\n",
            " 18% 18344/100000 [50:01<2:55:03,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:27,964 >> Initializing global attention on CLS token...\n",
            " 18% 18345/100000 [50:01<2:56:22,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:28,089 >> Initializing global attention on CLS token...\n",
            " 18% 18346/100000 [50:01<2:56:50,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:28,225 >> Initializing global attention on CLS token...\n",
            " 18% 18347/100000 [50:01<2:56:33,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:28,349 >> Initializing global attention on CLS token...\n",
            " 18% 18348/100000 [50:01<2:55:54,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:28,482 >> Initializing global attention on CLS token...\n",
            " 18% 18349/100000 [50:01<2:56:19,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:28,608 >> Initializing global attention on CLS token...\n",
            " 18% 18350/100000 [50:01<2:55:01,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:28,735 >> Initializing global attention on CLS token...\n",
            " 18% 18351/100000 [50:01<2:55:08,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:28,868 >> Initializing global attention on CLS token...\n",
            " 18% 18352/100000 [50:02<2:55:13,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:28,992 >> Initializing global attention on CLS token...\n",
            " 18% 18353/100000 [50:02<2:54:19,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:29,123 >> Initializing global attention on CLS token...\n",
            " 18% 18354/100000 [50:02<2:54:31,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:29,247 >> Initializing global attention on CLS token...\n",
            " 18% 18355/100000 [50:02<2:54:43,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:29,379 >> Initializing global attention on CLS token...\n",
            " 18% 18356/100000 [50:02<2:53:53,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:29,503 >> Initializing global attention on CLS token...\n",
            " 18% 18357/100000 [50:02<2:51:06,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:29,623 >> Initializing global attention on CLS token...\n",
            " 18% 18358/100000 [50:02<2:54:04,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:29,761 >> Initializing global attention on CLS token...\n",
            " 18% 18359/100000 [50:02<2:54:53,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:29,886 >> Initializing global attention on CLS token...\n",
            " 18% 18360/100000 [50:03<2:54:32,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:30,018 >> Initializing global attention on CLS token...\n",
            " 18% 18361/100000 [50:03<2:55:11,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:30,144 >> Initializing global attention on CLS token...\n",
            " 18% 18362/100000 [50:03<2:52:19,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:30,266 >> Initializing global attention on CLS token...\n",
            " 18% 18363/100000 [50:03<2:52:59,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:30,401 >> Initializing global attention on CLS token...\n",
            " 18% 18364/100000 [50:03<2:53:17,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:30,522 >> Initializing global attention on CLS token...\n",
            " 18% 18365/100000 [50:03<2:51:02,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:30,645 >> Initializing global attention on CLS token...\n",
            " 18% 18366/100000 [50:03<2:57:12,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:30,790 >> Initializing global attention on CLS token...\n",
            " 18% 18367/100000 [50:03<2:57:42,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:30,916 >> Initializing global attention on CLS token...\n",
            " 18% 18368/100000 [50:04<2:54:30,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:31,039 >> Initializing global attention on CLS token...\n",
            " 18% 18369/100000 [50:04<2:54:02,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:31,170 >> Initializing global attention on CLS token...\n",
            " 18% 18370/100000 [50:04<2:54:46,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:31,303 >> Initializing global attention on CLS token...\n",
            " 18% 18371/100000 [50:04<2:57:49,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:31,432 >> Initializing global attention on CLS token...\n",
            " 18% 18372/100000 [50:04<2:54:00,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:31,553 >> Initializing global attention on CLS token...\n",
            " 18% 18373/100000 [50:04<2:55:18,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:31,690 >> Initializing global attention on CLS token...\n",
            " 18% 18374/100000 [50:04<2:56:28,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:31,821 >> Initializing global attention on CLS token...\n",
            " 18% 18375/100000 [50:05<3:00:38,  7.53it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:31,960 >> Initializing global attention on CLS token...\n",
            " 18% 18376/100000 [50:05<3:00:15,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:32,090 >> Initializing global attention on CLS token...\n",
            " 18% 18377/100000 [50:05<2:58:59,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:32,217 >> Initializing global attention on CLS token...\n",
            " 18% 18378/100000 [50:05<2:54:58,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:32,339 >> Initializing global attention on CLS token...\n",
            " 18% 18379/100000 [50:05<2:56:16,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:32,475 >> Initializing global attention on CLS token...\n",
            " 18% 18380/100000 [50:05<2:56:22,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:32,600 >> Initializing global attention on CLS token...\n",
            " 18% 18381/100000 [50:05<2:55:06,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:32,734 >> Initializing global attention on CLS token...\n",
            " 18% 18382/100000 [50:05<2:58:26,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:32,864 >> Initializing global attention on CLS token...\n",
            " 18% 18383/100000 [50:06<2:54:18,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:32,986 >> Initializing global attention on CLS token...\n",
            " 18% 18384/100000 [50:06<2:55:05,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:33,115 >> Initializing global attention on CLS token...\n",
            " 18% 18385/100000 [50:06<2:53:19,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:33,242 >> Initializing global attention on CLS token...\n",
            " 18% 18386/100000 [50:06<2:51:50,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:33,364 >> Initializing global attention on CLS token...\n",
            " 18% 18387/100000 [50:06<2:54:13,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:33,501 >> Initializing global attention on CLS token...\n",
            " 18% 18388/100000 [50:06<2:54:45,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:33,625 >> Initializing global attention on CLS token...\n",
            " 18% 18389/100000 [50:06<2:54:58,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:33,759 >> Initializing global attention on CLS token...\n",
            " 18% 18390/100000 [50:06<2:57:19,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:33,891 >> Initializing global attention on CLS token...\n",
            " 18% 18391/100000 [50:07<2:57:58,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:34,025 >> Initializing global attention on CLS token...\n",
            " 18% 18392/100000 [50:07<2:57:20,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:34,149 >> Initializing global attention on CLS token...\n",
            " 18% 18393/100000 [50:07<2:53:50,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:34,271 >> Initializing global attention on CLS token...\n",
            " 18% 18394/100000 [50:07<2:55:14,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:34,408 >> Initializing global attention on CLS token...\n",
            " 18% 18395/100000 [50:07<2:55:11,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:34,532 >> Initializing global attention on CLS token...\n",
            " 18% 18396/100000 [50:07<2:54:54,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:34,663 >> Initializing global attention on CLS token...\n",
            " 18% 18397/100000 [50:07<2:55:53,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:34,791 >> Initializing global attention on CLS token...\n",
            " 18% 18398/100000 [50:08<2:56:52,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:34,926 >> Initializing global attention on CLS token...\n",
            " 18% 18399/100000 [50:08<2:55:51,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:35,050 >> Initializing global attention on CLS token...\n",
            " 18% 18400/100000 [50:08<2:52:27,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:35,171 >> Initializing global attention on CLS token...\n",
            " 18% 18401/100000 [50:08<2:54:02,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:35,307 >> Initializing global attention on CLS token...\n",
            " 18% 18402/100000 [50:08<2:54:15,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:35,430 >> Initializing global attention on CLS token...\n",
            " 18% 18403/100000 [50:08<2:54:44,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:35,564 >> Initializing global attention on CLS token...\n",
            " 18% 18404/100000 [50:08<2:55:54,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:35,690 >> Initializing global attention on CLS token...\n",
            " 18% 18405/100000 [50:08<2:52:34,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:35,816 >> Initializing global attention on CLS token...\n",
            " 18% 18406/100000 [50:09<3:01:53,  7.48it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:35,961 >> Initializing global attention on CLS token...\n",
            " 18% 18407/100000 [50:09<2:56:54,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:36,082 >> Initializing global attention on CLS token...\n",
            " 18% 18408/100000 [50:09<2:56:10,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:36,215 >> Initializing global attention on CLS token...\n",
            " 18% 18409/100000 [50:09<2:57:20,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:36,347 >> Initializing global attention on CLS token...\n",
            " 18% 18410/100000 [50:09<2:56:39,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:36,472 >> Initializing global attention on CLS token...\n",
            " 18% 18411/100000 [50:09<2:52:55,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:36,593 >> Initializing global attention on CLS token...\n",
            " 18% 18412/100000 [50:09<2:53:54,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:36,728 >> Initializing global attention on CLS token...\n",
            " 18% 18413/100000 [50:09<2:54:11,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:36,851 >> Initializing global attention on CLS token...\n",
            " 18% 18414/100000 [50:10<2:54:38,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:36,986 >> Initializing global attention on CLS token...\n",
            " 18% 18415/100000 [50:10<2:55:43,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:37,111 >> Initializing global attention on CLS token...\n",
            " 18% 18416/100000 [50:10<2:53:32,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:37,240 >> Initializing global attention on CLS token...\n",
            " 18% 18417/100000 [50:10<2:55:22,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:37,367 >> Initializing global attention on CLS token...\n",
            " 18% 18418/100000 [50:10<2:51:48,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:37,489 >> Initializing global attention on CLS token...\n",
            " 18% 18419/100000 [50:10<2:54:44,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:37,627 >> Initializing global attention on CLS token...\n",
            " 18% 18420/100000 [50:10<2:55:41,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:37,753 >> Initializing global attention on CLS token...\n",
            " 18% 18421/100000 [50:10<2:54:24,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:37,883 >> Initializing global attention on CLS token...\n",
            " 18% 18422/100000 [50:11<2:59:01,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:38,018 >> Initializing global attention on CLS token...\n",
            " 18% 18423/100000 [50:11<2:58:02,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:38,150 >> Initializing global attention on CLS token...\n",
            " 18% 18424/100000 [50:11<2:54:51,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:38,271 >> Initializing global attention on CLS token...\n",
            " 18% 18425/100000 [50:11<2:51:25,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:38,391 >> Initializing global attention on CLS token...\n",
            " 18% 18426/100000 [50:11<2:58:31,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:38,540 >> Initializing global attention on CLS token...\n",
            " 18% 18427/100000 [50:11<2:57:47,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:38,664 >> Initializing global attention on CLS token...\n",
            " 18% 18428/100000 [50:11<2:56:58,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:38,797 >> Initializing global attention on CLS token...\n",
            " 18% 18429/100000 [50:12<2:56:24,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:38,921 >> Initializing global attention on CLS token...\n",
            " 18% 18430/100000 [50:12<2:54:31,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:39,052 >> Initializing global attention on CLS token...\n",
            " 18% 18431/100000 [50:12<2:56:08,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:39,179 >> Initializing global attention on CLS token...\n",
            " 18% 18432/100000 [50:12<2:52:52,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:39,301 >> Initializing global attention on CLS token...\n",
            " 18% 18433/100000 [50:12<2:53:34,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:39,434 >> Initializing global attention on CLS token...\n",
            " 18% 18434/100000 [50:12<2:56:27,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:39,569 >> Initializing global attention on CLS token...\n",
            " 18% 18435/100000 [50:12<3:01:23,  7.49it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:39,706 >> Initializing global attention on CLS token...\n",
            " 18% 18436/100000 [50:12<2:56:25,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:39,827 >> Initializing global attention on CLS token...\n",
            " 18% 18437/100000 [50:13<2:56:43,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:39,958 >> Initializing global attention on CLS token...\n",
            " 18% 18438/100000 [50:13<2:56:37,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:40,091 >> Initializing global attention on CLS token...\n",
            " 18% 18439/100000 [50:13<2:55:34,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:40,215 >> Initializing global attention on CLS token...\n",
            " 18% 18440/100000 [50:13<2:56:28,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:40,351 >> Initializing global attention on CLS token...\n",
            " 18% 18441/100000 [50:13<2:56:47,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:40,477 >> Initializing global attention on CLS token...\n",
            " 18% 18442/100000 [50:13<2:56:02,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:40,609 >> Initializing global attention on CLS token...\n",
            " 18% 18443/100000 [50:13<2:56:09,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:40,735 >> Initializing global attention on CLS token...\n",
            " 18% 18444/100000 [50:13<2:57:46,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:40,875 >> Initializing global attention on CLS token...\n",
            " 18% 18445/100000 [50:14<2:58:48,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:41,007 >> Initializing global attention on CLS token...\n",
            " 18% 18446/100000 [50:14<2:58:52,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:41,133 >> Initializing global attention on CLS token...\n",
            " 18% 18447/100000 [50:14<2:55:06,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:41,258 >> Initializing global attention on CLS token...\n",
            " 18% 18448/100000 [50:14<2:56:45,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:41,394 >> Initializing global attention on CLS token...\n",
            " 18% 18449/100000 [50:14<2:57:39,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:41,527 >> Initializing global attention on CLS token...\n",
            " 18% 18450/100000 [50:14<2:59:18,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:41,656 >> Initializing global attention on CLS token...\n",
            " 18% 18451/100000 [50:14<2:54:27,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:41,780 >> Initializing global attention on CLS token...\n",
            " 18% 18452/100000 [50:14<2:55:48,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:41,907 >> Initializing global attention on CLS token...\n",
            " 18% 18453/100000 [50:15<2:54:35,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:42,039 >> Initializing global attention on CLS token...\n",
            " 18% 18454/100000 [50:15<2:55:49,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:42,165 >> Initializing global attention on CLS token...\n",
            " 18% 18455/100000 [50:15<2:52:38,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:42,287 >> Initializing global attention on CLS token...\n",
            " 18% 18456/100000 [50:15<2:54:44,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:42,419 >> Initializing global attention on CLS token...\n",
            " 18% 18457/100000 [50:15<2:54:46,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:42,553 >> Initializing global attention on CLS token...\n",
            " 18% 18458/100000 [50:15<2:57:27,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:42,683 >> Initializing global attention on CLS token...\n",
            " 18% 18459/100000 [50:15<2:55:41,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:42,816 >> Initializing global attention on CLS token...\n",
            " 18% 18460/100000 [50:16<3:00:36,  7.52it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:42,951 >> Initializing global attention on CLS token...\n",
            " 18% 18461/100000 [50:16<2:56:09,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:43,073 >> Initializing global attention on CLS token...\n",
            " 18% 18462/100000 [50:16<2:57:38,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:43,210 >> Initializing global attention on CLS token...\n",
            " 18% 18463/100000 [50:16<3:00:11,  7.54it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:43,347 >> Initializing global attention on CLS token...\n",
            " 18% 18464/100000 [50:16<2:59:06,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:43,473 >> Initializing global attention on CLS token...\n",
            " 18% 18465/100000 [50:16<2:55:44,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:43,601 >> Initializing global attention on CLS token...\n",
            " 18% 18466/100000 [50:16<2:58:01,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:43,731 >> Initializing global attention on CLS token...\n",
            " 18% 18467/100000 [50:16<2:56:33,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:43,863 >> Initializing global attention on CLS token...\n",
            " 18% 18468/100000 [50:17<2:57:00,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:43,991 >> Initializing global attention on CLS token...\n",
            " 18% 18469/100000 [50:17<2:57:24,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:44,125 >> Initializing global attention on CLS token...\n",
            " 18% 18470/100000 [50:17<2:57:49,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:44,253 >> Initializing global attention on CLS token...\n",
            " 18% 18471/100000 [50:17<2:54:32,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:44,379 >> Initializing global attention on CLS token...\n",
            " 18% 18472/100000 [50:17<2:55:18,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:44,512 >> Initializing global attention on CLS token...\n",
            " 18% 18473/100000 [50:17<2:55:32,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:44,635 >> Initializing global attention on CLS token...\n",
            " 18% 18474/100000 [50:17<2:54:07,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:44,767 >> Initializing global attention on CLS token...\n",
            " 18% 18475/100000 [50:17<2:56:29,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:44,899 >> Initializing global attention on CLS token...\n",
            " 18% 18476/100000 [50:18<2:56:23,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:45,029 >> Initializing global attention on CLS token...\n",
            " 18% 18477/100000 [50:18<2:56:20,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:45,154 >> Initializing global attention on CLS token...\n",
            " 18% 18478/100000 [50:18<2:54:59,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:45,281 >> Initializing global attention on CLS token...\n",
            " 18% 18479/100000 [50:18<2:56:36,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:45,419 >> Initializing global attention on CLS token...\n",
            " 18% 18480/100000 [50:18<2:56:11,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:45,542 >> Initializing global attention on CLS token...\n",
            " 18% 18481/100000 [50:18<2:56:05,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:45,676 >> Initializing global attention on CLS token...\n",
            " 18% 18482/100000 [50:18<2:55:27,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:45,800 >> Initializing global attention on CLS token...\n",
            " 18% 18483/100000 [50:19<2:52:05,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:45,926 >> Initializing global attention on CLS token...\n",
            " 18% 18484/100000 [50:19<2:56:02,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:46,062 >> Initializing global attention on CLS token...\n",
            " 18% 18485/100000 [50:19<2:55:12,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:46,185 >> Initializing global attention on CLS token...\n",
            " 18% 18486/100000 [50:19<2:54:50,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:46,318 >> Initializing global attention on CLS token...\n",
            " 18% 18487/100000 [50:19<2:54:23,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:46,442 >> Initializing global attention on CLS token...\n",
            " 18% 18488/100000 [50:19<2:54:28,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:46,574 >> Initializing global attention on CLS token...\n",
            " 18% 18489/100000 [50:19<2:57:24,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:46,705 >> Initializing global attention on CLS token...\n",
            " 18% 18490/100000 [50:19<2:54:07,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:46,833 >> Initializing global attention on CLS token...\n",
            " 18% 18491/100000 [50:20<2:57:18,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:46,963 >> Initializing global attention on CLS token...\n",
            " 18% 18492/100000 [50:20<2:53:53,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:47,085 >> Initializing global attention on CLS token...\n",
            " 18% 18493/100000 [50:20<2:54:18,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:47,219 >> Initializing global attention on CLS token...\n",
            " 18% 18494/100000 [50:20<2:55:36,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:47,346 >> Initializing global attention on CLS token...\n",
            " 18% 18495/100000 [50:20<2:55:42,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:47,481 >> Initializing global attention on CLS token...\n",
            " 18% 18496/100000 [50:20<2:56:52,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:47,608 >> Initializing global attention on CLS token...\n",
            " 18% 18497/100000 [50:20<2:53:48,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:47,735 >> Initializing global attention on CLS token...\n",
            " 18% 18498/100000 [50:20<2:55:09,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:47,862 >> Initializing global attention on CLS token...\n",
            " 18% 18499/100000 [50:21<2:52:10,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:47,983 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.4367, 'learning_rate': 8.150500000000001e-05, 'epoch': 3.7}\n",
            " 18% 18500/100000 [50:21<2:54:45,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:48,123 >> Initializing global attention on CLS token...\n",
            " 19% 18501/100000 [50:21<2:55:43,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:48,248 >> Initializing global attention on CLS token...\n",
            " 19% 18502/100000 [50:21<2:58:30,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:48,389 >> Initializing global attention on CLS token...\n",
            " 19% 18503/100000 [50:21<2:58:28,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:48,515 >> Initializing global attention on CLS token...\n",
            " 19% 18504/100000 [50:21<2:54:29,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:48,641 >> Initializing global attention on CLS token...\n",
            " 19% 18505/100000 [50:21<2:56:39,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:48,771 >> Initializing global attention on CLS token...\n",
            " 19% 18506/100000 [50:21<2:53:47,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:48,894 >> Initializing global attention on CLS token...\n",
            " 19% 18507/100000 [50:22<2:53:52,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:49,027 >> Initializing global attention on CLS token...\n",
            " 19% 18508/100000 [50:22<2:54:43,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:49,152 >> Initializing global attention on CLS token...\n",
            " 19% 18509/100000 [50:22<2:53:02,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:49,283 >> Initializing global attention on CLS token...\n",
            " 19% 18510/100000 [50:22<2:56:05,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:49,415 >> Initializing global attention on CLS token...\n",
            " 19% 18511/100000 [50:22<2:55:02,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:49,540 >> Initializing global attention on CLS token...\n",
            " 19% 18512/100000 [50:22<2:58:16,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:49,677 >> Initializing global attention on CLS token...\n",
            " 19% 18513/100000 [50:22<2:55:17,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:49,799 >> Initializing global attention on CLS token...\n",
            " 19% 18514/100000 [50:23<2:54:06,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:49,931 >> Initializing global attention on CLS token...\n",
            " 19% 18515/100000 [50:23<2:55:07,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:50,056 >> Initializing global attention on CLS token...\n",
            " 19% 18516/100000 [50:23<2:55:42,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:50,190 >> Initializing global attention on CLS token...\n",
            " 19% 18517/100000 [50:23<2:56:11,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:50,317 >> Initializing global attention on CLS token...\n",
            " 19% 18518/100000 [50:23<2:54:06,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:50,456 >> Initializing global attention on CLS token...\n",
            " 19% 18519/100000 [50:23<2:59:51,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:50,584 >> Initializing global attention on CLS token...\n",
            " 19% 18520/100000 [50:23<2:55:53,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:50,707 >> Initializing global attention on CLS token...\n",
            " 19% 18521/100000 [50:23<2:54:40,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:50,839 >> Initializing global attention on CLS token...\n",
            " 19% 18522/100000 [50:24<2:58:41,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:50,981 >> Initializing global attention on CLS token...\n",
            " 19% 18523/100000 [50:24<2:58:05,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:51,107 >> Initializing global attention on CLS token...\n",
            " 19% 18524/100000 [50:24<2:54:01,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:51,229 >> Initializing global attention on CLS token...\n",
            " 19% 18525/100000 [50:24<2:53:52,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:51,360 >> Initializing global attention on CLS token...\n",
            " 19% 18526/100000 [50:24<2:54:41,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:51,486 >> Initializing global attention on CLS token...\n",
            " 19% 18527/100000 [50:24<2:51:23,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:51,607 >> Initializing global attention on CLS token...\n",
            " 19% 18528/100000 [50:24<2:52:46,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:51,741 >> Initializing global attention on CLS token...\n",
            " 19% 18529/100000 [50:24<2:52:50,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:51,863 >> Initializing global attention on CLS token...\n",
            " 19% 18530/100000 [50:25<2:52:42,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:51,996 >> Initializing global attention on CLS token...\n",
            " 19% 18531/100000 [50:25<2:54:17,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:52,122 >> Initializing global attention on CLS token...\n",
            " 19% 18532/100000 [50:25<2:51:20,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:52,243 >> Initializing global attention on CLS token...\n",
            " 19% 18533/100000 [50:25<2:52:21,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:52,375 >> Initializing global attention on CLS token...\n",
            " 19% 18534/100000 [50:25<2:53:11,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:52,503 >> Initializing global attention on CLS token...\n",
            " 19% 18535/100000 [50:25<2:51:13,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:52,623 >> Initializing global attention on CLS token...\n",
            " 19% 18536/100000 [50:25<2:53:20,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:52,759 >> Initializing global attention on CLS token...\n",
            " 19% 18537/100000 [50:25<2:53:21,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:52,882 >> Initializing global attention on CLS token...\n",
            " 19% 18538/100000 [50:26<2:51:49,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:53,011 >> Initializing global attention on CLS token...\n",
            " 19% 18539/100000 [50:26<2:54:03,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:53,138 >> Initializing global attention on CLS token...\n",
            " 19% 18540/100000 [50:26<2:51:12,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:53,259 >> Initializing global attention on CLS token...\n",
            " 19% 18541/100000 [50:26<2:51:38,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:53,386 >> Initializing global attention on CLS token...\n",
            " 19% 18542/100000 [50:26<2:51:37,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:53,518 >> Initializing global attention on CLS token...\n",
            " 19% 18543/100000 [50:26<2:53:23,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:53,645 >> Initializing global attention on CLS token...\n",
            " 19% 18544/100000 [50:26<2:51:14,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:53,766 >> Initializing global attention on CLS token...\n",
            " 19% 18545/100000 [50:26<2:52:36,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:53,900 >> Initializing global attention on CLS token...\n",
            " 19% 18546/100000 [50:27<2:52:02,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:54,021 >> Initializing global attention on CLS token...\n",
            " 19% 18547/100000 [50:27<2:49:57,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:54,143 >> Initializing global attention on CLS token...\n",
            " 19% 18548/100000 [50:27<2:51:08,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:54,277 >> Initializing global attention on CLS token...\n",
            " 19% 18549/100000 [50:27<2:52:30,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:54,400 >> Initializing global attention on CLS token...\n",
            " 19% 18550/100000 [50:27<2:52:11,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:54,533 >> Initializing global attention on CLS token...\n",
            " 19% 18551/100000 [50:27<2:57:25,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:54,667 >> Initializing global attention on CLS token...\n",
            " 19% 18552/100000 [50:27<2:55:07,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:54,796 >> Initializing global attention on CLS token...\n",
            " 19% 18553/100000 [50:28<2:55:21,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:54,921 >> Initializing global attention on CLS token...\n",
            " 19% 18554/100000 [50:28<2:52:31,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:55,044 >> Initializing global attention on CLS token...\n",
            " 19% 18555/100000 [50:28<2:53:25,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:55,177 >> Initializing global attention on CLS token...\n",
            " 19% 18556/100000 [50:28<2:53:02,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:55,300 >> Initializing global attention on CLS token...\n",
            " 19% 18557/100000 [50:28<2:53:50,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:55,434 >> Initializing global attention on CLS token...\n",
            " 19% 18558/100000 [50:28<2:55:09,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:55,560 >> Initializing global attention on CLS token...\n",
            " 19% 18559/100000 [50:28<2:53:28,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:55,690 >> Initializing global attention on CLS token...\n",
            " 19% 18560/100000 [50:28<2:58:00,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:55,824 >> Initializing global attention on CLS token...\n",
            " 19% 18561/100000 [50:29<2:53:48,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:55,945 >> Initializing global attention on CLS token...\n",
            " 19% 18562/100000 [50:29<2:53:35,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:56,077 >> Initializing global attention on CLS token...\n",
            " 19% 18563/100000 [50:29<2:55:39,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:56,206 >> Initializing global attention on CLS token...\n",
            " 19% 18564/100000 [50:29<2:54:39,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:56,337 >> Initializing global attention on CLS token...\n",
            " 19% 18565/100000 [50:29<2:55:26,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:56,464 >> Initializing global attention on CLS token...\n",
            " 19% 18566/100000 [50:29<2:52:49,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:56,587 >> Initializing global attention on CLS token...\n",
            " 19% 18567/100000 [50:29<2:57:42,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:56,731 >> Initializing global attention on CLS token...\n",
            " 19% 18568/100000 [50:29<2:56:48,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:56,854 >> Initializing global attention on CLS token...\n",
            " 19% 18569/100000 [50:30<2:57:09,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:56,990 >> Initializing global attention on CLS token...\n",
            " 19% 18570/100000 [50:30<2:56:34,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:57,114 >> Initializing global attention on CLS token...\n",
            " 19% 18571/100000 [50:30<2:55:36,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:57,247 >> Initializing global attention on CLS token...\n",
            " 19% 18572/100000 [50:30<2:55:06,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:57,370 >> Initializing global attention on CLS token...\n",
            " 19% 18573/100000 [50:30<2:51:42,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:57,491 >> Initializing global attention on CLS token...\n",
            " 19% 18574/100000 [50:30<2:52:38,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:57,620 >> Initializing global attention on CLS token...\n",
            " 19% 18575/100000 [50:30<2:53:31,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:57,753 >> Initializing global attention on CLS token...\n",
            " 19% 18576/100000 [50:30<2:54:46,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:57,881 >> Initializing global attention on CLS token...\n",
            " 19% 18577/100000 [50:31<2:51:30,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:58,007 >> Initializing global attention on CLS token...\n",
            " 19% 18578/100000 [50:31<2:57:02,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:58,142 >> Initializing global attention on CLS token...\n",
            " 19% 18579/100000 [50:31<2:53:19,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:58,263 >> Initializing global attention on CLS token...\n",
            " 19% 18580/100000 [50:31<2:53:20,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:58,394 >> Initializing global attention on CLS token...\n",
            " 19% 18581/100000 [50:31<2:53:29,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:58,518 >> Initializing global attention on CLS token...\n",
            " 19% 18582/100000 [50:31<2:52:13,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:58,648 >> Initializing global attention on CLS token...\n",
            " 19% 18583/100000 [50:31<2:58:12,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:58,785 >> Initializing global attention on CLS token...\n",
            " 19% 18584/100000 [50:31<2:54:44,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:58,912 >> Initializing global attention on CLS token...\n",
            " 19% 18585/100000 [50:32<2:55:07,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:59,038 >> Initializing global attention on CLS token...\n",
            " 19% 18586/100000 [50:32<2:51:49,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:59,158 >> Initializing global attention on CLS token...\n",
            " 19% 18587/100000 [50:32<2:51:48,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:59,289 >> Initializing global attention on CLS token...\n",
            " 19% 18588/100000 [50:32<2:52:51,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:59,414 >> Initializing global attention on CLS token...\n",
            " 19% 18589/100000 [50:32<2:52:01,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:59,543 >> Initializing global attention on CLS token...\n",
            " 19% 18590/100000 [50:32<2:52:53,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:59,668 >> Initializing global attention on CLS token...\n",
            " 19% 18591/100000 [50:32<2:51:51,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:59,793 >> Initializing global attention on CLS token...\n",
            " 19% 18592/100000 [50:33<2:53:46,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:17:59,928 >> Initializing global attention on CLS token...\n",
            " 19% 18593/100000 [50:33<2:53:14,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:00,051 >> Initializing global attention on CLS token...\n",
            " 19% 18594/100000 [50:33<2:51:07,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:00,174 >> Initializing global attention on CLS token...\n",
            " 19% 18595/100000 [50:33<2:52:00,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:00,306 >> Initializing global attention on CLS token...\n",
            " 19% 18596/100000 [50:33<2:52:23,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:00,431 >> Initializing global attention on CLS token...\n",
            " 19% 18597/100000 [50:33<2:53:06,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:00,563 >> Initializing global attention on CLS token...\n",
            " 19% 18598/100000 [50:33<2:53:59,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:00,688 >> Initializing global attention on CLS token...\n",
            " 19% 18599/100000 [50:33<2:52:15,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:00,814 >> Initializing global attention on CLS token...\n",
            " 19% 18600/100000 [50:34<2:54:30,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:00,945 >> Initializing global attention on CLS token...\n",
            " 19% 18601/100000 [50:34<2:51:34,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:01,066 >> Initializing global attention on CLS token...\n",
            " 19% 18602/100000 [50:34<2:52:42,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:01,200 >> Initializing global attention on CLS token...\n",
            " 19% 18603/100000 [50:34<2:56:28,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:01,332 >> Initializing global attention on CLS token...\n",
            " 19% 18604/100000 [50:34<2:55:07,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:01,464 >> Initializing global attention on CLS token...\n",
            " 19% 18605/100000 [50:34<2:55:41,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:01,590 >> Initializing global attention on CLS token...\n",
            " 19% 18606/100000 [50:34<2:53:17,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:01,713 >> Initializing global attention on CLS token...\n",
            " 19% 18607/100000 [50:34<2:55:15,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:01,854 >> Initializing global attention on CLS token...\n",
            " 19% 18608/100000 [50:35<2:55:52,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:01,978 >> Initializing global attention on CLS token...\n",
            " 19% 18609/100000 [50:35<2:53:02,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:02,099 >> Initializing global attention on CLS token...\n",
            " 19% 18610/100000 [50:35<2:55:06,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:02,238 >> Initializing global attention on CLS token...\n",
            " 19% 18611/100000 [50:35<2:53:44,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:02,357 >> Initializing global attention on CLS token...\n",
            " 19% 18612/100000 [50:35<2:50:46,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:02,481 >> Initializing global attention on CLS token...\n",
            " 19% 18613/100000 [50:35<2:54:29,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:02,618 >> Initializing global attention on CLS token...\n",
            " 19% 18614/100000 [50:35<2:55:06,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:02,744 >> Initializing global attention on CLS token...\n",
            " 19% 18615/100000 [50:35<2:58:42,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:02,887 >> Initializing global attention on CLS token...\n",
            " 19% 18616/100000 [50:36<2:58:57,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:03,014 >> Initializing global attention on CLS token...\n",
            " 19% 18617/100000 [50:36<2:55:49,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:03,144 >> Initializing global attention on CLS token...\n",
            " 19% 18618/100000 [50:36<2:57:00,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:03,271 >> Initializing global attention on CLS token...\n",
            " 19% 18619/100000 [50:36<2:52:47,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:03,391 >> Initializing global attention on CLS token...\n",
            " 19% 18620/100000 [50:36<2:53:08,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:03,524 >> Initializing global attention on CLS token...\n",
            " 19% 18621/100000 [50:36<2:53:31,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:03,653 >> Initializing global attention on CLS token...\n",
            " 19% 18622/100000 [50:36<3:00:48,  7.50it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:03,798 >> Initializing global attention on CLS token...\n",
            " 19% 18623/100000 [50:37<2:58:56,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:03,922 >> Initializing global attention on CLS token...\n",
            " 19% 18624/100000 [50:37<2:57:11,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:04,054 >> Initializing global attention on CLS token...\n",
            " 19% 18625/100000 [50:37<2:55:40,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:04,176 >> Initializing global attention on CLS token...\n",
            " 19% 18626/100000 [50:37<2:52:26,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:04,298 >> Initializing global attention on CLS token...\n",
            " 19% 18627/100000 [50:37<2:52:50,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:04,432 >> Initializing global attention on CLS token...\n",
            " 19% 18628/100000 [50:37<2:54:01,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:04,557 >> Initializing global attention on CLS token...\n",
            " 19% 18629/100000 [50:37<2:52:29,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:04,688 >> Initializing global attention on CLS token...\n",
            " 19% 18630/100000 [50:37<2:54:57,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:04,815 >> Initializing global attention on CLS token...\n",
            " 19% 18631/100000 [50:38<2:54:08,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:04,945 >> Initializing global attention on CLS token...\n",
            " 19% 18632/100000 [50:38<2:56:29,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:05,079 >> Initializing global attention on CLS token...\n",
            " 19% 18633/100000 [50:38<2:54:07,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:05,200 >> Initializing global attention on CLS token...\n",
            " 19% 18634/100000 [50:38<2:51:05,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:05,321 >> Initializing global attention on CLS token...\n",
            " 19% 18635/100000 [50:38<2:51:51,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:05,455 >> Initializing global attention on CLS token...\n",
            " 19% 18636/100000 [50:38<2:52:59,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:05,578 >> Initializing global attention on CLS token...\n",
            " 19% 18637/100000 [50:38<2:55:13,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:05,716 >> Initializing global attention on CLS token...\n",
            " 19% 18638/100000 [50:38<2:55:23,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:05,841 >> Initializing global attention on CLS token...\n",
            " 19% 18639/100000 [50:39<2:55:00,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:05,976 >> Initializing global attention on CLS token...\n",
            " 19% 18640/100000 [50:39<2:57:13,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:06,104 >> Initializing global attention on CLS token...\n",
            " 19% 18641/100000 [50:39<2:53:20,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:06,225 >> Initializing global attention on CLS token...\n",
            " 19% 18642/100000 [50:39<2:52:51,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:06,359 >> Initializing global attention on CLS token...\n",
            " 19% 18643/100000 [50:39<2:53:10,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:06,480 >> Initializing global attention on CLS token...\n",
            " 19% 18644/100000 [50:39<2:52:58,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:06,612 >> Initializing global attention on CLS token...\n",
            " 19% 18645/100000 [50:39<2:54:20,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:06,739 >> Initializing global attention on CLS token...\n",
            " 19% 18646/100000 [50:39<2:51:36,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:06,863 >> Initializing global attention on CLS token...\n",
            " 19% 18647/100000 [50:40<2:54:35,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:06,998 >> Initializing global attention on CLS token...\n",
            " 19% 18648/100000 [50:40<2:53:14,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:07,120 >> Initializing global attention on CLS token...\n",
            " 19% 18649/100000 [50:40<2:52:22,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:07,250 >> Initializing global attention on CLS token...\n",
            " 19% 18650/100000 [50:40<2:53:09,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:07,374 >> Initializing global attention on CLS token...\n",
            " 19% 18651/100000 [50:40<2:52:37,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:07,505 >> Initializing global attention on CLS token...\n",
            " 19% 18652/100000 [50:40<2:55:21,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:07,635 >> Initializing global attention on CLS token...\n",
            " 19% 18653/100000 [50:40<2:52:11,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:07,757 >> Initializing global attention on CLS token...\n",
            " 19% 18654/100000 [50:40<2:54:00,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:07,889 >> Initializing global attention on CLS token...\n",
            " 19% 18655/100000 [50:41<2:51:16,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:08,014 >> Initializing global attention on CLS token...\n",
            " 19% 18656/100000 [50:41<2:51:36,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:08,136 >> Initializing global attention on CLS token...\n",
            " 19% 18657/100000 [50:41<2:52:23,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:08,272 >> Initializing global attention on CLS token...\n",
            " 19% 18658/100000 [50:41<2:53:10,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:08,394 >> Initializing global attention on CLS token...\n",
            " 19% 18659/100000 [50:41<2:53:20,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:08,530 >> Initializing global attention on CLS token...\n",
            " 19% 18660/100000 [50:41<2:55:29,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:08,655 >> Initializing global attention on CLS token...\n",
            " 19% 18661/100000 [50:41<2:52:23,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:08,782 >> Initializing global attention on CLS token...\n",
            " 19% 18662/100000 [50:41<2:54:22,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:08,910 >> Initializing global attention on CLS token...\n",
            " 19% 18663/100000 [50:42<2:51:34,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:09,035 >> Initializing global attention on CLS token...\n",
            " 19% 18664/100000 [50:42<2:54:17,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:09,169 >> Initializing global attention on CLS token...\n",
            " 19% 18665/100000 [50:42<2:54:05,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:09,293 >> Initializing global attention on CLS token...\n",
            " 19% 18666/100000 [50:42<2:54:03,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:09,426 >> Initializing global attention on CLS token...\n",
            " 19% 18667/100000 [50:42<2:56:17,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:09,555 >> Initializing global attention on CLS token...\n",
            " 19% 18668/100000 [50:42<2:52:33,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:09,679 >> Initializing global attention on CLS token...\n",
            " 19% 18669/100000 [50:42<2:55:14,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:09,814 >> Initializing global attention on CLS token...\n",
            " 19% 18670/100000 [50:43<2:53:42,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:09,935 >> Initializing global attention on CLS token...\n",
            " 19% 18671/100000 [50:43<2:54:14,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:10,070 >> Initializing global attention on CLS token...\n",
            " 19% 18672/100000 [50:43<2:59:26,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:10,206 >> Initializing global attention on CLS token...\n",
            " 19% 18673/100000 [50:43<2:55:56,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:10,334 >> Initializing global attention on CLS token...\n",
            " 19% 18674/100000 [50:43<2:54:57,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:10,458 >> Initializing global attention on CLS token...\n",
            " 19% 18675/100000 [50:43<2:51:55,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:10,579 >> Initializing global attention on CLS token...\n",
            " 19% 18676/100000 [50:43<2:52:57,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:10,711 >> Initializing global attention on CLS token...\n",
            " 19% 18677/100000 [50:43<2:51:12,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:10,832 >> Initializing global attention on CLS token...\n",
            " 19% 18678/100000 [50:44<2:51:29,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:10,964 >> Initializing global attention on CLS token...\n",
            " 19% 18679/100000 [50:44<2:52:57,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:11,088 >> Initializing global attention on CLS token...\n",
            " 19% 18680/100000 [50:44<2:52:12,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:11,219 >> Initializing global attention on CLS token...\n",
            " 19% 18681/100000 [50:44<2:53:14,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:11,344 >> Initializing global attention on CLS token...\n",
            " 19% 18682/100000 [50:44<2:50:08,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:11,464 >> Initializing global attention on CLS token...\n",
            " 19% 18683/100000 [50:44<2:51:04,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:11,597 >> Initializing global attention on CLS token...\n",
            " 19% 18684/100000 [50:44<2:53:08,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:11,723 >> Initializing global attention on CLS token...\n",
            " 19% 18685/100000 [50:44<2:49:48,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:11,843 >> Initializing global attention on CLS token...\n",
            " 19% 18686/100000 [50:45<2:50:00,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:11,973 >> Initializing global attention on CLS token...\n",
            " 19% 18687/100000 [50:45<2:51:15,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:12,097 >> Initializing global attention on CLS token...\n",
            " 19% 18688/100000 [50:45<2:51:32,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:12,229 >> Initializing global attention on CLS token...\n",
            " 19% 18689/100000 [50:45<2:53:11,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:12,355 >> Initializing global attention on CLS token...\n",
            " 19% 18690/100000 [50:45<2:50:40,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:12,477 >> Initializing global attention on CLS token...\n",
            " 19% 18691/100000 [50:45<2:51:22,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:12,610 >> Initializing global attention on CLS token...\n",
            " 19% 18692/100000 [50:45<2:53:55,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:12,738 >> Initializing global attention on CLS token...\n",
            " 19% 18693/100000 [50:45<2:50:52,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:12,858 >> Initializing global attention on CLS token...\n",
            " 19% 18694/100000 [50:46<2:50:59,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:12,989 >> Initializing global attention on CLS token...\n",
            " 19% 18695/100000 [50:46<2:54:10,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:13,118 >> Initializing global attention on CLS token...\n",
            " 19% 18696/100000 [50:46<2:54:32,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:13,252 >> Initializing global attention on CLS token...\n",
            " 19% 18697/100000 [50:46<2:55:02,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:13,378 >> Initializing global attention on CLS token...\n",
            " 19% 18698/100000 [50:46<2:51:26,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:13,502 >> Initializing global attention on CLS token...\n",
            " 19% 18699/100000 [50:46<2:53:09,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:13,630 >> Initializing global attention on CLS token...\n",
            " 19% 18700/100000 [50:46<2:50:46,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:13,751 >> Initializing global attention on CLS token...\n",
            " 19% 18701/100000 [50:46<2:50:24,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:13,884 >> Initializing global attention on CLS token...\n",
            " 19% 18702/100000 [50:47<2:53:20,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:14,013 >> Initializing global attention on CLS token...\n",
            " 19% 18703/100000 [50:47<2:56:00,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:14,147 >> Initializing global attention on CLS token...\n",
            " 19% 18704/100000 [50:47<2:55:10,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:14,271 >> Initializing global attention on CLS token...\n",
            " 19% 18705/100000 [50:47<2:55:17,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:14,407 >> Initializing global attention on CLS token...\n",
            " 19% 18706/100000 [50:47<2:58:29,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:14,544 >> Initializing global attention on CLS token...\n",
            " 19% 18707/100000 [50:47<3:01:17,  7.47it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:14,677 >> Initializing global attention on CLS token...\n",
            " 19% 18708/100000 [50:47<2:55:57,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:14,798 >> Initializing global attention on CLS token...\n",
            " 19% 18709/100000 [50:48<2:55:08,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:14,930 >> Initializing global attention on CLS token...\n",
            " 19% 18710/100000 [50:48<2:55:03,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:15,055 >> Initializing global attention on CLS token...\n",
            " 19% 18711/100000 [50:48<2:52:13,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:15,177 >> Initializing global attention on CLS token...\n",
            " 19% 18712/100000 [50:48<2:53:02,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:15,310 >> Initializing global attention on CLS token...\n",
            " 19% 18713/100000 [50:48<2:54:11,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:15,436 >> Initializing global attention on CLS token...\n",
            " 19% 18714/100000 [50:48<2:52:59,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:15,567 >> Initializing global attention on CLS token...\n",
            " 19% 18715/100000 [50:48<2:53:40,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:15,691 >> Initializing global attention on CLS token...\n",
            " 19% 18716/100000 [50:48<2:50:34,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:15,812 >> Initializing global attention on CLS token...\n",
            " 19% 18717/100000 [50:49<2:52:01,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:15,941 >> Initializing global attention on CLS token...\n",
            " 19% 18718/100000 [50:49<2:49:16,  8.00it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:16,062 >> Initializing global attention on CLS token...\n",
            " 19% 18719/100000 [50:49<2:49:40,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:16,194 >> Initializing global attention on CLS token...\n",
            " 19% 18720/100000 [50:49<2:57:12,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:16,340 >> Initializing global attention on CLS token...\n",
            " 19% 18721/100000 [50:49<3:00:57,  7.49it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:16,477 >> Initializing global attention on CLS token...\n",
            " 19% 18722/100000 [50:49<3:00:59,  7.48it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:16,605 >> Initializing global attention on CLS token...\n",
            " 19% 18723/100000 [50:49<2:55:56,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:16,726 >> Initializing global attention on CLS token...\n",
            " 19% 18724/100000 [50:49<2:59:22,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:16,865 >> Initializing global attention on CLS token...\n",
            " 19% 18725/100000 [50:50<2:55:07,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:16,987 >> Initializing global attention on CLS token...\n",
            " 19% 18726/100000 [50:50<2:52:41,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:17,110 >> Initializing global attention on CLS token...\n",
            " 19% 18727/100000 [50:50<2:52:55,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:17,243 >> Initializing global attention on CLS token...\n",
            " 19% 18728/100000 [50:50<2:54:14,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:17,372 >> Initializing global attention on CLS token...\n",
            " 19% 18729/100000 [50:50<2:51:45,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:17,492 >> Initializing global attention on CLS token...\n",
            " 19% 18730/100000 [50:50<2:54:39,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:17,631 >> Initializing global attention on CLS token...\n",
            " 19% 18731/100000 [50:50<2:54:16,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:17,754 >> Initializing global attention on CLS token...\n",
            " 19% 18732/100000 [50:50<2:52:58,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:17,884 >> Initializing global attention on CLS token...\n",
            " 19% 18733/100000 [50:51<2:53:49,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:18,008 >> Initializing global attention on CLS token...\n",
            " 19% 18734/100000 [50:51<2:50:34,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:18,131 >> Initializing global attention on CLS token...\n",
            " 19% 18735/100000 [50:51<2:54:22,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:18,265 >> Initializing global attention on CLS token...\n",
            " 19% 18736/100000 [50:51<2:53:10,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:18,394 >> Initializing global attention on CLS token...\n",
            " 19% 18737/100000 [50:51<2:51:38,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:18,514 >> Initializing global attention on CLS token...\n",
            " 19% 18738/100000 [50:51<2:54:02,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:18,653 >> Initializing global attention on CLS token...\n",
            " 19% 18739/100000 [50:51<2:56:24,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:18,785 >> Initializing global attention on CLS token...\n",
            " 19% 18740/100000 [50:51<2:56:25,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:18,911 >> Initializing global attention on CLS token...\n",
            " 19% 18741/100000 [50:52<2:52:04,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:19,032 >> Initializing global attention on CLS token...\n",
            " 19% 18742/100000 [50:52<2:54:13,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:19,169 >> Initializing global attention on CLS token...\n",
            " 19% 18743/100000 [50:52<2:53:40,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:19,291 >> Initializing global attention on CLS token...\n",
            " 19% 18744/100000 [50:52<2:56:35,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:19,431 >> Initializing global attention on CLS token...\n",
            " 19% 18745/100000 [50:52<2:56:15,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:19,556 >> Initializing global attention on CLS token...\n",
            " 19% 18746/100000 [50:52<2:55:32,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:19,689 >> Initializing global attention on CLS token...\n",
            " 19% 18747/100000 [50:52<2:56:43,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:19,816 >> Initializing global attention on CLS token...\n",
            " 19% 18748/100000 [50:53<2:52:21,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:19,936 >> Initializing global attention on CLS token...\n",
            " 19% 18749/100000 [50:53<2:52:12,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:20,070 >> Initializing global attention on CLS token...\n",
            " 19% 18750/100000 [50:53<2:55:46,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:20,204 >> Initializing global attention on CLS token...\n",
            " 19% 18751/100000 [50:53<2:57:11,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:20,333 >> Initializing global attention on CLS token...\n",
            " 19% 18752/100000 [50:53<2:54:43,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:20,457 >> Initializing global attention on CLS token...\n",
            " 19% 18753/100000 [50:53<2:56:44,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:20,593 >> Initializing global attention on CLS token...\n",
            " 19% 18754/100000 [50:53<2:55:58,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:20,724 >> Initializing global attention on CLS token...\n",
            " 19% 18755/100000 [50:53<2:55:20,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:20,848 >> Initializing global attention on CLS token...\n",
            " 19% 18756/100000 [50:54<2:51:49,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:20,969 >> Initializing global attention on CLS token...\n",
            " 19% 18757/100000 [50:54<2:53:31,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:21,101 >> Initializing global attention on CLS token...\n",
            " 19% 18758/100000 [50:54<2:54:07,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:21,235 >> Initializing global attention on CLS token...\n",
            " 19% 18759/100000 [50:54<2:53:40,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:21,357 >> Initializing global attention on CLS token...\n",
            " 19% 18760/100000 [50:54<2:51:42,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:21,483 >> Initializing global attention on CLS token...\n",
            " 19% 18761/100000 [50:54<2:52:40,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:21,611 >> Initializing global attention on CLS token...\n",
            " 19% 18762/100000 [50:54<2:50:50,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:21,733 >> Initializing global attention on CLS token...\n",
            " 19% 18763/100000 [50:54<2:50:57,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:21,864 >> Initializing global attention on CLS token...\n",
            " 19% 18764/100000 [50:55<2:51:43,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:21,988 >> Initializing global attention on CLS token...\n",
            " 19% 18765/100000 [50:55<2:51:37,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:22,119 >> Initializing global attention on CLS token...\n",
            " 19% 18766/100000 [50:55<2:53:39,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:22,246 >> Initializing global attention on CLS token...\n",
            " 19% 18767/100000 [50:55<2:50:19,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:22,366 >> Initializing global attention on CLS token...\n",
            " 19% 18768/100000 [50:55<2:52:09,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:22,505 >> Initializing global attention on CLS token...\n",
            " 19% 18769/100000 [50:55<2:56:38,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:22,635 >> Initializing global attention on CLS token...\n",
            " 19% 18770/100000 [50:55<2:52:48,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:22,756 >> Initializing global attention on CLS token...\n",
            " 19% 18771/100000 [50:55<2:52:36,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:22,888 >> Initializing global attention on CLS token...\n",
            " 19% 18772/100000 [50:56<2:52:30,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:23,010 >> Initializing global attention on CLS token...\n",
            " 19% 18773/100000 [50:56<2:52:33,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:23,142 >> Initializing global attention on CLS token...\n",
            " 19% 18774/100000 [50:56<2:53:42,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:23,268 >> Initializing global attention on CLS token...\n",
            " 19% 18775/100000 [50:56<2:51:14,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:23,392 >> Initializing global attention on CLS token...\n",
            " 19% 18776/100000 [50:56<2:53:28,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:23,526 >> Initializing global attention on CLS token...\n",
            " 19% 18777/100000 [50:56<2:53:58,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:23,653 >> Initializing global attention on CLS token...\n",
            " 19% 18778/100000 [50:56<2:53:57,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:23,785 >> Initializing global attention on CLS token...\n",
            " 19% 18779/100000 [50:56<2:54:05,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:23,914 >> Initializing global attention on CLS token...\n",
            " 19% 18780/100000 [50:57<2:54:32,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:24,044 >> Initializing global attention on CLS token...\n",
            " 19% 18781/100000 [50:57<2:56:01,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:24,171 >> Initializing global attention on CLS token...\n",
            " 19% 18782/100000 [50:57<2:52:40,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:24,293 >> Initializing global attention on CLS token...\n",
            " 19% 18783/100000 [50:57<2:53:18,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:24,423 >> Initializing global attention on CLS token...\n",
            " 19% 18784/100000 [50:57<2:50:36,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:24,544 >> Initializing global attention on CLS token...\n",
            " 19% 18785/100000 [50:57<2:50:29,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:24,670 >> Initializing global attention on CLS token...\n",
            " 19% 18786/100000 [50:57<2:51:00,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:24,802 >> Initializing global attention on CLS token...\n",
            " 19% 18787/100000 [50:58<2:51:48,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:24,925 >> Initializing global attention on CLS token...\n",
            " 19% 18788/100000 [50:58<2:52:33,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:25,059 >> Initializing global attention on CLS token...\n",
            " 19% 18789/100000 [50:58<2:53:51,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:25,185 >> Initializing global attention on CLS token...\n",
            " 19% 18790/100000 [50:58<2:52:38,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:25,315 >> Initializing global attention on CLS token...\n",
            " 19% 18791/100000 [50:58<2:56:15,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:25,447 >> Initializing global attention on CLS token...\n",
            " 19% 18792/100000 [50:58<2:52:24,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:25,567 >> Initializing global attention on CLS token...\n",
            " 19% 18793/100000 [50:58<2:54:42,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:25,706 >> Initializing global attention on CLS token...\n",
            " 19% 18794/100000 [50:58<2:54:48,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:25,830 >> Initializing global attention on CLS token...\n",
            " 19% 18795/100000 [50:59<2:53:10,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:25,960 >> Initializing global attention on CLS token...\n",
            " 19% 18796/100000 [50:59<2:53:52,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:26,084 >> Initializing global attention on CLS token...\n",
            " 19% 18797/100000 [50:59<2:51:16,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:26,209 >> Initializing global attention on CLS token...\n",
            " 19% 18798/100000 [50:59<2:55:34,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:26,344 >> Initializing global attention on CLS token...\n",
            " 19% 18799/100000 [50:59<2:51:32,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:26,463 >> Initializing global attention on CLS token...\n",
            " 19% 18800/100000 [50:59<2:51:08,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:26,593 >> Initializing global attention on CLS token...\n",
            " 19% 18801/100000 [50:59<2:57:19,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:26,736 >> Initializing global attention on CLS token...\n",
            " 19% 18802/100000 [50:59<2:57:08,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:26,862 >> Initializing global attention on CLS token...\n",
            " 19% 18803/100000 [51:00<2:53:00,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:26,982 >> Initializing global attention on CLS token...\n",
            " 19% 18804/100000 [51:00<2:52:53,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:27,116 >> Initializing global attention on CLS token...\n",
            " 19% 18805/100000 [51:00<2:52:58,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:27,238 >> Initializing global attention on CLS token...\n",
            " 19% 18806/100000 [51:00<2:53:09,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:27,371 >> Initializing global attention on CLS token...\n",
            " 19% 18807/100000 [51:00<2:54:59,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:27,498 >> Initializing global attention on CLS token...\n",
            " 19% 18808/100000 [51:00<2:51:29,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:27,623 >> Initializing global attention on CLS token...\n",
            " 19% 18809/100000 [51:00<2:57:00,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:27,760 >> Initializing global attention on CLS token...\n",
            " 19% 18810/100000 [51:00<2:53:53,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:27,883 >> Initializing global attention on CLS token...\n",
            " 19% 18811/100000 [51:01<2:53:54,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:28,015 >> Initializing global attention on CLS token...\n",
            " 19% 18812/100000 [51:01<2:53:28,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:28,139 >> Initializing global attention on CLS token...\n",
            " 19% 18813/100000 [51:01<2:52:38,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:28,269 >> Initializing global attention on CLS token...\n",
            " 19% 18814/100000 [51:01<2:55:35,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:28,400 >> Initializing global attention on CLS token...\n",
            " 19% 18815/100000 [51:01<2:52:25,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:28,523 >> Initializing global attention on CLS token...\n",
            " 19% 18816/100000 [51:01<2:53:26,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:28,652 >> Initializing global attention on CLS token...\n",
            " 19% 18817/100000 [51:01<2:54:34,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:28,788 >> Initializing global attention on CLS token...\n",
            " 19% 18818/100000 [51:01<2:55:13,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:28,913 >> Initializing global attention on CLS token...\n",
            " 19% 18819/100000 [51:02<2:51:33,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:29,038 >> Initializing global attention on CLS token...\n",
            " 19% 18820/100000 [51:02<2:52:48,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:29,164 >> Initializing global attention on CLS token...\n",
            " 19% 18821/100000 [51:02<2:50:22,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:29,285 >> Initializing global attention on CLS token...\n",
            " 19% 18822/100000 [51:02<2:51:00,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:29,417 >> Initializing global attention on CLS token...\n",
            " 19% 18823/100000 [51:02<2:51:37,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:29,541 >> Initializing global attention on CLS token...\n",
            " 19% 18824/100000 [51:02<2:51:37,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:29,676 >> Initializing global attention on CLS token...\n",
            " 19% 18825/100000 [51:02<2:58:29,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:29,811 >> Initializing global attention on CLS token...\n",
            " 19% 18826/100000 [51:03<2:54:10,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:29,933 >> Initializing global attention on CLS token...\n",
            " 19% 18827/100000 [51:03<2:53:27,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:30,060 >> Initializing global attention on CLS token...\n",
            " 19% 18828/100000 [51:03<2:50:49,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:30,182 >> Initializing global attention on CLS token...\n",
            " 19% 18829/100000 [51:03<2:51:56,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:30,315 >> Initializing global attention on CLS token...\n",
            " 19% 18830/100000 [51:03<2:53:06,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:30,441 >> Initializing global attention on CLS token...\n",
            " 19% 18831/100000 [51:03<2:54:29,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:30,576 >> Initializing global attention on CLS token...\n",
            " 19% 18832/100000 [51:03<2:53:36,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:30,699 >> Initializing global attention on CLS token...\n",
            " 19% 18833/100000 [51:03<2:54:45,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:30,835 >> Initializing global attention on CLS token...\n",
            " 19% 18834/100000 [51:04<2:55:36,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:30,966 >> Initializing global attention on CLS token...\n",
            " 19% 18835/100000 [51:04<2:55:52,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:31,091 >> Initializing global attention on CLS token...\n",
            " 19% 18836/100000 [51:04<2:53:11,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:31,216 >> Initializing global attention on CLS token...\n",
            " 19% 18837/100000 [51:04<2:52:59,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:31,347 >> Initializing global attention on CLS token...\n",
            " 19% 18838/100000 [51:04<2:53:14,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:31,472 >> Initializing global attention on CLS token...\n",
            " 19% 18839/100000 [51:04<2:52:15,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:31,601 >> Initializing global attention on CLS token...\n",
            " 19% 18840/100000 [51:04<2:53:08,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:31,727 >> Initializing global attention on CLS token...\n",
            " 19% 18841/100000 [51:04<2:57:23,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:31,870 >> Initializing global attention on CLS token...\n",
            " 19% 18842/100000 [51:05<2:57:31,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:31,996 >> Initializing global attention on CLS token...\n",
            " 19% 18843/100000 [51:05<2:53:23,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:32,122 >> Initializing global attention on CLS token...\n",
            " 19% 18844/100000 [51:05<2:54:10,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:32,252 >> Initializing global attention on CLS token...\n",
            " 19% 18845/100000 [51:05<2:54:24,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:32,377 >> Initializing global attention on CLS token...\n",
            " 19% 18846/100000 [51:05<2:53:01,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:32,507 >> Initializing global attention on CLS token...\n",
            " 19% 18847/100000 [51:05<2:55:30,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:32,637 >> Initializing global attention on CLS token...\n",
            " 19% 18848/100000 [51:05<2:52:41,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:32,759 >> Initializing global attention on CLS token...\n",
            " 19% 18849/100000 [51:05<2:55:27,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:32,898 >> Initializing global attention on CLS token...\n",
            " 19% 18850/100000 [51:06<2:54:11,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:33,020 >> Initializing global attention on CLS token...\n",
            " 19% 18851/100000 [51:06<2:50:59,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:33,141 >> Initializing global attention on CLS token...\n",
            " 19% 18852/100000 [51:06<2:53:01,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:33,276 >> Initializing global attention on CLS token...\n",
            " 19% 18853/100000 [51:06<2:53:22,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:33,405 >> Initializing global attention on CLS token...\n",
            " 19% 18854/100000 [51:06<2:51:18,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:33,525 >> Initializing global attention on CLS token...\n",
            " 19% 18855/100000 [51:06<2:51:01,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:33,656 >> Initializing global attention on CLS token...\n",
            " 19% 18856/100000 [51:06<2:52:12,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:33,780 >> Initializing global attention on CLS token...\n",
            " 19% 18857/100000 [51:06<2:52:26,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:33,914 >> Initializing global attention on CLS token...\n",
            " 19% 18858/100000 [51:07<2:55:13,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:34,042 >> Initializing global attention on CLS token...\n",
            " 19% 18859/100000 [51:07<2:51:53,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:34,164 >> Initializing global attention on CLS token...\n",
            " 19% 18860/100000 [51:07<2:52:48,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:34,298 >> Initializing global attention on CLS token...\n",
            " 19% 18861/100000 [51:07<2:53:25,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:34,422 >> Initializing global attention on CLS token...\n",
            " 19% 18862/100000 [51:07<2:50:59,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:34,544 >> Initializing global attention on CLS token...\n",
            " 19% 18863/100000 [51:07<2:51:33,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:34,679 >> Initializing global attention on CLS token...\n",
            " 19% 18864/100000 [51:07<2:52:22,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:34,802 >> Initializing global attention on CLS token...\n",
            " 19% 18865/100000 [51:08<2:53:02,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:34,935 >> Initializing global attention on CLS token...\n",
            " 19% 18866/100000 [51:08<2:57:28,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:35,069 >> Initializing global attention on CLS token...\n",
            " 19% 18867/100000 [51:08<2:56:19,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:35,202 >> Initializing global attention on CLS token...\n",
            " 19% 18868/100000 [51:08<2:55:40,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:35,326 >> Initializing global attention on CLS token...\n",
            " 19% 18869/100000 [51:08<2:55:07,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:35,455 >> Initializing global attention on CLS token...\n",
            " 19% 18870/100000 [51:08<2:54:28,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:35,587 >> Initializing global attention on CLS token...\n",
            " 19% 18871/100000 [51:08<2:53:44,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:35,710 >> Initializing global attention on CLS token...\n",
            " 19% 18872/100000 [51:08<2:50:27,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:35,830 >> Initializing global attention on CLS token...\n",
            " 19% 18873/100000 [51:09<2:51:06,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:35,963 >> Initializing global attention on CLS token...\n",
            " 19% 18874/100000 [51:09<2:53:02,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:36,089 >> Initializing global attention on CLS token...\n",
            " 19% 18875/100000 [51:09<2:53:30,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:36,224 >> Initializing global attention on CLS token...\n",
            " 19% 18876/100000 [51:09<2:54:17,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:36,349 >> Initializing global attention on CLS token...\n",
            " 19% 18877/100000 [51:09<2:50:46,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:36,469 >> Initializing global attention on CLS token...\n",
            " 19% 18878/100000 [51:09<2:52:20,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:36,602 >> Initializing global attention on CLS token...\n",
            " 19% 18879/100000 [51:09<2:50:18,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:36,722 >> Initializing global attention on CLS token...\n",
            " 19% 18880/100000 [51:09<2:48:14,  8.04it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:36,843 >> Initializing global attention on CLS token...\n",
            " 19% 18881/100000 [51:10<2:49:16,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:36,974 >> Initializing global attention on CLS token...\n",
            " 19% 18882/100000 [51:10<2:51:41,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:37,101 >> Initializing global attention on CLS token...\n",
            " 19% 18883/100000 [51:10<2:51:09,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:37,231 >> Initializing global attention on CLS token...\n",
            " 19% 18884/100000 [51:10<2:52:46,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:37,358 >> Initializing global attention on CLS token...\n",
            " 19% 18885/100000 [51:10<2:50:35,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:37,480 >> Initializing global attention on CLS token...\n",
            " 19% 18886/100000 [51:10<2:54:06,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:37,615 >> Initializing global attention on CLS token...\n",
            " 19% 18887/100000 [51:10<2:51:16,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:37,736 >> Initializing global attention on CLS token...\n",
            " 19% 18888/100000 [51:10<2:48:56,  8.00it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:37,858 >> Initializing global attention on CLS token...\n",
            " 19% 18889/100000 [51:11<2:51:29,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:37,995 >> Initializing global attention on CLS token...\n",
            " 19% 18890/100000 [51:11<2:53:14,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:38,120 >> Initializing global attention on CLS token...\n",
            " 19% 18891/100000 [51:11<2:52:35,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:38,252 >> Initializing global attention on CLS token...\n",
            " 19% 18892/100000 [51:11<2:57:37,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:38,386 >> Initializing global attention on CLS token...\n",
            " 19% 18893/100000 [51:11<2:55:51,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:38,519 >> Initializing global attention on CLS token...\n",
            " 19% 18894/100000 [51:11<2:57:04,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:38,648 >> Initializing global attention on CLS token...\n",
            " 19% 18895/100000 [51:11<2:53:40,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:38,769 >> Initializing global attention on CLS token...\n",
            " 19% 18896/100000 [51:11<2:53:32,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:38,904 >> Initializing global attention on CLS token...\n",
            " 19% 18897/100000 [51:12<2:54:03,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:39,027 >> Initializing global attention on CLS token...\n",
            " 19% 18898/100000 [51:12<2:56:03,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:39,166 >> Initializing global attention on CLS token...\n",
            " 19% 18899/100000 [51:12<2:56:23,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:39,293 >> Initializing global attention on CLS token...\n",
            " 19% 18900/100000 [51:12<2:54:31,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:39,422 >> Initializing global attention on CLS token...\n",
            " 19% 18901/100000 [51:12<2:57:45,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:39,559 >> Initializing global attention on CLS token...\n",
            " 19% 18902/100000 [51:12<2:55:29,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:39,681 >> Initializing global attention on CLS token...\n",
            " 19% 18903/100000 [51:12<2:54:30,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:39,811 >> Initializing global attention on CLS token...\n",
            " 19% 18904/100000 [51:13<2:54:02,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:39,936 >> Initializing global attention on CLS token...\n",
            " 19% 18905/100000 [51:13<2:50:44,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:40,057 >> Initializing global attention on CLS token...\n",
            " 19% 18906/100000 [51:13<2:52:19,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:40,191 >> Initializing global attention on CLS token...\n",
            " 19% 18907/100000 [51:13<2:52:51,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:40,316 >> Initializing global attention on CLS token...\n",
            " 19% 18908/100000 [51:13<2:52:00,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:40,447 >> Initializing global attention on CLS token...\n",
            " 19% 18909/100000 [51:13<2:56:22,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:40,580 >> Initializing global attention on CLS token...\n",
            " 19% 18910/100000 [51:13<2:52:20,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:40,700 >> Initializing global attention on CLS token...\n",
            " 19% 18911/100000 [51:13<2:53:14,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:40,834 >> Initializing global attention on CLS token...\n",
            " 19% 18912/100000 [51:14<2:52:22,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:40,956 >> Initializing global attention on CLS token...\n",
            " 19% 18913/100000 [51:14<2:49:46,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:41,077 >> Initializing global attention on CLS token...\n",
            " 19% 18914/100000 [51:14<2:51:26,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:41,211 >> Initializing global attention on CLS token...\n",
            " 19% 18915/100000 [51:14<2:52:17,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:41,336 >> Initializing global attention on CLS token...\n",
            " 19% 18916/100000 [51:14<2:50:58,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:41,465 >> Initializing global attention on CLS token...\n",
            " 19% 18917/100000 [51:14<2:52:10,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:41,589 >> Initializing global attention on CLS token...\n",
            " 19% 18918/100000 [51:14<2:49:59,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:41,712 >> Initializing global attention on CLS token...\n",
            " 19% 18919/100000 [51:14<2:52:42,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:41,844 >> Initializing global attention on CLS token...\n",
            " 19% 18920/100000 [51:15<2:49:53,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:41,965 >> Initializing global attention on CLS token...\n",
            " 19% 18921/100000 [51:15<2:50:58,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:42,099 >> Initializing global attention on CLS token...\n",
            " 19% 18922/100000 [51:15<2:54:10,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:42,228 >> Initializing global attention on CLS token...\n",
            " 19% 18923/100000 [51:15<2:58:08,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:42,371 >> Initializing global attention on CLS token...\n",
            " 19% 18924/100000 [51:15<2:58:00,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:42,498 >> Initializing global attention on CLS token...\n",
            " 19% 18925/100000 [51:15<2:54:14,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:42,625 >> Initializing global attention on CLS token...\n",
            " 19% 18926/100000 [51:15<2:56:28,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:42,755 >> Initializing global attention on CLS token...\n",
            " 19% 18927/100000 [51:15<2:52:53,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:42,878 >> Initializing global attention on CLS token...\n",
            " 19% 18928/100000 [51:16<2:53:27,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:43,011 >> Initializing global attention on CLS token...\n",
            " 19% 18929/100000 [51:16<2:54:47,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:43,138 >> Initializing global attention on CLS token...\n",
            " 19% 18930/100000 [51:16<2:52:55,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:43,268 >> Initializing global attention on CLS token...\n",
            " 19% 18931/100000 [51:16<2:56:57,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:43,401 >> Initializing global attention on CLS token...\n",
            " 19% 18932/100000 [51:16<2:53:06,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:43,522 >> Initializing global attention on CLS token...\n",
            " 19% 18933/100000 [51:16<2:54:56,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:43,656 >> Initializing global attention on CLS token...\n",
            " 19% 18934/100000 [51:16<2:52:09,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:43,778 >> Initializing global attention on CLS token...\n",
            " 19% 18935/100000 [51:16<2:50:03,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:43,899 >> Initializing global attention on CLS token...\n",
            " 19% 18936/100000 [51:17<2:50:39,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:44,032 >> Initializing global attention on CLS token...\n",
            " 19% 18937/100000 [51:17<2:51:36,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:44,156 >> Initializing global attention on CLS token...\n",
            " 19% 18938/100000 [51:17<2:51:39,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:44,287 >> Initializing global attention on CLS token...\n",
            " 19% 18939/100000 [51:17<2:55:21,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:44,419 >> Initializing global attention on CLS token...\n",
            " 19% 18940/100000 [51:17<2:51:36,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:44,543 >> Initializing global attention on CLS token...\n",
            " 19% 18941/100000 [51:17<2:53:46,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:44,673 >> Initializing global attention on CLS token...\n",
            " 19% 18942/100000 [51:17<2:50:53,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:44,794 >> Initializing global attention on CLS token...\n",
            " 19% 18943/100000 [51:18<2:51:26,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:44,925 >> Initializing global attention on CLS token...\n",
            " 19% 18944/100000 [51:18<2:51:40,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:45,051 >> Initializing global attention on CLS token...\n",
            " 19% 18945/100000 [51:18<2:52:35,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:45,178 >> Initializing global attention on CLS token...\n",
            " 19% 18946/100000 [51:18<2:50:08,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:45,300 >> Initializing global attention on CLS token...\n",
            " 19% 18947/100000 [51:18<2:52:12,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:45,435 >> Initializing global attention on CLS token...\n",
            " 19% 18948/100000 [51:18<2:52:35,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:45,559 >> Initializing global attention on CLS token...\n",
            " 19% 18949/100000 [51:18<2:51:49,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:45,690 >> Initializing global attention on CLS token...\n",
            " 19% 18950/100000 [51:18<2:54:33,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:45,819 >> Initializing global attention on CLS token...\n",
            " 19% 18951/100000 [51:19<2:51:29,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:45,941 >> Initializing global attention on CLS token...\n",
            " 19% 18952/100000 [51:19<2:53:21,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:46,076 >> Initializing global attention on CLS token...\n",
            " 19% 18953/100000 [51:19<2:53:11,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:46,200 >> Initializing global attention on CLS token...\n",
            " 19% 18954/100000 [51:19<2:52:18,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:46,326 >> Initializing global attention on CLS token...\n",
            " 19% 18955/100000 [51:19<3:01:12,  7.45it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:46,485 >> Initializing global attention on CLS token...\n",
            " 19% 18956/100000 [51:19<3:01:12,  7.45it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:46,610 >> Initializing global attention on CLS token...\n",
            " 19% 18957/100000 [51:19<2:56:45,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:46,733 >> Initializing global attention on CLS token...\n",
            " 19% 18958/100000 [51:19<2:55:42,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:46,866 >> Initializing global attention on CLS token...\n",
            " 19% 18959/100000 [51:20<2:56:22,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:46,993 >> Initializing global attention on CLS token...\n",
            " 19% 18960/100000 [51:20<2:54:49,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:47,125 >> Initializing global attention on CLS token...\n",
            " 19% 18961/100000 [51:20<2:56:08,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:47,253 >> Initializing global attention on CLS token...\n",
            " 19% 18962/100000 [51:20<2:52:28,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:47,381 >> Initializing global attention on CLS token...\n",
            " 19% 18963/100000 [51:20<2:58:48,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:47,519 >> Initializing global attention on CLS token...\n",
            " 19% 18964/100000 [51:20<2:59:04,  7.54it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:47,655 >> Initializing global attention on CLS token...\n",
            " 19% 18965/100000 [51:20<2:57:44,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:47,780 >> Initializing global attention on CLS token...\n",
            " 19% 18966/100000 [51:20<2:57:24,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:47,914 >> Initializing global attention on CLS token...\n",
            " 19% 18967/100000 [51:21<2:55:58,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:48,038 >> Initializing global attention on CLS token...\n",
            " 19% 18968/100000 [51:21<2:52:01,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:48,159 >> Initializing global attention on CLS token...\n",
            " 19% 18969/100000 [51:21<2:54:01,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:48,294 >> Initializing global attention on CLS token...\n",
            " 19% 18970/100000 [51:21<2:51:58,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:48,415 >> Initializing global attention on CLS token...\n",
            " 19% 18971/100000 [51:21<2:51:57,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:48,543 >> Initializing global attention on CLS token...\n",
            " 19% 18972/100000 [51:21<2:53:37,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:48,678 >> Initializing global attention on CLS token...\n",
            " 19% 18973/100000 [51:21<2:56:37,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:48,814 >> Initializing global attention on CLS token...\n",
            " 19% 18974/100000 [51:22<2:55:50,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:48,939 >> Initializing global attention on CLS token...\n",
            " 19% 18975/100000 [51:22<2:52:01,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:49,060 >> Initializing global attention on CLS token...\n",
            " 19% 18976/100000 [51:22<2:52:51,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:49,193 >> Initializing global attention on CLS token...\n",
            " 19% 18977/100000 [51:22<2:52:27,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:49,316 >> Initializing global attention on CLS token...\n",
            " 19% 18978/100000 [51:22<2:52:43,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:49,449 >> Initializing global attention on CLS token...\n",
            " 19% 18979/100000 [51:22<2:57:42,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:49,585 >> Initializing global attention on CLS token...\n",
            " 19% 18980/100000 [51:22<2:55:47,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:49,717 >> Initializing global attention on CLS token...\n",
            " 19% 18981/100000 [51:22<2:57:51,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:49,847 >> Initializing global attention on CLS token...\n",
            " 19% 18982/100000 [51:23<2:53:22,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:49,967 >> Initializing global attention on CLS token...\n",
            " 19% 18983/100000 [51:23<2:53:46,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:50,101 >> Initializing global attention on CLS token...\n",
            " 19% 18984/100000 [51:23<2:53:14,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:50,224 >> Initializing global attention on CLS token...\n",
            " 19% 18985/100000 [51:23<2:51:46,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:50,353 >> Initializing global attention on CLS token...\n",
            " 19% 18986/100000 [51:23<2:55:50,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:50,486 >> Initializing global attention on CLS token...\n",
            " 19% 18987/100000 [51:23<2:54:11,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:50,617 >> Initializing global attention on CLS token...\n",
            " 19% 18988/100000 [51:23<2:55:42,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:50,745 >> Initializing global attention on CLS token...\n",
            " 19% 18989/100000 [51:23<2:52:27,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:50,867 >> Initializing global attention on CLS token...\n",
            " 19% 18990/100000 [51:24<2:53:22,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:51,002 >> Initializing global attention on CLS token...\n",
            " 19% 18991/100000 [51:24<2:54:50,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:51,133 >> Initializing global attention on CLS token...\n",
            " 19% 18992/100000 [51:24<2:54:59,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:51,259 >> Initializing global attention on CLS token...\n",
            " 19% 18993/100000 [51:24<2:51:36,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:51,380 >> Initializing global attention on CLS token...\n",
            " 19% 18994/100000 [51:24<2:54:44,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:51,521 >> Initializing global attention on CLS token...\n",
            " 19% 18995/100000 [51:24<2:58:48,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:51,660 >> Initializing global attention on CLS token...\n",
            " 19% 18996/100000 [51:24<3:00:08,  7.49it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:51,790 >> Initializing global attention on CLS token...\n",
            " 19% 18997/100000 [51:24<2:55:13,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:51,911 >> Initializing global attention on CLS token...\n",
            " 19% 18998/100000 [51:25<2:54:45,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:52,045 >> Initializing global attention on CLS token...\n",
            " 19% 18999/100000 [51:25<2:55:26,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:52,176 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.4322, 'learning_rate': 8.1006e-05, 'epoch': 3.8}\n",
            " 19% 19000/100000 [51:25<2:58:05,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:52,312 >> Initializing global attention on CLS token...\n",
            " 19% 19001/100000 [51:25<2:56:14,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:52,440 >> Initializing global attention on CLS token...\n",
            " 19% 19002/100000 [51:25<2:59:46,  7.51it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:52,576 >> Initializing global attention on CLS token...\n",
            " 19% 19003/100000 [51:25<2:57:05,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:52,702 >> Initializing global attention on CLS token...\n",
            " 19% 19004/100000 [51:25<2:56:31,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:52,836 >> Initializing global attention on CLS token...\n",
            " 19% 19005/100000 [51:26<2:55:45,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:52,960 >> Initializing global attention on CLS token...\n",
            " 19% 19006/100000 [51:26<2:52:19,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:53,081 >> Initializing global attention on CLS token...\n",
            " 19% 19007/100000 [51:26<2:52:54,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:53,215 >> Initializing global attention on CLS token...\n",
            " 19% 19008/100000 [51:26<2:53:13,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:53,340 >> Initializing global attention on CLS token...\n",
            " 19% 19009/100000 [51:26<2:54:48,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:53,476 >> Initializing global attention on CLS token...\n",
            " 19% 19010/100000 [51:26<2:55:22,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:53,603 >> Initializing global attention on CLS token...\n",
            " 19% 19011/100000 [51:26<2:53:00,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:53,730 >> Initializing global attention on CLS token...\n",
            " 19% 19012/100000 [51:26<2:55:03,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:53,860 >> Initializing global attention on CLS token...\n",
            " 19% 19013/100000 [51:27<2:51:51,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:53,982 >> Initializing global attention on CLS token...\n",
            " 19% 19014/100000 [51:27<2:51:25,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:54,113 >> Initializing global attention on CLS token...\n",
            " 19% 19015/100000 [51:27<2:52:09,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:54,237 >> Initializing global attention on CLS token...\n",
            " 19% 19016/100000 [51:27<2:52:08,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:54,368 >> Initializing global attention on CLS token...\n",
            " 19% 19017/100000 [51:27<2:53:15,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:54,495 >> Initializing global attention on CLS token...\n",
            " 19% 19018/100000 [51:27<2:50:57,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:54,618 >> Initializing global attention on CLS token...\n",
            " 19% 19019/100000 [51:27<2:54:18,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:54,762 >> Initializing global attention on CLS token...\n",
            " 19% 19020/100000 [51:27<2:58:26,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:54,896 >> Initializing global attention on CLS token...\n",
            " 19% 19021/100000 [51:28<2:57:29,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:55,022 >> Initializing global attention on CLS token...\n",
            " 19% 19022/100000 [51:28<2:54:15,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:55,150 >> Initializing global attention on CLS token...\n",
            " 19% 19023/100000 [51:28<2:55:39,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:55,278 >> Initializing global attention on CLS token...\n",
            " 19% 19024/100000 [51:28<2:51:37,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:55,398 >> Initializing global attention on CLS token...\n",
            " 19% 19025/100000 [51:28<2:52:01,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:55,531 >> Initializing global attention on CLS token...\n",
            " 19% 19026/100000 [51:28<2:54:57,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:55,665 >> Initializing global attention on CLS token...\n",
            " 19% 19027/100000 [51:28<3:00:59,  7.46it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:55,806 >> Initializing global attention on CLS token...\n",
            " 19% 19028/100000 [51:29<2:55:52,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:55,927 >> Initializing global attention on CLS token...\n",
            " 19% 19029/100000 [51:29<2:54:42,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:56,060 >> Initializing global attention on CLS token...\n",
            " 19% 19030/100000 [51:29<2:57:03,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:56,195 >> Initializing global attention on CLS token...\n",
            " 19% 19031/100000 [51:29<2:57:16,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:56,321 >> Initializing global attention on CLS token...\n",
            " 19% 19032/100000 [51:29<2:52:51,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:56,442 >> Initializing global attention on CLS token...\n",
            " 19% 19033/100000 [51:29<2:58:01,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:56,584 >> Initializing global attention on CLS token...\n",
            " 19% 19034/100000 [51:29<2:54:04,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:56,705 >> Initializing global attention on CLS token...\n",
            " 19% 19035/100000 [51:29<2:54:42,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:56,840 >> Initializing global attention on CLS token...\n",
            " 19% 19036/100000 [51:30<2:58:46,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:56,980 >> Initializing global attention on CLS token...\n",
            " 19% 19037/100000 [51:30<2:57:10,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:57,103 >> Initializing global attention on CLS token...\n",
            " 19% 19038/100000 [51:30<2:53:03,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:57,225 >> Initializing global attention on CLS token...\n",
            " 19% 19039/100000 [51:30<2:53:59,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:57,360 >> Initializing global attention on CLS token...\n",
            " 19% 19040/100000 [51:30<2:55:01,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:57,491 >> Initializing global attention on CLS token...\n",
            " 19% 19041/100000 [51:30<2:55:05,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:57,617 >> Initializing global attention on CLS token...\n",
            " 19% 19042/100000 [51:30<2:51:54,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:57,738 >> Initializing global attention on CLS token...\n",
            " 19% 19043/100000 [51:30<2:57:41,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:57,885 >> Initializing global attention on CLS token...\n",
            " 19% 19044/100000 [51:31<2:59:19,  7.52it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:58,020 >> Initializing global attention on CLS token...\n",
            " 19% 19045/100000 [51:31<2:57:25,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:58,144 >> Initializing global attention on CLS token...\n",
            " 19% 19046/100000 [51:31<2:52:55,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:58,269 >> Initializing global attention on CLS token...\n",
            " 19% 19047/100000 [51:31<2:54:05,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:58,396 >> Initializing global attention on CLS token...\n",
            " 19% 19048/100000 [51:31<2:51:31,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:58,519 >> Initializing global attention on CLS token...\n",
            " 19% 19049/100000 [51:31<2:53:47,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:58,656 >> Initializing global attention on CLS token...\n",
            " 19% 19050/100000 [51:31<2:53:58,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:58,780 >> Initializing global attention on CLS token...\n",
            " 19% 19051/100000 [51:31<2:57:13,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:58,922 >> Initializing global attention on CLS token...\n",
            " 19% 19052/100000 [51:32<2:56:52,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:59,048 >> Initializing global attention on CLS token...\n",
            " 19% 19053/100000 [51:32<2:53:19,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:59,173 >> Initializing global attention on CLS token...\n",
            " 19% 19054/100000 [51:32<2:53:51,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:59,299 >> Initializing global attention on CLS token...\n",
            " 19% 19055/100000 [51:32<2:53:32,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:59,432 >> Initializing global attention on CLS token...\n",
            " 19% 19056/100000 [51:32<2:57:13,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:59,566 >> Initializing global attention on CLS token...\n",
            " 19% 19057/100000 [51:32<2:55:17,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:59,696 >> Initializing global attention on CLS token...\n",
            " 19% 19058/100000 [51:32<2:57:42,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:59,833 >> Initializing global attention on CLS token...\n",
            " 19% 19059/100000 [51:33<2:56:28,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:18:59,957 >> Initializing global attention on CLS token...\n",
            " 19% 19060/100000 [51:33<2:55:35,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:00,090 >> Initializing global attention on CLS token...\n",
            " 19% 19061/100000 [51:33<2:57:08,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:00,224 >> Initializing global attention on CLS token...\n",
            " 19% 19062/100000 [51:33<2:56:36,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:00,349 >> Initializing global attention on CLS token...\n",
            " 19% 19063/100000 [51:33<2:52:19,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:00,470 >> Initializing global attention on CLS token...\n",
            " 19% 19064/100000 [51:33<2:52:56,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:00,602 >> Initializing global attention on CLS token...\n",
            " 19% 19065/100000 [51:33<2:52:35,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:00,732 >> Initializing global attention on CLS token...\n",
            " 19% 19066/100000 [51:33<2:55:14,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:00,862 >> Initializing global attention on CLS token...\n",
            " 19% 19067/100000 [51:34<2:53:36,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:00,992 >> Initializing global attention on CLS token...\n",
            " 19% 19068/100000 [51:34<2:57:05,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:01,126 >> Initializing global attention on CLS token...\n",
            " 19% 19069/100000 [51:34<2:53:23,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:01,246 >> Initializing global attention on CLS token...\n",
            " 19% 19070/100000 [51:34<2:52:04,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:01,376 >> Initializing global attention on CLS token...\n",
            " 19% 19071/100000 [51:34<2:52:07,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:01,499 >> Initializing global attention on CLS token...\n",
            " 19% 19072/100000 [51:34<2:53:35,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:01,637 >> Initializing global attention on CLS token...\n",
            " 19% 19073/100000 [51:34<2:55:10,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:01,763 >> Initializing global attention on CLS token...\n",
            " 19% 19074/100000 [51:34<2:53:46,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:01,893 >> Initializing global attention on CLS token...\n",
            " 19% 19075/100000 [51:35<2:57:07,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:02,035 >> Initializing global attention on CLS token...\n",
            " 19% 19076/100000 [51:35<2:58:00,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:02,161 >> Initializing global attention on CLS token...\n",
            " 19% 19077/100000 [51:35<2:57:52,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:02,296 >> Initializing global attention on CLS token...\n",
            " 19% 19078/100000 [51:35<2:55:23,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:02,418 >> Initializing global attention on CLS token...\n",
            " 19% 19079/100000 [51:35<2:58:25,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:02,560 >> Initializing global attention on CLS token...\n",
            " 19% 19080/100000 [51:35<2:57:50,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:02,686 >> Initializing global attention on CLS token...\n",
            " 19% 19081/100000 [51:35<2:55:55,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:02,819 >> Initializing global attention on CLS token...\n",
            " 19% 19082/100000 [51:36<2:56:42,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:02,945 >> Initializing global attention on CLS token...\n",
            " 19% 19083/100000 [51:36<2:53:13,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:03,068 >> Initializing global attention on CLS token...\n",
            " 19% 19084/100000 [51:36<2:53:02,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:03,203 >> Initializing global attention on CLS token...\n",
            " 19% 19085/100000 [51:36<2:54:08,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:03,327 >> Initializing global attention on CLS token...\n",
            " 19% 19086/100000 [51:36<2:53:11,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:03,469 >> Initializing global attention on CLS token...\n",
            " 19% 19087/100000 [51:36<2:58:49,  7.54it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:03,597 >> Initializing global attention on CLS token...\n",
            " 19% 19088/100000 [51:36<2:57:00,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:03,729 >> Initializing global attention on CLS token...\n",
            " 19% 19089/100000 [51:36<2:59:05,  7.53it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:03,861 >> Initializing global attention on CLS token...\n",
            " 19% 19090/100000 [51:37<2:54:23,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:03,982 >> Initializing global attention on CLS token...\n",
            " 19% 19091/100000 [51:37<2:55:46,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:04,120 >> Initializing global attention on CLS token...\n",
            " 19% 19092/100000 [51:37<2:55:31,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:04,244 >> Initializing global attention on CLS token...\n",
            " 19% 19093/100000 [51:37<2:51:07,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:04,364 >> Initializing global attention on CLS token...\n",
            " 19% 19094/100000 [51:37<2:50:48,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:04,495 >> Initializing global attention on CLS token...\n",
            " 19% 19095/100000 [51:37<2:52:48,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:04,622 >> Initializing global attention on CLS token...\n",
            " 19% 19096/100000 [51:37<2:52:20,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:04,753 >> Initializing global attention on CLS token...\n",
            " 19% 19097/100000 [51:37<2:54:15,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:04,883 >> Initializing global attention on CLS token...\n",
            " 19% 19098/100000 [51:38<2:51:51,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:05,006 >> Initializing global attention on CLS token...\n",
            " 19% 19099/100000 [51:38<3:01:52,  7.41it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:05,158 >> Initializing global attention on CLS token...\n",
            " 19% 19100/100000 [51:38<2:57:29,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:05,281 >> Initializing global attention on CLS token...\n",
            " 19% 19101/100000 [51:38<2:55:41,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:05,413 >> Initializing global attention on CLS token...\n",
            " 19% 19102/100000 [51:38<2:54:48,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:05,536 >> Initializing global attention on CLS token...\n",
            " 19% 19103/100000 [51:38<2:53:43,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:05,667 >> Initializing global attention on CLS token...\n",
            " 19% 19104/100000 [51:38<2:54:53,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:05,795 >> Initializing global attention on CLS token...\n",
            " 19% 19105/100000 [51:38<2:51:29,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:05,920 >> Initializing global attention on CLS token...\n",
            " 19% 19106/100000 [51:39<2:53:37,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:06,048 >> Initializing global attention on CLS token...\n",
            " 19% 19107/100000 [51:39<2:52:18,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:06,174 >> Initializing global attention on CLS token...\n",
            " 19% 19108/100000 [51:39<2:52:02,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:06,305 >> Initializing global attention on CLS token...\n",
            " 19% 19109/100000 [51:39<2:53:55,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:06,438 >> Initializing global attention on CLS token...\n",
            " 19% 19110/100000 [51:39<2:54:39,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:06,565 >> Initializing global attention on CLS token...\n",
            " 19% 19111/100000 [51:39<2:51:56,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:06,688 >> Initializing global attention on CLS token...\n",
            " 19% 19112/100000 [51:39<2:52:34,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:06,820 >> Initializing global attention on CLS token...\n",
            " 19% 19113/100000 [51:40<2:52:16,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:06,943 >> Initializing global attention on CLS token...\n",
            " 19% 19114/100000 [51:40<2:51:32,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:07,077 >> Initializing global attention on CLS token...\n",
            " 19% 19115/100000 [51:40<2:54:24,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:07,205 >> Initializing global attention on CLS token...\n",
            " 19% 19116/100000 [51:40<2:51:42,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:07,333 >> Initializing global attention on CLS token...\n",
            " 19% 19117/100000 [51:40<2:54:06,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:07,460 >> Initializing global attention on CLS token...\n",
            " 19% 19118/100000 [51:40<2:55:54,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:07,593 >> Initializing global attention on CLS token...\n",
            " 19% 19119/100000 [51:40<2:54:57,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:07,728 >> Initializing global attention on CLS token...\n",
            " 19% 19120/100000 [51:40<2:54:41,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:07,850 >> Initializing global attention on CLS token...\n",
            " 19% 19121/100000 [51:41<2:52:47,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:07,982 >> Initializing global attention on CLS token...\n",
            " 19% 19122/100000 [51:41<2:54:16,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:08,107 >> Initializing global attention on CLS token...\n",
            " 19% 19123/100000 [51:41<2:51:34,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:08,236 >> Initializing global attention on CLS token...\n",
            " 19% 19124/100000 [51:41<2:53:34,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:08,363 >> Initializing global attention on CLS token...\n",
            " 19% 19125/100000 [51:41<2:50:32,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:08,483 >> Initializing global attention on CLS token...\n",
            " 19% 19126/100000 [51:41<2:50:40,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:08,614 >> Initializing global attention on CLS token...\n",
            " 19% 19127/100000 [51:41<2:51:42,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:08,740 >> Initializing global attention on CLS token...\n",
            " 19% 19128/100000 [51:41<2:50:50,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:08,869 >> Initializing global attention on CLS token...\n",
            " 19% 19129/100000 [51:42<2:52:24,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:08,999 >> Initializing global attention on CLS token...\n",
            " 19% 19130/100000 [51:42<2:51:31,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:09,121 >> Initializing global attention on CLS token...\n",
            " 19% 19131/100000 [51:42<2:54:11,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:09,260 >> Initializing global attention on CLS token...\n",
            " 19% 19132/100000 [51:42<2:53:50,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:09,383 >> Initializing global attention on CLS token...\n",
            " 19% 19133/100000 [51:42<2:50:05,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:09,503 >> Initializing global attention on CLS token...\n",
            " 19% 19134/100000 [51:42<2:51:31,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:09,639 >> Initializing global attention on CLS token...\n",
            " 19% 19135/100000 [51:42<2:53:30,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:09,765 >> Initializing global attention on CLS token...\n",
            " 19% 19136/100000 [51:42<2:52:48,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:09,897 >> Initializing global attention on CLS token...\n",
            " 19% 19137/100000 [51:43<2:53:46,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:10,023 >> Initializing global attention on CLS token...\n",
            " 19% 19138/100000 [51:43<2:55:20,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:10,160 >> Initializing global attention on CLS token...\n",
            " 19% 19139/100000 [51:43<3:02:27,  7.39it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:10,307 >> Initializing global attention on CLS token...\n",
            " 19% 19140/100000 [51:43<2:57:51,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:10,427 >> Initializing global attention on CLS token...\n",
            " 19% 19141/100000 [51:43<2:57:20,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:10,561 >> Initializing global attention on CLS token...\n",
            " 19% 19142/100000 [51:43<2:54:52,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:10,683 >> Initializing global attention on CLS token...\n",
            " 19% 19143/100000 [51:43<2:51:11,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:10,805 >> Initializing global attention on CLS token...\n",
            " 19% 19144/100000 [51:44<2:50:57,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:10,936 >> Initializing global attention on CLS token...\n",
            " 19% 19145/100000 [51:44<2:52:12,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:11,061 >> Initializing global attention on CLS token...\n",
            " 19% 19146/100000 [51:44<2:50:58,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:11,190 >> Initializing global attention on CLS token...\n",
            " 19% 19147/100000 [51:44<2:55:12,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:11,324 >> Initializing global attention on CLS token...\n",
            " 19% 19148/100000 [51:44<2:51:59,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:11,446 >> Initializing global attention on CLS token...\n",
            " 19% 19149/100000 [51:44<2:52:38,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:11,574 >> Initializing global attention on CLS token...\n",
            " 19% 19150/100000 [51:44<2:50:02,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:11,696 >> Initializing global attention on CLS token...\n",
            " 19% 19151/100000 [51:44<2:50:01,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:11,827 >> Initializing global attention on CLS token...\n",
            " 19% 19152/100000 [51:45<2:51:06,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:11,951 >> Initializing global attention on CLS token...\n",
            " 19% 19153/100000 [51:45<2:50:49,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:12,081 >> Initializing global attention on CLS token...\n",
            " 19% 19154/100000 [51:45<2:53:52,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:12,212 >> Initializing global attention on CLS token...\n",
            " 19% 19155/100000 [51:45<2:52:23,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:12,342 >> Initializing global attention on CLS token...\n",
            " 19% 19156/100000 [51:45<2:55:47,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:12,478 >> Initializing global attention on CLS token...\n",
            " 19% 19157/100000 [51:45<2:53:51,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:12,599 >> Initializing global attention on CLS token...\n",
            " 19% 19158/100000 [51:45<2:50:57,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:12,721 >> Initializing global attention on CLS token...\n",
            " 19% 19159/100000 [51:45<2:51:43,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:12,854 >> Initializing global attention on CLS token...\n",
            " 19% 19160/100000 [51:46<2:51:22,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:12,976 >> Initializing global attention on CLS token...\n",
            " 19% 19161/100000 [51:46<2:50:45,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:13,107 >> Initializing global attention on CLS token...\n",
            " 19% 19162/100000 [51:46<2:51:46,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:13,231 >> Initializing global attention on CLS token...\n",
            " 19% 19163/100000 [51:46<2:49:21,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:13,358 >> Initializing global attention on CLS token...\n",
            " 19% 19164/100000 [51:46<2:52:35,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:13,486 >> Initializing global attention on CLS token...\n",
            " 19% 19165/100000 [51:46<2:49:29,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:13,608 >> Initializing global attention on CLS token...\n",
            " 19% 19166/100000 [51:46<2:50:41,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:13,740 >> Initializing global attention on CLS token...\n",
            " 19% 19167/100000 [51:46<2:50:44,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:13,863 >> Initializing global attention on CLS token...\n",
            " 19% 19168/100000 [51:47<2:50:53,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:13,994 >> Initializing global attention on CLS token...\n",
            " 19% 19169/100000 [51:47<2:54:33,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:14,125 >> Initializing global attention on CLS token...\n",
            " 19% 19170/100000 [51:47<2:50:41,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:14,246 >> Initializing global attention on CLS token...\n",
            " 19% 19171/100000 [51:47<2:51:48,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:14,375 >> Initializing global attention on CLS token...\n",
            " 19% 19172/100000 [51:47<2:50:10,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:14,498 >> Initializing global attention on CLS token...\n",
            " 19% 19173/100000 [51:47<2:48:07,  8.01it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:14,620 >> Initializing global attention on CLS token...\n",
            " 19% 19174/100000 [51:47<2:49:24,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:14,752 >> Initializing global attention on CLS token...\n",
            " 19% 19175/100000 [51:47<2:52:26,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:14,880 >> Initializing global attention on CLS token...\n",
            " 19% 19176/100000 [51:48<2:51:33,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:15,011 >> Initializing global attention on CLS token...\n",
            " 19% 19177/100000 [51:48<2:53:12,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:15,139 >> Initializing global attention on CLS token...\n",
            " 19% 19178/100000 [51:48<2:50:28,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:15,260 >> Initializing global attention on CLS token...\n",
            " 19% 19179/100000 [51:48<2:53:51,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:15,400 >> Initializing global attention on CLS token...\n",
            " 19% 19180/100000 [51:48<2:53:14,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:15,522 >> Initializing global attention on CLS token...\n",
            " 19% 19181/100000 [51:48<2:52:23,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:15,652 >> Initializing global attention on CLS token...\n",
            " 19% 19182/100000 [51:48<2:54:50,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:15,787 >> Initializing global attention on CLS token...\n",
            " 19% 19183/100000 [51:48<2:52:28,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:15,907 >> Initializing global attention on CLS token...\n",
            " 19% 19184/100000 [51:49<2:49:55,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:16,029 >> Initializing global attention on CLS token...\n",
            " 19% 19185/100000 [51:49<2:51:37,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:16,165 >> Initializing global attention on CLS token...\n",
            " 19% 19186/100000 [51:49<2:53:16,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:16,290 >> Initializing global attention on CLS token...\n",
            " 19% 19187/100000 [51:49<2:51:10,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:16,418 >> Initializing global attention on CLS token...\n",
            " 19% 19188/100000 [51:49<2:54:43,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:16,550 >> Initializing global attention on CLS token...\n",
            " 19% 19189/100000 [51:49<2:52:47,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:16,678 >> Initializing global attention on CLS token...\n",
            " 19% 19190/100000 [51:49<2:55:29,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:16,811 >> Initializing global attention on CLS token...\n",
            " 19% 19191/100000 [51:50<2:52:03,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:16,932 >> Initializing global attention on CLS token...\n",
            " 19% 19192/100000 [51:50<2:51:23,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:17,063 >> Initializing global attention on CLS token...\n",
            " 19% 19193/100000 [51:50<2:52:01,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:17,187 >> Initializing global attention on CLS token...\n",
            " 19% 19194/100000 [51:50<2:50:50,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:17,316 >> Initializing global attention on CLS token...\n",
            " 19% 19195/100000 [51:50<2:52:22,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:17,443 >> Initializing global attention on CLS token...\n",
            " 19% 19196/100000 [51:50<2:54:43,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:17,580 >> Initializing global attention on CLS token...\n",
            " 19% 19197/100000 [51:50<2:54:27,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:17,705 >> Initializing global attention on CLS token...\n",
            " 19% 19198/100000 [51:50<2:52:39,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:17,834 >> Initializing global attention on CLS token...\n",
            " 19% 19199/100000 [51:51<2:54:05,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:17,966 >> Initializing global attention on CLS token...\n",
            " 19% 19200/100000 [51:51<2:53:00,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:18,089 >> Initializing global attention on CLS token...\n",
            " 19% 19201/100000 [51:51<2:53:14,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:18,223 >> Initializing global attention on CLS token...\n",
            " 19% 19202/100000 [51:51<2:52:14,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:18,344 >> Initializing global attention on CLS token...\n",
            " 19% 19203/100000 [51:51<2:51:14,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:18,474 >> Initializing global attention on CLS token...\n",
            " 19% 19204/100000 [51:51<2:53:01,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:18,601 >> Initializing global attention on CLS token...\n",
            " 19% 19205/100000 [51:51<2:54:28,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:18,737 >> Initializing global attention on CLS token...\n",
            " 19% 19206/100000 [51:51<2:53:31,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:18,860 >> Initializing global attention on CLS token...\n",
            " 19% 19207/100000 [51:52<2:49:58,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:18,980 >> Initializing global attention on CLS token...\n",
            " 19% 19208/100000 [51:52<2:50:10,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:19,111 >> Initializing global attention on CLS token...\n",
            " 19% 19209/100000 [51:52<2:49:42,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:19,234 >> Initializing global attention on CLS token...\n",
            " 19% 19210/100000 [51:52<2:48:36,  7.99it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:19,355 >> Initializing global attention on CLS token...\n",
            " 19% 19211/100000 [51:52<2:48:41,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:19,485 >> Initializing global attention on CLS token...\n",
            " 19% 19212/100000 [51:52<2:50:44,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:19,612 >> Initializing global attention on CLS token...\n",
            " 19% 19213/100000 [51:52<2:50:41,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:19,743 >> Initializing global attention on CLS token...\n",
            " 19% 19214/100000 [51:52<2:51:57,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:19,868 >> Initializing global attention on CLS token...\n",
            " 19% 19215/100000 [51:53<2:50:15,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:19,991 >> Initializing global attention on CLS token...\n",
            " 19% 19216/100000 [51:53<2:51:31,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:20,124 >> Initializing global attention on CLS token...\n",
            " 19% 19217/100000 [51:53<2:50:43,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:20,246 >> Initializing global attention on CLS token...\n",
            " 19% 19218/100000 [51:53<2:47:56,  8.02it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:20,366 >> Initializing global attention on CLS token...\n",
            " 19% 19219/100000 [51:53<2:50:17,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:20,503 >> Initializing global attention on CLS token...\n",
            " 19% 19220/100000 [51:53<2:52:42,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:20,632 >> Initializing global attention on CLS token...\n",
            " 19% 19221/100000 [51:53<2:54:00,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:20,765 >> Initializing global attention on CLS token...\n",
            " 19% 19222/100000 [51:53<2:53:54,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:20,890 >> Initializing global attention on CLS token...\n",
            " 19% 19223/100000 [51:54<2:50:45,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:21,015 >> Initializing global attention on CLS token...\n",
            " 19% 19224/100000 [51:54<2:52:49,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:21,145 >> Initializing global attention on CLS token...\n",
            " 19% 19225/100000 [51:54<2:50:25,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:21,266 >> Initializing global attention on CLS token...\n",
            " 19% 19226/100000 [51:54<2:49:46,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:21,398 >> Initializing global attention on CLS token...\n",
            " 19% 19227/100000 [51:54<2:51:39,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:21,521 >> Initializing global attention on CLS token...\n",
            " 19% 19228/100000 [51:54<2:54:03,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:21,660 >> Initializing global attention on CLS token...\n",
            " 19% 19229/100000 [51:54<2:56:12,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:21,790 >> Initializing global attention on CLS token...\n",
            " 19% 19230/100000 [51:54<2:51:53,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:21,911 >> Initializing global attention on CLS token...\n",
            " 19% 19231/100000 [51:55<2:52:08,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:22,038 >> Initializing global attention on CLS token...\n",
            " 19% 19232/100000 [51:55<2:49:06,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:22,158 >> Initializing global attention on CLS token...\n",
            " 19% 19233/100000 [51:55<2:50:27,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:22,293 >> Initializing global attention on CLS token...\n",
            " 19% 19234/100000 [51:55<2:51:46,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:22,417 >> Initializing global attention on CLS token...\n",
            " 19% 19235/100000 [51:55<2:52:49,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:22,552 >> Initializing global attention on CLS token...\n",
            " 19% 19236/100000 [51:55<2:58:40,  7.53it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:22,690 >> Initializing global attention on CLS token...\n",
            " 19% 19237/100000 [51:55<2:57:01,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:22,824 >> Initializing global attention on CLS token...\n",
            " 19% 19238/100000 [51:56<2:55:29,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:22,947 >> Initializing global attention on CLS token...\n",
            " 19% 19239/100000 [51:56<2:54:38,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:23,079 >> Initializing global attention on CLS token...\n",
            " 19% 19240/100000 [51:56<2:54:50,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:23,207 >> Initializing global attention on CLS token...\n",
            " 19% 19241/100000 [51:56<2:55:29,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:23,341 >> Initializing global attention on CLS token...\n",
            " 19% 19242/100000 [51:56<2:55:12,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:23,471 >> Initializing global attention on CLS token...\n",
            " 19% 19243/100000 [51:56<2:56:19,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:23,600 >> Initializing global attention on CLS token...\n",
            " 19% 19244/100000 [51:56<2:54:37,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:23,729 >> Initializing global attention on CLS token...\n",
            " 19% 19245/100000 [51:56<2:54:08,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:23,856 >> Initializing global attention on CLS token...\n",
            " 19% 19246/100000 [51:57<2:53:41,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:23,988 >> Initializing global attention on CLS token...\n",
            " 19% 19247/100000 [51:57<2:53:20,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:24,111 >> Initializing global attention on CLS token...\n",
            " 19% 19248/100000 [51:57<2:50:43,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:24,238 >> Initializing global attention on CLS token...\n",
            " 19% 19249/100000 [51:57<2:53:04,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:24,366 >> Initializing global attention on CLS token...\n",
            " 19% 19250/100000 [51:57<2:53:07,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:24,499 >> Initializing global attention on CLS token...\n",
            " 19% 19251/100000 [51:57<2:53:40,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:24,625 >> Initializing global attention on CLS token...\n",
            " 19% 19252/100000 [51:57<2:56:32,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:24,766 >> Initializing global attention on CLS token...\n",
            " 19% 19253/100000 [51:57<2:58:02,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:24,896 >> Initializing global attention on CLS token...\n",
            " 19% 19254/100000 [51:58<2:52:39,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:25,015 >> Initializing global attention on CLS token...\n",
            " 19% 19255/100000 [51:58<2:52:39,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:25,149 >> Initializing global attention on CLS token...\n",
            " 19% 19256/100000 [51:58<2:54:17,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:25,276 >> Initializing global attention on CLS token...\n",
            " 19% 19257/100000 [51:58<2:53:31,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:25,407 >> Initializing global attention on CLS token...\n",
            " 19% 19258/100000 [51:58<2:52:39,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:25,530 >> Initializing global attention on CLS token...\n",
            " 19% 19259/100000 [51:58<2:53:03,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:25,664 >> Initializing global attention on CLS token...\n",
            " 19% 19260/100000 [51:58<2:54:15,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:25,791 >> Initializing global attention on CLS token...\n",
            " 19% 19261/100000 [51:58<2:50:41,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:25,911 >> Initializing global attention on CLS token...\n",
            " 19% 19262/100000 [51:59<2:50:39,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:26,042 >> Initializing global attention on CLS token...\n",
            " 19% 19263/100000 [51:59<2:50:19,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:26,165 >> Initializing global attention on CLS token...\n",
            " 19% 19264/100000 [51:59<2:52:43,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:26,301 >> Initializing global attention on CLS token...\n",
            " 19% 19265/100000 [51:59<2:53:07,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:26,426 >> Initializing global attention on CLS token...\n",
            " 19% 19266/100000 [51:59<2:54:25,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:26,564 >> Initializing global attention on CLS token...\n",
            " 19% 19267/100000 [51:59<2:53:43,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:26,686 >> Initializing global attention on CLS token...\n",
            " 19% 19268/100000 [51:59<3:00:09,  7.47it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:26,837 >> Initializing global attention on CLS token...\n",
            " 19% 19269/100000 [52:00<2:58:17,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:26,961 >> Initializing global attention on CLS token...\n",
            " 19% 19270/100000 [52:00<2:55:30,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:27,090 >> Initializing global attention on CLS token...\n",
            " 19% 19271/100000 [52:00<2:55:47,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:27,217 >> Initializing global attention on CLS token...\n",
            " 19% 19272/100000 [52:00<2:58:40,  7.53it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:27,359 >> Initializing global attention on CLS token...\n",
            " 19% 19273/100000 [52:00<2:57:34,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:27,485 >> Initializing global attention on CLS token...\n",
            " 19% 19274/100000 [52:00<2:54:39,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:27,615 >> Initializing global attention on CLS token...\n",
            " 19% 19275/100000 [52:00<2:55:03,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:27,741 >> Initializing global attention on CLS token...\n",
            " 19% 19276/100000 [52:00<2:54:31,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:27,878 >> Initializing global attention on CLS token...\n",
            " 19% 19277/100000 [52:01<2:55:08,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:28,001 >> Initializing global attention on CLS token...\n",
            " 19% 19278/100000 [52:01<2:53:10,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:28,130 >> Initializing global attention on CLS token...\n",
            " 19% 19279/100000 [52:01<2:53:14,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:28,255 >> Initializing global attention on CLS token...\n",
            " 19% 19280/100000 [52:01<2:53:59,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:28,390 >> Initializing global attention on CLS token...\n",
            " 19% 19281/100000 [52:01<2:53:28,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:28,513 >> Initializing global attention on CLS token...\n",
            " 19% 19282/100000 [52:01<2:50:41,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:28,638 >> Initializing global attention on CLS token...\n",
            " 19% 19283/100000 [52:01<2:54:00,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:28,779 >> Initializing global attention on CLS token...\n",
            " 19% 19284/100000 [52:01<2:59:51,  7.48it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:28,919 >> Initializing global attention on CLS token...\n",
            " 19% 19285/100000 [52:02<2:57:59,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:29,043 >> Initializing global attention on CLS token...\n",
            " 19% 19286/100000 [52:02<2:52:47,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:29,163 >> Initializing global attention on CLS token...\n",
            " 19% 19287/100000 [52:02<2:54:10,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:29,299 >> Initializing global attention on CLS token...\n",
            " 19% 19288/100000 [52:02<2:52:47,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:29,421 >> Initializing global attention on CLS token...\n",
            " 19% 19289/100000 [52:02<2:50:00,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:29,543 >> Initializing global attention on CLS token...\n",
            " 19% 19290/100000 [52:02<2:51:11,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:29,676 >> Initializing global attention on CLS token...\n",
            " 19% 19291/100000 [52:02<2:51:16,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:29,805 >> Initializing global attention on CLS token...\n",
            " 19% 19292/100000 [52:03<2:59:57,  7.47it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:29,948 >> Initializing global attention on CLS token...\n",
            " 19% 19293/100000 [52:03<2:54:06,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:30,068 >> Initializing global attention on CLS token...\n",
            " 19% 19294/100000 [52:03<2:53:11,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:30,199 >> Initializing global attention on CLS token...\n",
            " 19% 19295/100000 [52:03<2:52:33,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:30,322 >> Initializing global attention on CLS token...\n",
            " 19% 19296/100000 [52:03<2:55:00,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:30,463 >> Initializing global attention on CLS token...\n",
            " 19% 19297/100000 [52:03<2:55:02,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:30,586 >> Initializing global attention on CLS token...\n",
            " 19% 19298/100000 [52:03<2:54:00,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:30,719 >> Initializing global attention on CLS token...\n",
            " 19% 19299/100000 [52:03<2:54:33,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:30,848 >> Initializing global attention on CLS token...\n",
            " 19% 19300/100000 [52:04<2:56:16,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:30,979 >> Initializing global attention on CLS token...\n",
            " 19% 19301/100000 [52:04<2:51:20,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:31,098 >> Initializing global attention on CLS token...\n",
            " 19% 19302/100000 [52:04<2:50:53,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:31,228 >> Initializing global attention on CLS token...\n",
            " 19% 19303/100000 [52:04<2:50:38,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:31,350 >> Initializing global attention on CLS token...\n",
            " 19% 19304/100000 [52:04<2:48:06,  8.00it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:31,471 >> Initializing global attention on CLS token...\n",
            " 19% 19305/100000 [52:04<2:47:52,  8.01it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:31,600 >> Initializing global attention on CLS token...\n",
            " 19% 19306/100000 [52:04<2:50:06,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:31,726 >> Initializing global attention on CLS token...\n",
            " 19% 19307/100000 [52:04<2:49:13,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:31,855 >> Initializing global attention on CLS token...\n",
            " 19% 19308/100000 [52:05<2:52:31,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:31,990 >> Initializing global attention on CLS token...\n",
            " 19% 19309/100000 [52:05<2:53:39,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:32,120 >> Initializing global attention on CLS token...\n",
            " 19% 19310/100000 [52:05<2:53:43,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:32,245 >> Initializing global attention on CLS token...\n",
            " 19% 19311/100000 [52:05<2:52:27,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:32,375 >> Initializing global attention on CLS token...\n",
            " 19% 19312/100000 [52:05<2:52:02,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:32,498 >> Initializing global attention on CLS token...\n",
            " 19% 19313/100000 [52:05<2:49:38,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:32,620 >> Initializing global attention on CLS token...\n",
            " 19% 19314/100000 [52:05<2:54:59,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:32,762 >> Initializing global attention on CLS token...\n",
            " 19% 19315/100000 [52:05<2:53:03,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:32,885 >> Initializing global attention on CLS token...\n",
            " 19% 19316/100000 [52:06<2:51:34,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:33,014 >> Initializing global attention on CLS token...\n",
            " 19% 19317/100000 [52:06<2:53:42,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:33,147 >> Initializing global attention on CLS token...\n",
            " 19% 19318/100000 [52:06<2:52:02,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:33,267 >> Initializing global attention on CLS token...\n",
            " 19% 19319/100000 [52:06<2:49:16,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:33,389 >> Initializing global attention on CLS token...\n",
            " 19% 19320/100000 [52:06<2:50:13,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:33,521 >> Initializing global attention on CLS token...\n",
            " 19% 19321/100000 [52:06<2:50:32,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:33,645 >> Initializing global attention on CLS token...\n",
            " 19% 19322/100000 [52:06<2:51:05,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:33,777 >> Initializing global attention on CLS token...\n",
            " 19% 19323/100000 [52:06<2:52:54,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:33,904 >> Initializing global attention on CLS token...\n",
            " 19% 19324/100000 [52:07<2:49:38,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:34,030 >> Initializing global attention on CLS token...\n",
            " 19% 19325/100000 [52:07<2:56:43,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:34,169 >> Initializing global attention on CLS token...\n",
            " 19% 19326/100000 [52:07<2:52:19,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:34,289 >> Initializing global attention on CLS token...\n",
            " 19% 19327/100000 [52:07<2:51:18,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:34,414 >> Initializing global attention on CLS token...\n",
            " 19% 19328/100000 [52:07<2:50:36,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:34,547 >> Initializing global attention on CLS token...\n",
            " 19% 19329/100000 [52:07<2:51:42,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:34,671 >> Initializing global attention on CLS token...\n",
            " 19% 19330/100000 [52:07<2:52:18,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:34,804 >> Initializing global attention on CLS token...\n",
            " 19% 19331/100000 [52:08<2:53:23,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:34,930 >> Initializing global attention on CLS token...\n",
            " 19% 19332/100000 [52:08<2:50:39,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:35,060 >> Initializing global attention on CLS token...\n",
            " 19% 19333/100000 [52:08<2:54:39,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:35,191 >> Initializing global attention on CLS token...\n",
            " 19% 19334/100000 [52:08<2:51:09,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:35,310 >> Initializing global attention on CLS token...\n",
            " 19% 19335/100000 [52:08<2:51:12,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:35,444 >> Initializing global attention on CLS token...\n",
            " 19% 19336/100000 [52:08<2:51:50,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:35,566 >> Initializing global attention on CLS token...\n",
            " 19% 19337/100000 [52:08<2:50:50,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:35,697 >> Initializing global attention on CLS token...\n",
            " 19% 19338/100000 [52:08<2:53:36,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:35,826 >> Initializing global attention on CLS token...\n",
            " 19% 19339/100000 [52:09<2:50:18,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:35,950 >> Initializing global attention on CLS token...\n",
            " 19% 19340/100000 [52:09<2:52:52,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:36,080 >> Initializing global attention on CLS token...\n",
            " 19% 19341/100000 [52:09<2:53:24,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:36,215 >> Initializing global attention on CLS token...\n",
            " 19% 19342/100000 [52:09<2:54:07,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:36,342 >> Initializing global attention on CLS token...\n",
            " 19% 19343/100000 [52:09<2:52:28,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:36,470 >> Initializing global attention on CLS token...\n",
            " 19% 19344/100000 [52:09<2:52:56,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:36,595 >> Initializing global attention on CLS token...\n",
            " 19% 19345/100000 [52:09<2:49:30,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:36,716 >> Initializing global attention on CLS token...\n",
            " 19% 19346/100000 [52:09<2:50:29,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:36,849 >> Initializing global attention on CLS token...\n",
            " 19% 19347/100000 [52:10<2:54:05,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:36,986 >> Initializing global attention on CLS token...\n",
            " 19% 19348/100000 [52:10<2:57:19,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:37,120 >> Initializing global attention on CLS token...\n",
            " 19% 19349/100000 [52:10<2:54:01,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:37,241 >> Initializing global attention on CLS token...\n",
            " 19% 19350/100000 [52:10<2:54:41,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:37,377 >> Initializing global attention on CLS token...\n",
            " 19% 19351/100000 [52:10<2:53:43,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:37,500 >> Initializing global attention on CLS token...\n",
            " 19% 19352/100000 [52:10<2:50:09,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:37,620 >> Initializing global attention on CLS token...\n",
            " 19% 19353/100000 [52:10<2:50:45,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:37,754 >> Initializing global attention on CLS token...\n",
            " 19% 19354/100000 [52:10<2:52:48,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:37,880 >> Initializing global attention on CLS token...\n",
            " 19% 19355/100000 [52:11<2:52:09,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:38,012 >> Initializing global attention on CLS token...\n",
            " 19% 19356/100000 [52:11<2:51:39,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:38,135 >> Initializing global attention on CLS token...\n",
            " 19% 19357/100000 [52:11<2:54:49,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:38,274 >> Initializing global attention on CLS token...\n",
            " 19% 19358/100000 [52:11<2:53:20,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:38,397 >> Initializing global attention on CLS token...\n",
            " 19% 19359/100000 [52:11<2:52:51,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:38,528 >> Initializing global attention on CLS token...\n",
            " 19% 19360/100000 [52:11<2:53:02,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:38,653 >> Initializing global attention on CLS token...\n",
            " 19% 19361/100000 [52:11<2:53:23,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:38,789 >> Initializing global attention on CLS token...\n",
            " 19% 19362/100000 [52:11<2:54:04,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:38,913 >> Initializing global attention on CLS token...\n",
            " 19% 19363/100000 [52:12<2:52:50,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:39,044 >> Initializing global attention on CLS token...\n",
            " 19% 19364/100000 [52:12<2:52:20,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:39,168 >> Initializing global attention on CLS token...\n",
            " 19% 19365/100000 [52:12<2:49:34,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:39,294 >> Initializing global attention on CLS token...\n",
            " 19% 19366/100000 [52:12<2:58:43,  7.52it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:39,444 >> Initializing global attention on CLS token...\n",
            " 19% 19367/100000 [52:12<2:56:17,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:39,565 >> Initializing global attention on CLS token...\n",
            " 19% 19368/100000 [52:12<2:54:05,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:39,696 >> Initializing global attention on CLS token...\n",
            " 19% 19369/100000 [52:12<2:53:05,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:39,818 >> Initializing global attention on CLS token...\n",
            " 19% 19370/100000 [52:13<2:52:47,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:39,950 >> Initializing global attention on CLS token...\n",
            " 19% 19371/100000 [52:13<2:52:45,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:40,074 >> Initializing global attention on CLS token...\n",
            " 19% 19372/100000 [52:13<2:49:24,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:40,199 >> Initializing global attention on CLS token...\n",
            " 19% 19373/100000 [52:13<2:58:04,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:40,347 >> Initializing global attention on CLS token...\n",
            " 19% 19374/100000 [52:13<2:56:04,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:40,470 >> Initializing global attention on CLS token...\n",
            " 19% 19375/100000 [52:13<2:54:15,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:40,600 >> Initializing global attention on CLS token...\n",
            " 19% 19376/100000 [52:13<2:54:59,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:40,732 >> Initializing global attention on CLS token...\n",
            " 19% 19377/100000 [52:13<2:55:37,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:40,860 >> Initializing global attention on CLS token...\n",
            " 19% 19378/100000 [52:14<2:51:42,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:40,981 >> Initializing global attention on CLS token...\n",
            " 19% 19379/100000 [52:14<2:56:24,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:41,122 >> Initializing global attention on CLS token...\n",
            " 19% 19380/100000 [52:14<2:54:43,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:41,251 >> Initializing global attention on CLS token...\n",
            " 19% 19381/100000 [52:14<2:57:23,  7.57it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:41,386 >> Initializing global attention on CLS token...\n",
            " 19% 19382/100000 [52:14<2:54:02,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:41,507 >> Initializing global attention on CLS token...\n",
            " 19% 19383/100000 [52:14<2:54:36,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:41,643 >> Initializing global attention on CLS token...\n",
            " 19% 19384/100000 [52:14<2:55:23,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:41,775 >> Initializing global attention on CLS token...\n",
            " 19% 19385/100000 [52:14<2:55:03,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:41,900 >> Initializing global attention on CLS token...\n",
            " 19% 19386/100000 [52:15<2:50:46,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:42,027 >> Initializing global attention on CLS token...\n",
            " 19% 19387/100000 [52:15<2:53:49,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:42,154 >> Initializing global attention on CLS token...\n",
            " 19% 19388/100000 [52:15<2:52:19,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:42,284 >> Initializing global attention on CLS token...\n",
            " 19% 19389/100000 [52:15<2:53:50,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:42,415 >> Initializing global attention on CLS token...\n",
            " 19% 19390/100000 [52:15<2:53:49,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:42,546 >> Initializing global attention on CLS token...\n",
            " 19% 19391/100000 [52:15<2:54:10,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:42,671 >> Initializing global attention on CLS token...\n",
            " 19% 19392/100000 [52:15<2:50:25,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:42,792 >> Initializing global attention on CLS token...\n",
            " 19% 19393/100000 [52:15<2:50:12,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:42,922 >> Initializing global attention on CLS token...\n",
            " 19% 19394/100000 [52:16<2:50:49,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:43,053 >> Initializing global attention on CLS token...\n",
            " 19% 19395/100000 [52:16<2:54:26,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:43,185 >> Initializing global attention on CLS token...\n",
            " 19% 19396/100000 [52:16<2:52:30,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:43,307 >> Initializing global attention on CLS token...\n",
            " 19% 19397/100000 [52:16<2:53:28,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:43,444 >> Initializing global attention on CLS token...\n",
            " 19% 19398/100000 [52:16<2:53:27,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:43,568 >> Initializing global attention on CLS token...\n",
            " 19% 19399/100000 [52:16<2:52:53,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:43,700 >> Initializing global attention on CLS token...\n",
            " 19% 19400/100000 [52:16<2:52:34,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:43,823 >> Initializing global attention on CLS token...\n",
            " 19% 19401/100000 [52:17<2:49:29,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:43,944 >> Initializing global attention on CLS token...\n",
            " 19% 19402/100000 [52:17<2:51:26,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:44,076 >> Initializing global attention on CLS token...\n",
            " 19% 19403/100000 [52:17<2:50:46,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:44,205 >> Initializing global attention on CLS token...\n",
            " 19% 19404/100000 [52:17<2:51:14,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:44,330 >> Initializing global attention on CLS token...\n",
            " 19% 19405/100000 [52:17<2:49:50,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:44,458 >> Initializing global attention on CLS token...\n",
            " 19% 19406/100000 [52:17<2:53:48,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:44,594 >> Initializing global attention on CLS token...\n",
            " 19% 19407/100000 [52:17<2:52:19,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:44,715 >> Initializing global attention on CLS token...\n",
            " 19% 19408/100000 [52:17<2:52:11,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:44,847 >> Initializing global attention on CLS token...\n",
            " 19% 19409/100000 [52:18<2:51:47,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:44,972 >> Initializing global attention on CLS token...\n",
            " 19% 19410/100000 [52:18<2:50:53,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:45,101 >> Initializing global attention on CLS token...\n",
            " 19% 19411/100000 [52:18<2:56:19,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:45,237 >> Initializing global attention on CLS token...\n",
            " 19% 19412/100000 [52:18<2:51:52,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:45,361 >> Initializing global attention on CLS token...\n",
            " 19% 19413/100000 [52:18<2:55:37,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:45,501 >> Initializing global attention on CLS token...\n",
            " 19% 19414/100000 [52:18<2:55:35,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:45,625 >> Initializing global attention on CLS token...\n",
            " 19% 19415/100000 [52:18<2:53:41,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:45,757 >> Initializing global attention on CLS token...\n",
            " 19% 19416/100000 [52:18<2:54:23,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:45,885 >> Initializing global attention on CLS token...\n",
            " 19% 19417/100000 [52:19<2:53:36,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:46,014 >> Initializing global attention on CLS token...\n",
            " 19% 19418/100000 [52:19<2:53:23,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:46,139 >> Initializing global attention on CLS token...\n",
            " 19% 19419/100000 [52:19<2:49:31,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:46,259 >> Initializing global attention on CLS token...\n",
            " 19% 19420/100000 [52:19<2:50:57,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:46,389 >> Initializing global attention on CLS token...\n",
            " 19% 19421/100000 [52:19<2:49:26,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:46,516 >> Initializing global attention on CLS token...\n",
            " 19% 19422/100000 [52:19<2:51:41,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:46,648 >> Initializing global attention on CLS token...\n",
            " 19% 19423/100000 [52:19<2:53:48,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:46,778 >> Initializing global attention on CLS token...\n",
            " 19% 19424/100000 [52:19<2:52:08,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:46,906 >> Initializing global attention on CLS token...\n",
            " 19% 19425/100000 [52:20<2:53:34,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:47,034 >> Initializing global attention on CLS token...\n",
            " 19% 19426/100000 [52:20<2:49:37,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:47,153 >> Initializing global attention on CLS token...\n",
            " 19% 19427/100000 [52:20<2:49:44,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:47,288 >> Initializing global attention on CLS token...\n",
            " 19% 19428/100000 [52:20<2:50:50,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:47,409 >> Initializing global attention on CLS token...\n",
            " 19% 19429/100000 [52:20<2:49:04,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:47,537 >> Initializing global attention on CLS token...\n",
            " 19% 19430/100000 [52:20<2:54:04,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:47,676 >> Initializing global attention on CLS token...\n",
            " 19% 19431/100000 [52:20<2:54:19,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:47,800 >> Initializing global attention on CLS token...\n",
            " 19% 19432/100000 [52:21<2:53:05,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:47,932 >> Initializing global attention on CLS token...\n",
            " 19% 19433/100000 [52:21<2:55:49,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:48,063 >> Initializing global attention on CLS token...\n",
            " 19% 19434/100000 [52:21<2:53:58,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:48,194 >> Initializing global attention on CLS token...\n",
            " 19% 19435/100000 [52:21<2:54:44,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:48,321 >> Initializing global attention on CLS token...\n",
            " 19% 19436/100000 [52:21<2:50:49,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:48,441 >> Initializing global attention on CLS token...\n",
            " 19% 19437/100000 [52:21<2:58:01,  7.54it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:48,594 >> Initializing global attention on CLS token...\n",
            " 19% 19438/100000 [52:21<2:57:16,  7.57it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:48,717 >> Initializing global attention on CLS token...\n",
            " 19% 19439/100000 [52:21<2:53:26,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:48,839 >> Initializing global attention on CLS token...\n",
            " 19% 19440/100000 [52:22<2:52:49,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:48,972 >> Initializing global attention on CLS token...\n",
            " 19% 19441/100000 [52:22<2:52:27,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:49,100 >> Initializing global attention on CLS token...\n",
            " 19% 19442/100000 [52:22<2:56:32,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:49,233 >> Initializing global attention on CLS token...\n",
            " 19% 19443/100000 [52:22<2:52:03,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:49,354 >> Initializing global attention on CLS token...\n",
            " 19% 19444/100000 [52:22<2:51:11,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:49,485 >> Initializing global attention on CLS token...\n",
            " 19% 19445/100000 [52:22<2:54:25,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:49,616 >> Initializing global attention on CLS token...\n",
            " 19% 19446/100000 [52:22<2:54:58,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:49,751 >> Initializing global attention on CLS token...\n",
            " 19% 19447/100000 [52:22<2:58:49,  7.51it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:49,886 >> Initializing global attention on CLS token...\n",
            " 19% 19448/100000 [52:23<2:55:47,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:50,018 >> Initializing global attention on CLS token...\n",
            " 19% 19449/100000 [52:23<2:55:55,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:50,143 >> Initializing global attention on CLS token...\n",
            " 19% 19450/100000 [52:23<2:51:51,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:50,264 >> Initializing global attention on CLS token...\n",
            " 19% 19451/100000 [52:23<2:51:16,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:50,398 >> Initializing global attention on CLS token...\n",
            " 19% 19452/100000 [52:23<2:52:38,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:50,522 >> Initializing global attention on CLS token...\n",
            " 19% 19453/100000 [52:23<2:54:12,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:50,660 >> Initializing global attention on CLS token...\n",
            " 19% 19454/100000 [52:23<2:55:13,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:50,788 >> Initializing global attention on CLS token...\n",
            " 19% 19455/100000 [52:23<2:53:56,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:50,918 >> Initializing global attention on CLS token...\n",
            " 19% 19456/100000 [52:24<2:54:52,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:51,049 >> Initializing global attention on CLS token...\n",
            " 19% 19457/100000 [52:24<2:51:54,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:51,169 >> Initializing global attention on CLS token...\n",
            " 19% 19458/100000 [52:24<2:51:34,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:51,302 >> Initializing global attention on CLS token...\n",
            " 19% 19459/100000 [52:24<2:54:51,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:51,432 >> Initializing global attention on CLS token...\n",
            " 19% 19460/100000 [52:24<2:53:12,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:51,564 >> Initializing global attention on CLS token...\n",
            " 19% 19461/100000 [52:24<2:56:16,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:51,695 >> Initializing global attention on CLS token...\n",
            " 19% 19462/100000 [52:24<2:52:53,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:51,823 >> Initializing global attention on CLS token...\n",
            " 19% 19463/100000 [52:25<2:54:43,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:51,956 >> Initializing global attention on CLS token...\n",
            " 19% 19464/100000 [52:25<2:52:53,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:52,077 >> Initializing global attention on CLS token...\n",
            " 19% 19465/100000 [52:25<2:52:03,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:52,209 >> Initializing global attention on CLS token...\n",
            " 19% 19466/100000 [52:25<2:52:32,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:52,333 >> Initializing global attention on CLS token...\n",
            " 19% 19467/100000 [52:25<2:52:32,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:52,466 >> Initializing global attention on CLS token...\n",
            " 19% 19468/100000 [52:25<2:53:26,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:52,593 >> Initializing global attention on CLS token...\n",
            " 19% 19469/100000 [52:25<2:52:13,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:52,725 >> Initializing global attention on CLS token...\n",
            " 19% 19470/100000 [52:25<2:58:30,  7.52it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:52,863 >> Initializing global attention on CLS token...\n",
            " 19% 19471/100000 [52:26<2:53:45,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:52,983 >> Initializing global attention on CLS token...\n",
            " 19% 19472/100000 [52:26<2:53:30,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:53,118 >> Initializing global attention on CLS token...\n",
            " 19% 19473/100000 [52:26<2:53:31,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:53,242 >> Initializing global attention on CLS token...\n",
            " 19% 19474/100000 [52:26<2:53:10,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:53,375 >> Initializing global attention on CLS token...\n",
            " 19% 19475/100000 [52:26<2:53:38,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:53,504 >> Initializing global attention on CLS token...\n",
            " 19% 19476/100000 [52:26<2:52:20,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:53,632 >> Initializing global attention on CLS token...\n",
            " 19% 19477/100000 [52:26<2:59:42,  7.47it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:53,775 >> Initializing global attention on CLS token...\n",
            " 19% 19478/100000 [52:26<2:54:41,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:53,895 >> Initializing global attention on CLS token...\n",
            " 19% 19479/100000 [52:27<2:54:40,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:54,029 >> Initializing global attention on CLS token...\n",
            " 19% 19480/100000 [52:27<2:54:27,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:54,155 >> Initializing global attention on CLS token...\n",
            " 19% 19481/100000 [52:27<2:50:35,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:54,275 >> Initializing global attention on CLS token...\n",
            " 19% 19482/100000 [52:27<2:50:57,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:54,408 >> Initializing global attention on CLS token...\n",
            " 19% 19483/100000 [52:27<2:51:00,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:54,531 >> Initializing global attention on CLS token...\n",
            " 19% 19484/100000 [52:27<2:51:16,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:54,664 >> Initializing global attention on CLS token...\n",
            " 19% 19485/100000 [52:27<2:56:04,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:54,799 >> Initializing global attention on CLS token...\n",
            " 19% 19486/100000 [52:27<2:51:48,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:54,919 >> Initializing global attention on CLS token...\n",
            " 19% 19487/100000 [52:28<2:52:38,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:55,055 >> Initializing global attention on CLS token...\n",
            " 19% 19488/100000 [52:28<2:52:43,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:55,178 >> Initializing global attention on CLS token...\n",
            " 19% 19489/100000 [52:28<2:49:44,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:55,300 >> Initializing global attention on CLS token...\n",
            " 19% 19490/100000 [52:28<2:51:14,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:55,436 >> Initializing global attention on CLS token...\n",
            " 19% 19491/100000 [52:28<2:53:34,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:55,563 >> Initializing global attention on CLS token...\n",
            " 19% 19492/100000 [52:28<2:52:20,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:55,694 >> Initializing global attention on CLS token...\n",
            " 19% 19493/100000 [52:28<2:56:07,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:55,827 >> Initializing global attention on CLS token...\n",
            " 19% 19494/100000 [52:29<2:54:58,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:55,960 >> Initializing global attention on CLS token...\n",
            " 19% 19495/100000 [52:29<2:55:16,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:56,087 >> Initializing global attention on CLS token...\n",
            " 19% 19496/100000 [52:29<2:51:40,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:56,208 >> Initializing global attention on CLS token...\n",
            " 19% 19497/100000 [52:29<2:53:46,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:56,346 >> Initializing global attention on CLS token...\n",
            " 19% 19498/100000 [52:29<2:52:38,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:56,468 >> Initializing global attention on CLS token...\n",
            " 19% 19499/100000 [52:29<2:49:24,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:56,589 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.4358, 'learning_rate': 8.0506e-05, 'epoch': 3.9}\n",
            " 20% 19500/100000 [52:29<2:53:22,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:56,743 >> Initializing global attention on CLS token...\n",
            " 20% 19501/100000 [52:29<3:01:03,  7.41it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:56,875 >> Initializing global attention on CLS token...\n",
            " 20% 19502/100000 [52:30<2:59:17,  7.48it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:57,008 >> Initializing global attention on CLS token...\n",
            " 20% 19503/100000 [52:30<2:57:09,  7.57it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:57,132 >> Initializing global attention on CLS token...\n",
            " 20% 19504/100000 [52:30<2:55:41,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:57,264 >> Initializing global attention on CLS token...\n",
            " 20% 19505/100000 [52:30<2:53:41,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:57,387 >> Initializing global attention on CLS token...\n",
            " 20% 19506/100000 [52:30<2:50:49,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:57,509 >> Initializing global attention on CLS token...\n",
            " 20% 19507/100000 [52:30<2:53:07,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:57,648 >> Initializing global attention on CLS token...\n",
            " 20% 19508/100000 [52:30<2:53:35,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:57,774 >> Initializing global attention on CLS token...\n",
            " 20% 19509/100000 [52:30<2:54:24,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:57,908 >> Initializing global attention on CLS token...\n",
            " 20% 19510/100000 [52:31<2:54:11,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:58,033 >> Initializing global attention on CLS token...\n",
            " 20% 19511/100000 [52:31<2:50:43,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:58,159 >> Initializing global attention on CLS token...\n",
            " 20% 19512/100000 [52:31<2:51:45,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:58,284 >> Initializing global attention on CLS token...\n",
            " 20% 19513/100000 [52:31<2:48:46,  7.95it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:58,406 >> Initializing global attention on CLS token...\n",
            " 20% 19514/100000 [52:31<2:50:20,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:58,540 >> Initializing global attention on CLS token...\n",
            " 20% 19515/100000 [52:31<2:52:22,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:58,667 >> Initializing global attention on CLS token...\n",
            " 20% 19516/100000 [52:31<2:53:17,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:58,802 >> Initializing global attention on CLS token...\n",
            " 20% 19517/100000 [52:32<2:59:11,  7.49it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:58,941 >> Initializing global attention on CLS token...\n",
            " 20% 19518/100000 [52:32<2:56:37,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:59,073 >> Initializing global attention on CLS token...\n",
            " 20% 19519/100000 [52:32<2:55:08,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:59,196 >> Initializing global attention on CLS token...\n",
            " 20% 19520/100000 [52:32<2:54:16,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:59,329 >> Initializing global attention on CLS token...\n",
            " 20% 19521/100000 [52:32<2:53:19,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:59,453 >> Initializing global attention on CLS token...\n",
            " 20% 19522/100000 [52:32<2:50:15,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:59,580 >> Initializing global attention on CLS token...\n",
            " 20% 19523/100000 [52:32<2:53:14,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:59,710 >> Initializing global attention on CLS token...\n",
            " 20% 19524/100000 [52:32<2:52:07,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:59,836 >> Initializing global attention on CLS token...\n",
            " 20% 19525/100000 [52:33<2:54:05,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:19:59,975 >> Initializing global attention on CLS token...\n",
            " 20% 19526/100000 [52:33<2:56:21,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:00,103 >> Initializing global attention on CLS token...\n",
            " 20% 19527/100000 [52:33<2:53:56,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:00,233 >> Initializing global attention on CLS token...\n",
            " 20% 19528/100000 [52:33<2:53:17,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:00,357 >> Initializing global attention on CLS token...\n",
            " 20% 19529/100000 [52:33<2:49:41,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:00,481 >> Initializing global attention on CLS token...\n",
            " 20% 19530/100000 [52:33<2:53:13,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:00,613 >> Initializing global attention on CLS token...\n",
            " 20% 19531/100000 [52:33<2:50:03,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:00,734 >> Initializing global attention on CLS token...\n",
            " 20% 19532/100000 [52:33<2:51:42,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:00,884 >> Initializing global attention on CLS token...\n",
            " 20% 19533/100000 [52:34<2:59:40,  7.46it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:01,013 >> Initializing global attention on CLS token...\n",
            " 20% 19534/100000 [52:34<2:56:19,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:01,143 >> Initializing global attention on CLS token...\n",
            " 20% 19535/100000 [52:34<2:56:20,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:01,270 >> Initializing global attention on CLS token...\n",
            " 20% 19536/100000 [52:34<2:54:22,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:01,402 >> Initializing global attention on CLS token...\n",
            " 20% 19537/100000 [52:34<2:56:08,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:01,531 >> Initializing global attention on CLS token...\n",
            " 20% 19538/100000 [52:34<2:52:14,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:01,653 >> Initializing global attention on CLS token...\n",
            " 20% 19539/100000 [52:34<2:52:48,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:01,787 >> Initializing global attention on CLS token...\n",
            " 20% 19540/100000 [52:34<2:52:33,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:01,911 >> Initializing global attention on CLS token...\n",
            " 20% 19541/100000 [52:35<2:56:14,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:02,053 >> Initializing global attention on CLS token...\n",
            " 20% 19542/100000 [52:35<2:56:35,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:02,181 >> Initializing global attention on CLS token...\n",
            " 20% 19543/100000 [52:35<2:53:46,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:02,310 >> Initializing global attention on CLS token...\n",
            " 20% 19544/100000 [52:35<2:53:44,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:02,435 >> Initializing global attention on CLS token...\n",
            " 20% 19545/100000 [52:35<2:50:08,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:02,556 >> Initializing global attention on CLS token...\n",
            " 20% 19546/100000 [52:35<2:50:45,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:02,690 >> Initializing global attention on CLS token...\n",
            " 20% 19547/100000 [52:35<2:52:06,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:02,814 >> Initializing global attention on CLS token...\n",
            " 20% 19548/100000 [52:36<2:52:11,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:02,948 >> Initializing global attention on CLS token...\n",
            " 20% 19549/100000 [52:36<2:54:36,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:03,078 >> Initializing global attention on CLS token...\n",
            " 20% 19550/100000 [52:36<2:52:16,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:03,208 >> Initializing global attention on CLS token...\n",
            " 20% 19551/100000 [52:36<2:53:55,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:03,335 >> Initializing global attention on CLS token...\n",
            " 20% 19552/100000 [52:36<2:50:05,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:03,455 >> Initializing global attention on CLS token...\n",
            " 20% 19553/100000 [52:36<2:52:36,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:03,595 >> Initializing global attention on CLS token...\n",
            " 20% 19554/100000 [52:36<2:53:57,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:03,721 >> Initializing global attention on CLS token...\n",
            " 20% 19555/100000 [52:36<2:51:57,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:03,851 >> Initializing global attention on CLS token...\n",
            " 20% 19556/100000 [52:37<2:55:39,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:03,982 >> Initializing global attention on CLS token...\n",
            " 20% 19557/100000 [52:37<2:53:09,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:04,113 >> Initializing global attention on CLS token...\n",
            " 20% 19558/100000 [52:37<2:55:20,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:04,242 >> Initializing global attention on CLS token...\n",
            " 20% 19559/100000 [52:37<2:51:26,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:04,363 >> Initializing global attention on CLS token...\n",
            " 20% 19560/100000 [52:37<2:53:15,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:04,501 >> Initializing global attention on CLS token...\n",
            " 20% 19561/100000 [52:37<2:52:40,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:04,624 >> Initializing global attention on CLS token...\n",
            " 20% 19562/100000 [52:37<2:52:03,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:04,755 >> Initializing global attention on CLS token...\n",
            " 20% 19563/100000 [52:37<2:54:01,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:04,886 >> Initializing global attention on CLS token...\n",
            " 20% 19564/100000 [52:38<2:52:02,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:05,013 >> Initializing global attention on CLS token...\n",
            " 20% 19565/100000 [52:38<2:56:21,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:05,152 >> Initializing global attention on CLS token...\n",
            " 20% 19566/100000 [52:38<2:53:18,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:05,272 >> Initializing global attention on CLS token...\n",
            " 20% 19567/100000 [52:38<2:52:35,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:05,405 >> Initializing global attention on CLS token...\n",
            " 20% 19568/100000 [52:38<2:53:26,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:05,530 >> Initializing global attention on CLS token...\n",
            " 20% 19569/100000 [52:38<2:52:40,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:05,662 >> Initializing global attention on CLS token...\n",
            " 20% 19570/100000 [52:38<2:53:18,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:05,788 >> Initializing global attention on CLS token...\n",
            " 20% 19571/100000 [52:38<2:50:17,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:05,915 >> Initializing global attention on CLS token...\n",
            " 20% 19572/100000 [52:39<2:53:21,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:06,044 >> Initializing global attention on CLS token...\n",
            " 20% 19573/100000 [52:39<2:51:43,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:06,170 >> Initializing global attention on CLS token...\n",
            " 20% 19574/100000 [52:39<2:51:51,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:06,303 >> Initializing global attention on CLS token...\n",
            " 20% 19575/100000 [52:39<2:52:47,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:06,429 >> Initializing global attention on CLS token...\n",
            " 20% 19576/100000 [52:39<2:51:35,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:06,560 >> Initializing global attention on CLS token...\n",
            " 20% 19577/100000 [52:39<2:53:50,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:06,688 >> Initializing global attention on CLS token...\n",
            " 20% 19578/100000 [52:39<2:50:44,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:06,811 >> Initializing global attention on CLS token...\n",
            " 20% 19579/100000 [52:40<2:55:04,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:06,950 >> Initializing global attention on CLS token...\n",
            " 20% 19580/100000 [52:40<2:52:04,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:07,071 >> Initializing global attention on CLS token...\n",
            " 20% 19581/100000 [52:40<2:56:58,  7.57it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:07,217 >> Initializing global attention on CLS token...\n",
            " 20% 19582/100000 [52:40<2:56:04,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:07,342 >> Initializing global attention on CLS token...\n",
            " 20% 19583/100000 [52:40<2:55:16,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:07,477 >> Initializing global attention on CLS token...\n",
            " 20% 19584/100000 [52:40<2:56:13,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:07,605 >> Initializing global attention on CLS token...\n",
            " 20% 19585/100000 [52:40<2:53:34,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:07,734 >> Initializing global attention on CLS token...\n",
            " 20% 19586/100000 [52:40<2:54:16,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:07,861 >> Initializing global attention on CLS token...\n",
            " 20% 19587/100000 [52:41<2:51:10,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:07,983 >> Initializing global attention on CLS token...\n",
            " 20% 19588/100000 [52:41<2:50:54,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:08,114 >> Initializing global attention on CLS token...\n",
            " 20% 19589/100000 [52:41<2:52:02,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:08,245 >> Initializing global attention on CLS token...\n",
            " 20% 19590/100000 [52:41<2:53:19,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:08,376 >> Initializing global attention on CLS token...\n",
            " 20% 19591/100000 [52:41<2:54:59,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:08,505 >> Initializing global attention on CLS token...\n",
            " 20% 19592/100000 [52:41<2:50:45,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:08,633 >> Initializing global attention on CLS token...\n",
            " 20% 19593/100000 [52:41<2:59:03,  7.48it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:08,773 >> Initializing global attention on CLS token...\n",
            " 20% 19594/100000 [52:41<2:54:08,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:08,895 >> Initializing global attention on CLS token...\n",
            " 20% 19595/100000 [52:42<2:54:59,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:09,031 >> Initializing global attention on CLS token...\n",
            " 20% 19596/100000 [52:42<2:54:07,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:09,155 >> Initializing global attention on CLS token...\n",
            " 20% 19597/100000 [52:42<2:53:15,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:09,283 >> Initializing global attention on CLS token...\n",
            " 20% 19598/100000 [52:42<2:52:23,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:09,414 >> Initializing global attention on CLS token...\n",
            " 20% 19599/100000 [52:42<2:51:27,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:09,543 >> Initializing global attention on CLS token...\n",
            " 20% 19600/100000 [52:42<2:55:49,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:09,676 >> Initializing global attention on CLS token...\n",
            " 20% 19601/100000 [52:42<2:51:18,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:09,795 >> Initializing global attention on CLS token...\n",
            " 20% 19602/100000 [52:43<2:50:36,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:09,926 >> Initializing global attention on CLS token...\n",
            " 20% 19603/100000 [52:43<2:52:04,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:10,053 >> Initializing global attention on CLS token...\n",
            " 20% 19604/100000 [52:43<2:50:24,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:10,183 >> Initializing global attention on CLS token...\n",
            " 20% 19605/100000 [52:43<2:59:31,  7.46it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:10,336 >> Initializing global attention on CLS token...\n",
            " 20% 19606/100000 [52:43<3:00:32,  7.42it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:10,468 >> Initializing global attention on CLS token...\n",
            " 20% 19607/100000 [52:43<3:02:23,  7.35it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:10,602 >> Initializing global attention on CLS token...\n",
            " 20% 19608/100000 [52:43<2:56:16,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:10,724 >> Initializing global attention on CLS token...\n",
            " 20% 19609/100000 [52:43<2:54:57,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:10,857 >> Initializing global attention on CLS token...\n",
            " 20% 19610/100000 [52:44<2:54:48,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:10,982 >> Initializing global attention on CLS token...\n",
            " 20% 19611/100000 [52:44<2:51:18,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:11,104 >> Initializing global attention on CLS token...\n",
            " 20% 19612/100000 [52:44<2:50:30,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:11,234 >> Initializing global attention on CLS token...\n",
            " 20% 19613/100000 [52:44<2:53:45,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:11,365 >> Initializing global attention on CLS token...\n",
            " 20% 19614/100000 [52:44<2:52:10,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:11,495 >> Initializing global attention on CLS token...\n",
            " 20% 19615/100000 [52:44<2:53:26,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:11,622 >> Initializing global attention on CLS token...\n",
            " 20% 19616/100000 [52:44<2:53:28,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:11,756 >> Initializing global attention on CLS token...\n",
            " 20% 19617/100000 [52:44<2:53:58,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:11,883 >> Initializing global attention on CLS token...\n",
            " 20% 19618/100000 [52:45<2:50:01,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:12,007 >> Initializing global attention on CLS token...\n",
            " 20% 19619/100000 [52:45<2:50:43,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:12,136 >> Initializing global attention on CLS token...\n",
            " 20% 19620/100000 [52:45<2:52:39,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:12,268 >> Initializing global attention on CLS token...\n",
            " 20% 19621/100000 [52:45<2:56:18,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:12,402 >> Initializing global attention on CLS token...\n",
            " 20% 19622/100000 [52:45<2:52:47,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:12,524 >> Initializing global attention on CLS token...\n",
            " 20% 19623/100000 [52:45<2:52:25,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:12,657 >> Initializing global attention on CLS token...\n",
            " 20% 19624/100000 [52:45<2:51:40,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:12,779 >> Initializing global attention on CLS token...\n",
            " 20% 19625/100000 [52:45<2:52:31,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:12,915 >> Initializing global attention on CLS token...\n",
            " 20% 19626/100000 [52:46<2:56:45,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:13,049 >> Initializing global attention on CLS token...\n",
            " 20% 19627/100000 [52:46<2:55:58,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:13,183 >> Initializing global attention on CLS token...\n",
            " 20% 19628/100000 [52:46<2:55:22,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:13,309 >> Initializing global attention on CLS token...\n",
            " 20% 19629/100000 [52:46<2:52:23,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:13,432 >> Initializing global attention on CLS token...\n",
            " 20% 19630/100000 [52:46<2:52:50,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:13,568 >> Initializing global attention on CLS token...\n",
            " 20% 19631/100000 [52:46<2:55:26,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:13,705 >> Initializing global attention on CLS token...\n",
            " 20% 19632/100000 [52:46<2:56:44,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:13,833 >> Initializing global attention on CLS token...\n",
            " 20% 19633/100000 [52:47<2:52:26,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:13,953 >> Initializing global attention on CLS token...\n",
            " 20% 19634/100000 [52:47<2:53:19,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:14,088 >> Initializing global attention on CLS token...\n",
            " 20% 19635/100000 [52:47<2:52:20,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:14,212 >> Initializing global attention on CLS token...\n",
            " 20% 19636/100000 [52:47<2:49:38,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:14,334 >> Initializing global attention on CLS token...\n",
            " 20% 19637/100000 [52:47<2:54:57,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:14,478 >> Initializing global attention on CLS token...\n",
            " 20% 19638/100000 [52:47<2:54:09,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:14,602 >> Initializing global attention on CLS token...\n",
            " 20% 19639/100000 [52:47<2:54:04,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:14,736 >> Initializing global attention on CLS token...\n",
            " 20% 19640/100000 [52:47<2:54:11,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:14,862 >> Initializing global attention on CLS token...\n",
            " 20% 19641/100000 [52:48<2:54:47,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:14,998 >> Initializing global attention on CLS token...\n",
            " 20% 19642/100000 [52:48<2:54:36,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:15,123 >> Initializing global attention on CLS token...\n",
            " 20% 19643/100000 [52:48<2:50:23,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:15,243 >> Initializing global attention on CLS token...\n",
            " 20% 19644/100000 [52:48<2:51:57,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:15,382 >> Initializing global attention on CLS token...\n",
            " 20% 19645/100000 [52:48<2:58:41,  7.49it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:15,526 >> Initializing global attention on CLS token...\n",
            " 20% 19646/100000 [52:48<2:59:39,  7.45it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:15,655 >> Initializing global attention on CLS token...\n",
            " 20% 19647/100000 [52:48<2:55:02,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:15,778 >> Initializing global attention on CLS token...\n",
            " 20% 19648/100000 [52:48<2:54:37,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:15,913 >> Initializing global attention on CLS token...\n",
            " 20% 19649/100000 [52:49<2:53:36,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:16,036 >> Initializing global attention on CLS token...\n",
            " 20% 19650/100000 [52:49<2:50:51,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:16,158 >> Initializing global attention on CLS token...\n",
            " 20% 19651/100000 [52:49<2:51:00,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:16,291 >> Initializing global attention on CLS token...\n",
            " 20% 19652/100000 [52:49<2:53:27,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:16,420 >> Initializing global attention on CLS token...\n",
            " 20% 19653/100000 [52:49<2:53:58,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:16,556 >> Initializing global attention on CLS token...\n",
            " 20% 19654/100000 [52:49<2:54:22,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:16,682 >> Initializing global attention on CLS token...\n",
            " 20% 19655/100000 [52:49<2:51:02,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:16,808 >> Initializing global attention on CLS token...\n",
            " 20% 19656/100000 [52:50<2:53:47,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:16,945 >> Initializing global attention on CLS token...\n",
            " 20% 19657/100000 [52:50<2:54:35,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:17,073 >> Initializing global attention on CLS token...\n",
            " 20% 19658/100000 [52:50<2:54:26,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:17,207 >> Initializing global attention on CLS token...\n",
            " 20% 19659/100000 [52:50<2:54:08,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:17,331 >> Initializing global attention on CLS token...\n",
            " 20% 19660/100000 [52:50<2:52:28,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:17,460 >> Initializing global attention on CLS token...\n",
            " 20% 19661/100000 [52:50<2:54:33,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:17,595 >> Initializing global attention on CLS token...\n",
            " 20% 19662/100000 [52:50<2:56:24,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:17,730 >> Initializing global attention on CLS token...\n",
            " 20% 19663/100000 [52:50<2:56:01,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:17,855 >> Initializing global attention on CLS token...\n",
            " 20% 19664/100000 [52:51<2:51:50,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:17,977 >> Initializing global attention on CLS token...\n",
            " 20% 19665/100000 [52:51<2:54:52,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:18,117 >> Initializing global attention on CLS token...\n",
            " 20% 19666/100000 [52:51<2:53:42,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:18,240 >> Initializing global attention on CLS token...\n",
            " 20% 19667/100000 [52:51<2:50:25,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:18,362 >> Initializing global attention on CLS token...\n",
            " 20% 19668/100000 [52:51<2:51:47,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:18,497 >> Initializing global attention on CLS token...\n",
            " 20% 19669/100000 [52:51<2:53:27,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:18,625 >> Initializing global attention on CLS token...\n",
            " 20% 19670/100000 [52:51<2:52:44,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:18,761 >> Initializing global attention on CLS token...\n",
            " 20% 19671/100000 [52:51<2:55:00,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:18,887 >> Initializing global attention on CLS token...\n",
            " 20% 19672/100000 [52:52<2:51:54,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:19,014 >> Initializing global attention on CLS token...\n",
            " 20% 19673/100000 [52:52<2:54:39,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:19,146 >> Initializing global attention on CLS token...\n",
            " 20% 19674/100000 [52:52<2:50:50,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:19,266 >> Initializing global attention on CLS token...\n",
            " 20% 19675/100000 [52:52<2:50:51,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:19,398 >> Initializing global attention on CLS token...\n",
            " 20% 19676/100000 [52:52<2:50:58,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:19,528 >> Initializing global attention on CLS token...\n",
            " 20% 19677/100000 [52:52<2:58:30,  7.50it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:19,672 >> Initializing global attention on CLS token...\n",
            " 20% 19678/100000 [52:52<2:54:46,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:19,793 >> Initializing global attention on CLS token...\n",
            " 20% 19679/100000 [52:53<2:53:22,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:19,926 >> Initializing global attention on CLS token...\n",
            " 20% 19680/100000 [52:53<2:53:19,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:20,049 >> Initializing global attention on CLS token...\n",
            " 20% 19681/100000 [52:53<2:53:58,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:20,184 >> Initializing global attention on CLS token...\n",
            " 20% 19682/100000 [52:53<2:53:44,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:20,310 >> Initializing global attention on CLS token...\n",
            " 20% 19683/100000 [52:53<2:52:31,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:20,440 >> Initializing global attention on CLS token...\n",
            " 20% 19684/100000 [52:53<2:52:21,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:20,565 >> Initializing global attention on CLS token...\n",
            " 20% 19685/100000 [52:53<2:51:00,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:20,691 >> Initializing global attention on CLS token...\n",
            " 20% 19686/100000 [52:53<2:50:33,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:20,822 >> Initializing global attention on CLS token...\n",
            " 20% 19687/100000 [52:54<2:57:13,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:20,965 >> Initializing global attention on CLS token...\n",
            " 20% 19688/100000 [52:54<2:57:53,  7.52it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:21,095 >> Initializing global attention on CLS token...\n",
            " 20% 19689/100000 [52:54<2:53:06,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:21,216 >> Initializing global attention on CLS token...\n",
            " 20% 19690/100000 [52:54<2:52:20,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:21,347 >> Initializing global attention on CLS token...\n",
            " 20% 19691/100000 [52:54<2:51:31,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:21,470 >> Initializing global attention on CLS token...\n",
            " 20% 19692/100000 [52:54<2:48:48,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:21,592 >> Initializing global attention on CLS token...\n",
            " 20% 19693/100000 [52:54<2:51:57,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:21,730 >> Initializing global attention on CLS token...\n",
            " 20% 19694/100000 [52:54<2:52:35,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:21,856 >> Initializing global attention on CLS token...\n",
            " 20% 19695/100000 [52:55<2:51:39,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:21,987 >> Initializing global attention on CLS token...\n",
            " 20% 19696/100000 [52:55<2:53:34,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:22,116 >> Initializing global attention on CLS token...\n",
            " 20% 19697/100000 [52:55<2:49:51,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:22,242 >> Initializing global attention on CLS token...\n",
            " 20% 19698/100000 [52:55<2:52:08,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:22,369 >> Initializing global attention on CLS token...\n",
            " 20% 19699/100000 [52:55<2:48:45,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:22,489 >> Initializing global attention on CLS token...\n",
            " 20% 19700/100000 [52:55<2:49:25,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:22,621 >> Initializing global attention on CLS token...\n",
            " 20% 19701/100000 [52:55<2:53:22,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:22,753 >> Initializing global attention on CLS token...\n",
            " 20% 19702/100000 [52:55<2:53:10,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:22,886 >> Initializing global attention on CLS token...\n",
            " 20% 19703/100000 [52:56<2:55:52,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:23,018 >> Initializing global attention on CLS token...\n",
            " 20% 19704/100000 [52:56<2:51:18,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:23,140 >> Initializing global attention on CLS token...\n",
            " 20% 19705/100000 [52:56<2:53:36,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:23,277 >> Initializing global attention on CLS token...\n",
            " 20% 19706/100000 [52:56<2:53:49,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:23,402 >> Initializing global attention on CLS token...\n",
            " 20% 19707/100000 [52:56<2:52:40,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:23,534 >> Initializing global attention on CLS token...\n",
            " 20% 19708/100000 [52:56<2:53:20,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:23,660 >> Initializing global attention on CLS token...\n",
            " 20% 19709/100000 [52:56<2:55:22,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:23,805 >> Initializing global attention on CLS token...\n",
            " 20% 19710/100000 [52:57<2:57:05,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:23,930 >> Initializing global attention on CLS token...\n",
            " 20% 19711/100000 [52:57<2:55:13,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:24,062 >> Initializing global attention on CLS token...\n",
            " 20% 19712/100000 [52:57<2:54:55,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:24,187 >> Initializing global attention on CLS token...\n",
            " 20% 19713/100000 [52:57<2:51:42,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:24,311 >> Initializing global attention on CLS token...\n",
            " 20% 19714/100000 [52:57<2:51:02,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:24,441 >> Initializing global attention on CLS token...\n",
            " 20% 19715/100000 [52:57<2:50:30,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:24,564 >> Initializing global attention on CLS token...\n",
            " 20% 19716/100000 [52:57<2:51:57,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:24,700 >> Initializing global attention on CLS token...\n",
            " 20% 19717/100000 [52:57<2:55:29,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:24,835 >> Initializing global attention on CLS token...\n",
            " 20% 19718/100000 [52:58<2:53:58,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:24,964 >> Initializing global attention on CLS token...\n",
            " 20% 19719/100000 [52:58<2:55:44,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:25,098 >> Initializing global attention on CLS token...\n",
            " 20% 19720/100000 [52:58<2:53:09,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:25,218 >> Initializing global attention on CLS token...\n",
            " 20% 19721/100000 [52:58<2:52:13,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:25,350 >> Initializing global attention on CLS token...\n",
            " 20% 19722/100000 [52:58<2:54:02,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:25,483 >> Initializing global attention on CLS token...\n",
            " 20% 19723/100000 [52:58<2:53:29,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:25,608 >> Initializing global attention on CLS token...\n",
            " 20% 19724/100000 [52:58<2:49:54,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:25,728 >> Initializing global attention on CLS token...\n",
            " 20% 19725/100000 [52:58<2:53:31,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:25,870 >> Initializing global attention on CLS token...\n",
            " 20% 19726/100000 [52:59<2:57:55,  7.52it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:26,010 >> Initializing global attention on CLS token...\n",
            " 20% 19727/100000 [52:59<2:56:24,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:26,134 >> Initializing global attention on CLS token...\n",
            " 20% 19728/100000 [52:59<2:53:00,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:26,259 >> Initializing global attention on CLS token...\n",
            " 20% 19729/100000 [52:59<2:54:47,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:26,395 >> Initializing global attention on CLS token...\n",
            " 20% 19730/100000 [52:59<2:52:11,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:26,516 >> Initializing global attention on CLS token...\n",
            " 20% 19731/100000 [52:59<2:51:27,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:26,648 >> Initializing global attention on CLS token...\n",
            " 20% 19732/100000 [52:59<2:52:36,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:26,774 >> Initializing global attention on CLS token...\n",
            " 20% 19733/100000 [52:59<2:53:46,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:26,912 >> Initializing global attention on CLS token...\n",
            " 20% 19734/100000 [53:00<2:58:36,  7.49it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:27,047 >> Initializing global attention on CLS token...\n",
            " 20% 19735/100000 [53:00<2:52:56,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:27,172 >> Initializing global attention on CLS token...\n",
            " 20% 19736/100000 [53:00<2:57:00,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:27,309 >> Initializing global attention on CLS token...\n",
            " 20% 19737/100000 [53:00<2:53:00,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:27,429 >> Initializing global attention on CLS token...\n",
            " 20% 19738/100000 [53:00<2:51:58,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:27,560 >> Initializing global attention on CLS token...\n",
            " 20% 19739/100000 [53:00<2:52:32,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:27,685 >> Initializing global attention on CLS token...\n",
            " 20% 19740/100000 [53:00<2:51:09,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:27,815 >> Initializing global attention on CLS token...\n",
            " 20% 19741/100000 [53:01<2:53:15,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:27,948 >> Initializing global attention on CLS token...\n",
            " 20% 19742/100000 [53:01<2:51:49,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:28,074 >> Initializing global attention on CLS token...\n",
            " 20% 19743/100000 [53:01<2:53:28,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:28,203 >> Initializing global attention on CLS token...\n",
            " 20% 19744/100000 [53:01<2:49:55,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:28,323 >> Initializing global attention on CLS token...\n",
            " 20% 19745/100000 [53:01<2:50:45,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:28,457 >> Initializing global attention on CLS token...\n",
            " 20% 19746/100000 [53:01<2:50:11,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:28,579 >> Initializing global attention on CLS token...\n",
            " 20% 19747/100000 [53:01<2:49:40,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:28,709 >> Initializing global attention on CLS token...\n",
            " 20% 19748/100000 [53:01<2:50:48,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:28,834 >> Initializing global attention on CLS token...\n",
            " 20% 19749/100000 [53:02<2:50:24,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:28,972 >> Initializing global attention on CLS token...\n",
            " 20% 19750/100000 [53:02<2:58:39,  7.49it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:29,113 >> Initializing global attention on CLS token...\n",
            " 20% 19751/100000 [53:02<2:58:30,  7.49it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:29,242 >> Initializing global attention on CLS token...\n",
            " 20% 19752/100000 [53:02<2:53:16,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:29,363 >> Initializing global attention on CLS token...\n",
            " 20% 19753/100000 [53:02<2:52:54,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:29,496 >> Initializing global attention on CLS token...\n",
            " 20% 19754/100000 [53:02<2:52:07,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:29,618 >> Initializing global attention on CLS token...\n",
            " 20% 19755/100000 [53:02<2:49:02,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:29,740 >> Initializing global attention on CLS token...\n",
            " 20% 19756/100000 [53:02<2:51:53,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:29,881 >> Initializing global attention on CLS token...\n",
            " 20% 19757/100000 [53:03<2:53:13,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:30,005 >> Initializing global attention on CLS token...\n",
            " 20% 19758/100000 [53:03<2:53:22,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:30,140 >> Initializing global attention on CLS token...\n",
            " 20% 19759/100000 [53:03<2:53:27,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:30,266 >> Initializing global attention on CLS token...\n",
            " 20% 19760/100000 [53:03<2:50:29,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:30,392 >> Initializing global attention on CLS token...\n",
            " 20% 19761/100000 [53:03<2:52:43,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:30,521 >> Initializing global attention on CLS token...\n",
            " 20% 19762/100000 [53:03<2:49:28,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:30,642 >> Initializing global attention on CLS token...\n",
            " 20% 19763/100000 [53:03<2:49:31,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:30,773 >> Initializing global attention on CLS token...\n",
            " 20% 19764/100000 [53:03<2:50:58,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:30,899 >> Initializing global attention on CLS token...\n",
            " 20% 19765/100000 [53:04<2:50:22,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:31,029 >> Initializing global attention on CLS token...\n",
            " 20% 19766/100000 [53:04<2:54:58,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:31,164 >> Initializing global attention on CLS token...\n",
            " 20% 19767/100000 [53:04<2:50:50,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:31,284 >> Initializing global attention on CLS token...\n",
            " 20% 19768/100000 [53:04<2:54:26,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:31,423 >> Initializing global attention on CLS token...\n",
            " 20% 19769/100000 [53:04<2:50:55,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:31,543 >> Initializing global attention on CLS token...\n",
            " 20% 19770/100000 [53:04<2:47:52,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:31,663 >> Initializing global attention on CLS token...\n",
            " 20% 19771/100000 [53:04<2:50:06,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:31,800 >> Initializing global attention on CLS token...\n",
            " 20% 19772/100000 [53:05<2:50:27,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:31,922 >> Initializing global attention on CLS token...\n",
            " 20% 19773/100000 [53:05<2:49:25,  7.89it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:32,052 >> Initializing global attention on CLS token...\n",
            " 20% 19774/100000 [53:05<2:51:57,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:32,180 >> Initializing global attention on CLS token...\n",
            " 20% 19775/100000 [53:05<2:48:42,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:32,305 >> Initializing global attention on CLS token...\n",
            " 20% 19776/100000 [53:05<2:51:55,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:32,439 >> Initializing global attention on CLS token...\n",
            " 20% 19777/100000 [53:05<2:50:41,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:32,561 >> Initializing global attention on CLS token...\n",
            " 20% 19778/100000 [53:05<2:51:21,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:32,695 >> Initializing global attention on CLS token...\n",
            " 20% 19779/100000 [53:05<2:53:38,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:32,828 >> Initializing global attention on CLS token...\n",
            " 20% 19780/100000 [53:06<2:55:05,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:32,957 >> Initializing global attention on CLS token...\n",
            " 20% 19781/100000 [53:06<2:51:43,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:33,080 >> Initializing global attention on CLS token...\n",
            " 20% 19782/100000 [53:06<2:52:32,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:33,215 >> Initializing global attention on CLS token...\n",
            " 20% 19783/100000 [53:06<2:52:42,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:33,340 >> Initializing global attention on CLS token...\n",
            " 20% 19784/100000 [53:06<2:49:37,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:33,462 >> Initializing global attention on CLS token...\n",
            " 20% 19785/100000 [53:06<2:49:44,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:33,594 >> Initializing global attention on CLS token...\n",
            " 20% 19786/100000 [53:06<2:52:18,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:33,722 >> Initializing global attention on CLS token...\n",
            " 20% 19787/100000 [53:06<2:51:26,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:33,854 >> Initializing global attention on CLS token...\n",
            " 20% 19788/100000 [53:07<2:54:58,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:33,986 >> Initializing global attention on CLS token...\n",
            " 20% 19789/100000 [53:07<2:52:45,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:34,116 >> Initializing global attention on CLS token...\n",
            " 20% 19790/100000 [53:07<2:56:01,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:34,253 >> Initializing global attention on CLS token...\n",
            " 20% 19791/100000 [53:07<2:54:03,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:34,375 >> Initializing global attention on CLS token...\n",
            " 20% 19792/100000 [53:07<2:54:37,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:34,513 >> Initializing global attention on CLS token...\n",
            " 20% 19793/100000 [53:07<2:54:25,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:34,637 >> Initializing global attention on CLS token...\n",
            " 20% 19794/100000 [53:07<2:54:47,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:34,774 >> Initializing global attention on CLS token...\n",
            " 20% 19795/100000 [53:07<2:55:28,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:34,903 >> Initializing global attention on CLS token...\n",
            " 20% 19796/100000 [53:08<2:53:39,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:35,032 >> Initializing global attention on CLS token...\n",
            " 20% 19797/100000 [53:08<2:54:36,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:35,164 >> Initializing global attention on CLS token...\n",
            " 20% 19798/100000 [53:08<2:53:52,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:35,289 >> Initializing global attention on CLS token...\n",
            " 20% 19799/100000 [53:08<2:54:56,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:35,427 >> Initializing global attention on CLS token...\n",
            " 20% 19800/100000 [53:08<2:57:00,  7.55it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:35,562 >> Initializing global attention on CLS token...\n",
            " 20% 19801/100000 [53:08<2:56:15,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:35,688 >> Initializing global attention on CLS token...\n",
            " 20% 19802/100000 [53:08<2:51:40,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:35,808 >> Initializing global attention on CLS token...\n",
            " 20% 19803/100000 [53:09<2:52:18,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:35,944 >> Initializing global attention on CLS token...\n",
            " 20% 19804/100000 [53:09<2:52:26,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:36,068 >> Initializing global attention on CLS token...\n",
            " 20% 19805/100000 [53:09<2:49:17,  7.90it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:36,189 >> Initializing global attention on CLS token...\n",
            " 20% 19806/100000 [53:09<2:52:12,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:36,329 >> Initializing global attention on CLS token...\n",
            " 20% 19807/100000 [53:09<2:53:58,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:36,458 >> Initializing global attention on CLS token...\n",
            " 20% 19808/100000 [53:09<2:52:57,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:36,588 >> Initializing global attention on CLS token...\n",
            " 20% 19809/100000 [53:09<2:54:12,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:36,717 >> Initializing global attention on CLS token...\n",
            " 20% 19810/100000 [53:09<2:52:56,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:36,849 >> Initializing global attention on CLS token...\n",
            " 20% 19811/100000 [53:10<2:56:39,  7.57it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:36,982 >> Initializing global attention on CLS token...\n",
            " 20% 19812/100000 [53:10<2:52:20,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:37,103 >> Initializing global attention on CLS token...\n",
            " 20% 19813/100000 [53:10<2:52:37,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:37,239 >> Initializing global attention on CLS token...\n",
            " 20% 19814/100000 [53:10<2:53:47,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:37,366 >> Initializing global attention on CLS token...\n",
            " 20% 19815/100000 [53:10<2:54:57,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:37,504 >> Initializing global attention on CLS token...\n",
            " 20% 19816/100000 [53:10<2:56:07,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:37,633 >> Initializing global attention on CLS token...\n",
            " 20% 19817/100000 [53:10<2:55:04,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:37,766 >> Initializing global attention on CLS token...\n",
            " 20% 19818/100000 [53:10<2:55:32,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:37,894 >> Initializing global attention on CLS token...\n",
            " 20% 19819/100000 [53:11<2:51:25,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:38,015 >> Initializing global attention on CLS token...\n",
            " 20% 19820/100000 [53:11<2:51:52,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:38,149 >> Initializing global attention on CLS token...\n",
            " 20% 19821/100000 [53:11<2:51:51,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:38,273 >> Initializing global attention on CLS token...\n",
            " 20% 19822/100000 [53:11<2:50:30,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:38,399 >> Initializing global attention on CLS token...\n",
            " 20% 19823/100000 [53:11<2:54:21,  7.66it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:38,541 >> Initializing global attention on CLS token...\n",
            " 20% 19824/100000 [53:11<2:55:58,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:38,674 >> Initializing global attention on CLS token...\n",
            " 20% 19825/100000 [53:11<2:56:17,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:38,802 >> Initializing global attention on CLS token...\n",
            " 20% 19826/100000 [53:12<2:52:30,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:38,925 >> Initializing global attention on CLS token...\n",
            " 20% 19827/100000 [53:12<2:52:39,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:39,059 >> Initializing global attention on CLS token...\n",
            " 20% 19828/100000 [53:12<2:55:01,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:39,189 >> Initializing global attention on CLS token...\n",
            " 20% 19829/100000 [53:12<2:51:17,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:39,311 >> Initializing global attention on CLS token...\n",
            " 20% 19830/100000 [53:12<2:55:19,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:39,454 >> Initializing global attention on CLS token...\n",
            " 20% 19831/100000 [53:12<2:54:16,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:39,578 >> Initializing global attention on CLS token...\n",
            " 20% 19832/100000 [53:12<2:53:39,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:39,711 >> Initializing global attention on CLS token...\n",
            " 20% 19833/100000 [53:12<2:53:31,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:39,836 >> Initializing global attention on CLS token...\n",
            " 20% 19834/100000 [53:13<2:55:01,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:39,975 >> Initializing global attention on CLS token...\n",
            " 20% 19835/100000 [53:13<2:53:42,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:40,098 >> Initializing global attention on CLS token...\n",
            " 20% 19836/100000 [53:13<2:50:24,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:40,219 >> Initializing global attention on CLS token...\n",
            " 20% 19837/100000 [53:13<2:51:04,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:40,353 >> Initializing global attention on CLS token...\n",
            " 20% 19838/100000 [53:13<2:53:55,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:40,488 >> Initializing global attention on CLS token...\n",
            " 20% 19839/100000 [53:13<2:56:15,  7.58it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:40,620 >> Initializing global attention on CLS token...\n",
            " 20% 19840/100000 [53:13<2:51:58,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:40,741 >> Initializing global attention on CLS token...\n",
            " 20% 19841/100000 [53:13<2:52:55,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:40,880 >> Initializing global attention on CLS token...\n",
            " 20% 19842/100000 [53:14<2:55:56,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:41,009 >> Initializing global attention on CLS token...\n",
            " 20% 19843/100000 [53:14<2:54:35,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:41,142 >> Initializing global attention on CLS token...\n",
            " 20% 19844/100000 [53:14<2:54:44,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:41,269 >> Initializing global attention on CLS token...\n",
            " 20% 19845/100000 [53:14<2:53:26,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:41,402 >> Initializing global attention on CLS token...\n",
            " 20% 19846/100000 [53:14<2:58:52,  7.47it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:41,544 >> Initializing global attention on CLS token...\n",
            " 20% 19847/100000 [53:14<2:56:05,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:41,667 >> Initializing global attention on CLS token...\n",
            " 20% 19848/100000 [53:14<2:55:38,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:41,802 >> Initializing global attention on CLS token...\n",
            " 20% 19849/100000 [53:15<2:53:56,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:41,924 >> Initializing global attention on CLS token...\n",
            " 20% 19850/100000 [53:15<2:50:54,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:42,047 >> Initializing global attention on CLS token...\n",
            " 20% 19851/100000 [53:15<2:50:29,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:42,180 >> Initializing global attention on CLS token...\n",
            " 20% 19852/100000 [53:15<2:51:25,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:42,304 >> Initializing global attention on CLS token...\n",
            " 20% 19853/100000 [53:15<2:50:17,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:42,434 >> Initializing global attention on CLS token...\n",
            " 20% 19854/100000 [53:15<2:54:35,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:42,571 >> Initializing global attention on CLS token...\n",
            " 20% 19855/100000 [53:15<2:53:55,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:42,703 >> Initializing global attention on CLS token...\n",
            " 20% 19856/100000 [53:15<2:55:27,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:42,831 >> Initializing global attention on CLS token...\n",
            " 20% 19857/100000 [53:16<2:51:15,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:42,952 >> Initializing global attention on CLS token...\n",
            " 20% 19858/100000 [53:16<2:50:45,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:43,083 >> Initializing global attention on CLS token...\n",
            " 20% 19859/100000 [53:16<2:51:04,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:43,207 >> Initializing global attention on CLS token...\n",
            " 20% 19860/100000 [53:16<2:51:03,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:43,340 >> Initializing global attention on CLS token...\n",
            " 20% 19861/100000 [53:16<2:51:35,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:43,465 >> Initializing global attention on CLS token...\n",
            " 20% 19862/100000 [53:16<2:51:02,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:43,599 >> Initializing global attention on CLS token...\n",
            " 20% 19863/100000 [53:16<2:54:13,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:43,729 >> Initializing global attention on CLS token...\n",
            " 20% 19864/100000 [53:16<2:51:26,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:43,851 >> Initializing global attention on CLS token...\n",
            " 20% 19865/100000 [53:17<2:52:08,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:43,986 >> Initializing global attention on CLS token...\n",
            " 20% 19866/100000 [53:17<2:52:47,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:44,112 >> Initializing global attention on CLS token...\n",
            " 20% 19867/100000 [53:17<2:51:44,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:44,243 >> Initializing global attention on CLS token...\n",
            " 20% 19868/100000 [53:17<2:50:50,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:44,365 >> Initializing global attention on CLS token...\n",
            " 20% 19869/100000 [53:17<2:48:50,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:44,488 >> Initializing global attention on CLS token...\n",
            " 20% 19870/100000 [53:17<2:51:59,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:44,626 >> Initializing global attention on CLS token...\n",
            " 20% 19871/100000 [53:17<2:51:26,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:44,750 >> Initializing global attention on CLS token...\n",
            " 20% 19872/100000 [53:17<2:48:11,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:44,871 >> Initializing global attention on CLS token...\n",
            " 20% 19873/100000 [53:18<2:49:37,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:45,004 >> Initializing global attention on CLS token...\n",
            " 20% 19874/100000 [53:18<2:50:01,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:45,129 >> Initializing global attention on CLS token...\n",
            " 20% 19875/100000 [53:18<2:49:45,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:45,259 >> Initializing global attention on CLS token...\n",
            " 20% 19876/100000 [53:18<2:51:44,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:45,386 >> Initializing global attention on CLS token...\n",
            " 20% 19877/100000 [53:18<2:51:11,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:45,518 >> Initializing global attention on CLS token...\n",
            " 20% 19878/100000 [53:18<2:58:50,  7.47it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:45,667 >> Initializing global attention on CLS token...\n",
            " 20% 19879/100000 [53:18<2:55:52,  7.59it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:45,787 >> Initializing global attention on CLS token...\n",
            " 20% 19880/100000 [53:19<2:56:43,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:45,926 >> Initializing global attention on CLS token...\n",
            " 20% 19881/100000 [53:19<2:55:07,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:46,050 >> Initializing global attention on CLS token...\n",
            " 20% 19882/100000 [53:19<2:51:31,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:46,172 >> Initializing global attention on CLS token...\n",
            " 20% 19883/100000 [53:19<2:54:27,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:46,313 >> Initializing global attention on CLS token...\n",
            " 20% 19884/100000 [53:19<2:53:55,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:46,437 >> Initializing global attention on CLS token...\n",
            " 20% 19885/100000 [53:19<2:50:23,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:46,559 >> Initializing global attention on CLS token...\n",
            " 20% 19886/100000 [53:19<2:58:07,  7.50it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:46,711 >> Initializing global attention on CLS token...\n",
            " 20% 19887/100000 [53:19<2:57:13,  7.53it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:46,836 >> Initializing global attention on CLS token...\n",
            " 20% 19888/100000 [53:20<2:55:25,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:46,970 >> Initializing global attention on CLS token...\n",
            " 20% 19889/100000 [53:20<2:56:34,  7.56it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:47,099 >> Initializing global attention on CLS token...\n",
            " 20% 19890/100000 [53:20<2:55:31,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:47,233 >> Initializing global attention on CLS token...\n",
            " 20% 19891/100000 [53:20<2:54:58,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:47,359 >> Initializing global attention on CLS token...\n",
            " 20% 19892/100000 [53:20<2:51:11,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:47,483 >> Initializing global attention on CLS token...\n",
            " 20% 19893/100000 [53:20<2:52:16,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:47,615 >> Initializing global attention on CLS token...\n",
            " 20% 19894/100000 [53:20<2:53:07,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:47,743 >> Initializing global attention on CLS token...\n",
            " 20% 19895/100000 [53:20<2:51:39,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:47,877 >> Initializing global attention on CLS token...\n",
            " 20% 19896/100000 [53:21<2:54:00,  7.67it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:48,003 >> Initializing global attention on CLS token...\n",
            " 20% 19897/100000 [53:21<2:52:20,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:48,133 >> Initializing global attention on CLS token...\n",
            " 20% 19898/100000 [53:21<2:53:32,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:48,262 >> Initializing global attention on CLS token...\n",
            " 20% 19899/100000 [53:21<2:49:50,  7.86it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:48,382 >> Initializing global attention on CLS token...\n",
            " 20% 19900/100000 [53:21<2:49:26,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:48,513 >> Initializing global attention on CLS token...\n",
            " 20% 19901/100000 [53:21<2:50:59,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:48,643 >> Initializing global attention on CLS token...\n",
            " 20% 19902/100000 [53:21<2:58:21,  7.48it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:48,790 >> Initializing global attention on CLS token...\n",
            " 20% 19903/100000 [53:21<2:54:59,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:48,911 >> Initializing global attention on CLS token...\n",
            " 20% 19904/100000 [53:22<2:55:25,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:49,047 >> Initializing global attention on CLS token...\n",
            " 20% 19905/100000 [53:22<2:53:52,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:49,170 >> Initializing global attention on CLS token...\n",
            " 20% 19906/100000 [53:22<2:49:36,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:49,290 >> Initializing global attention on CLS token...\n",
            " 20% 19907/100000 [53:22<2:51:01,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:49,427 >> Initializing global attention on CLS token...\n",
            " 20% 19908/100000 [53:22<2:52:16,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:49,552 >> Initializing global attention on CLS token...\n",
            " 20% 19909/100000 [53:22<2:51:42,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:49,684 >> Initializing global attention on CLS token...\n",
            " 20% 19910/100000 [53:22<2:55:30,  7.61it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:49,817 >> Initializing global attention on CLS token...\n",
            " 20% 19911/100000 [53:23<2:52:53,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:49,947 >> Initializing global attention on CLS token...\n",
            " 20% 19912/100000 [53:23<2:54:59,  7.63it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:50,080 >> Initializing global attention on CLS token...\n",
            " 20% 19913/100000 [53:23<2:51:37,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:50,200 >> Initializing global attention on CLS token...\n",
            " 20% 19914/100000 [53:23<2:51:39,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:50,336 >> Initializing global attention on CLS token...\n",
            " 20% 19915/100000 [53:23<2:52:01,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:50,458 >> Initializing global attention on CLS token...\n",
            " 20% 19916/100000 [53:23<2:50:36,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:50,588 >> Initializing global attention on CLS token...\n",
            " 20% 19917/100000 [53:23<2:51:35,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:50,714 >> Initializing global attention on CLS token...\n",
            " 20% 19918/100000 [53:23<2:49:39,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:50,842 >> Initializing global attention on CLS token...\n",
            " 20% 19919/100000 [53:24<2:51:12,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:50,969 >> Initializing global attention on CLS token...\n",
            " 20% 19920/100000 [53:24<2:48:46,  7.91it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:51,091 >> Initializing global attention on CLS token...\n",
            " 20% 19921/100000 [53:24<2:49:36,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:51,223 >> Initializing global attention on CLS token...\n",
            " 20% 19922/100000 [53:24<2:50:45,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:51,351 >> Initializing global attention on CLS token...\n",
            " 20% 19923/100000 [53:24<2:51:58,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:51,485 >> Initializing global attention on CLS token...\n",
            " 20% 19924/100000 [53:24<2:53:43,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:51,613 >> Initializing global attention on CLS token...\n",
            " 20% 19925/100000 [53:24<2:49:56,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:51,735 >> Initializing global attention on CLS token...\n",
            " 20% 19926/100000 [53:24<2:53:29,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:51,879 >> Initializing global attention on CLS token...\n",
            " 20% 19927/100000 [53:25<2:54:38,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:52,006 >> Initializing global attention on CLS token...\n",
            " 20% 19928/100000 [53:25<2:51:04,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:52,126 >> Initializing global attention on CLS token...\n",
            " 20% 19929/100000 [53:25<2:50:54,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:52,256 >> Initializing global attention on CLS token...\n",
            " 20% 19930/100000 [53:25<2:50:14,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:52,379 >> Initializing global attention on CLS token...\n",
            " 20% 19931/100000 [53:25<2:47:12,  7.98it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:52,500 >> Initializing global attention on CLS token...\n",
            " 20% 19932/100000 [53:25<2:47:39,  7.96it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:52,630 >> Initializing global attention on CLS token...\n",
            " 20% 19933/100000 [53:25<2:48:34,  7.92it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:52,754 >> Initializing global attention on CLS token...\n",
            " 20% 19934/100000 [53:25<2:52:46,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:52,896 >> Initializing global attention on CLS token...\n",
            " 20% 19935/100000 [53:26<2:55:03,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:53,026 >> Initializing global attention on CLS token...\n",
            " 20% 19936/100000 [53:26<2:50:07,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:53,145 >> Initializing global attention on CLS token...\n",
            " 20% 19937/100000 [53:26<2:50:42,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:53,280 >> Initializing global attention on CLS token...\n",
            " 20% 19938/100000 [53:26<2:50:25,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:53,401 >> Initializing global attention on CLS token...\n",
            " 20% 19939/100000 [53:26<2:50:17,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:53,535 >> Initializing global attention on CLS token...\n",
            " 20% 19940/100000 [53:26<2:52:50,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:53,662 >> Initializing global attention on CLS token...\n",
            " 20% 19941/100000 [53:26<2:52:05,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:53,794 >> Initializing global attention on CLS token...\n",
            " 20% 19942/100000 [53:27<2:54:21,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:53,926 >> Initializing global attention on CLS token...\n",
            " 20% 19943/100000 [53:27<2:51:13,  7.79it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:54,047 >> Initializing global attention on CLS token...\n",
            " 20% 19944/100000 [53:27<2:51:38,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:54,179 >> Initializing global attention on CLS token...\n",
            " 20% 19945/100000 [53:27<2:49:38,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:54,300 >> Initializing global attention on CLS token...\n",
            " 20% 19946/100000 [53:27<2:46:50,  8.00it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:54,422 >> Initializing global attention on CLS token...\n",
            " 20% 19947/100000 [53:27<2:48:06,  7.94it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:54,553 >> Initializing global attention on CLS token...\n",
            " 20% 19948/100000 [53:27<2:49:24,  7.88it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:54,678 >> Initializing global attention on CLS token...\n",
            " 20% 19949/100000 [53:27<2:49:29,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:54,809 >> Initializing global attention on CLS token...\n",
            " 20% 19950/100000 [53:28<2:52:37,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:54,943 >> Initializing global attention on CLS token...\n",
            " 20% 19951/100000 [53:28<2:51:26,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:55,073 >> Initializing global attention on CLS token...\n",
            " 20% 19952/100000 [53:28<2:53:41,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:55,202 >> Initializing global attention on CLS token...\n",
            " 20% 19953/100000 [53:28<2:50:28,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:55,323 >> Initializing global attention on CLS token...\n",
            " 20% 19954/100000 [53:28<2:49:58,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:55,453 >> Initializing global attention on CLS token...\n",
            " 20% 19955/100000 [53:28<2:49:56,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:55,579 >> Initializing global attention on CLS token...\n",
            " 20% 19956/100000 [53:28<2:50:54,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:55,711 >> Initializing global attention on CLS token...\n",
            " 20% 19957/100000 [53:28<2:51:02,  7.80it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:55,835 >> Initializing global attention on CLS token...\n",
            " 20% 19958/100000 [53:29<2:50:21,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:55,971 >> Initializing global attention on CLS token...\n",
            " 20% 19959/100000 [53:29<2:53:45,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:56,099 >> Initializing global attention on CLS token...\n",
            " 20% 19960/100000 [53:29<2:49:57,  7.85it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:56,219 >> Initializing global attention on CLS token...\n",
            " 20% 19961/100000 [53:29<2:50:24,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:56,352 >> Initializing global attention on CLS token...\n",
            " 20% 19962/100000 [53:29<2:50:23,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:56,475 >> Initializing global attention on CLS token...\n",
            " 20% 19963/100000 [53:29<2:51:59,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:56,611 >> Initializing global attention on CLS token...\n",
            " 20% 19964/100000 [53:29<2:52:07,  7.75it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:56,736 >> Initializing global attention on CLS token...\n",
            " 20% 19965/100000 [53:29<2:49:31,  7.87it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:56,858 >> Initializing global attention on CLS token...\n",
            " 20% 19966/100000 [53:30<2:53:31,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:57,005 >> Initializing global attention on CLS token...\n",
            " 20% 19967/100000 [53:30<2:54:30,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:57,128 >> Initializing global attention on CLS token...\n",
            " 20% 19968/100000 [53:30<2:50:35,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:57,249 >> Initializing global attention on CLS token...\n",
            " 20% 19969/100000 [53:30<2:50:26,  7.83it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:57,380 >> Initializing global attention on CLS token...\n",
            " 20% 19970/100000 [53:30<2:50:11,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:57,504 >> Initializing global attention on CLS token...\n",
            " 20% 19971/100000 [53:30<2:47:22,  7.97it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:57,624 >> Initializing global attention on CLS token...\n",
            " 20% 19972/100000 [53:30<2:48:12,  7.93it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:57,757 >> Initializing global attention on CLS token...\n",
            " 20% 19973/100000 [53:30<2:51:35,  7.77it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:57,888 >> Initializing global attention on CLS token...\n",
            " 20% 19974/100000 [53:31<2:52:40,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:58,024 >> Initializing global attention on CLS token...\n",
            " 20% 19975/100000 [53:31<2:56:48,  7.54it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:58,158 >> Initializing global attention on CLS token...\n",
            " 20% 19976/100000 [53:31<2:51:57,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:58,283 >> Initializing global attention on CLS token...\n",
            " 20% 19977/100000 [53:31<2:53:43,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:58,416 >> Initializing global attention on CLS token...\n",
            " 20% 19978/100000 [53:31<2:52:23,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:58,538 >> Initializing global attention on CLS token...\n",
            " 20% 19979/100000 [53:31<2:54:16,  7.65it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:58,679 >> Initializing global attention on CLS token...\n",
            " 20% 19980/100000 [53:31<2:54:29,  7.64it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:58,803 >> Initializing global attention on CLS token...\n",
            " 20% 19981/100000 [53:32<2:53:29,  7.69it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:58,936 >> Initializing global attention on CLS token...\n",
            " 20% 19982/100000 [53:32<2:52:20,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:59,059 >> Initializing global attention on CLS token...\n",
            " 20% 19983/100000 [53:32<2:50:02,  7.84it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:59,186 >> Initializing global attention on CLS token...\n",
            " 20% 19984/100000 [53:32<2:53:15,  7.70it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:59,321 >> Initializing global attention on CLS token...\n",
            " 20% 19985/100000 [53:32<2:50:45,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:59,442 >> Initializing global attention on CLS token...\n",
            " 20% 19986/100000 [53:32<2:50:45,  7.81it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:59,574 >> Initializing global attention on CLS token...\n",
            " 20% 19987/100000 [53:32<2:51:56,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:59,701 >> Initializing global attention on CLS token...\n",
            " 20% 19988/100000 [53:32<2:50:32,  7.82it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:59,832 >> Initializing global attention on CLS token...\n",
            " 20% 19989/100000 [53:33<2:55:32,  7.60it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:20:59,967 >> Initializing global attention on CLS token...\n",
            " 20% 19990/100000 [53:33<2:51:21,  7.78it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:00,095 >> Initializing global attention on CLS token...\n",
            " 20% 19991/100000 [53:33<2:58:09,  7.48it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:00,238 >> Initializing global attention on CLS token...\n",
            " 20% 19992/100000 [53:33<2:55:03,  7.62it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:00,359 >> Initializing global attention on CLS token...\n",
            " 20% 19993/100000 [53:33<2:53:43,  7.68it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:00,492 >> Initializing global attention on CLS token...\n",
            " 20% 19994/100000 [53:33<2:52:52,  7.71it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:00,618 >> Initializing global attention on CLS token...\n",
            " 20% 19995/100000 [53:33<2:52:29,  7.73it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:00,749 >> Initializing global attention on CLS token...\n",
            " 20% 19996/100000 [53:33<2:52:42,  7.72it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:00,876 >> Initializing global attention on CLS token...\n",
            " 20% 19997/100000 [53:34<2:51:51,  7.76it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,005 >> Initializing global attention on CLS token...\n",
            " 20% 19998/100000 [53:34<2:52:16,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,135 >> Initializing global attention on CLS token...\n",
            " 20% 19999/100000 [53:34<2:52:09,  7.74it/s][INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,264 >> Initializing global attention on CLS token...\n",
            "{'loss': 0.441, 'learning_rate': 8.0006e-05, 'epoch': 4.0}\n",
            " 20% 20000/100000 [53:34<2:54:55,  7.62it/s][INFO|trainer.py:703] 2023-01-15 17:21:01,402 >> The following columns in the evaluation set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-15 17:21:01,406 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-15 17:21:01,406 >>   Num examples = 10000\n",
            "[INFO|trainer.py:2949] 2023-01-15 17:21:01,407 >>   Batch size = 6\n",
            "[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,418 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 0/1667 [00:00<?, ?it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,529 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 2/1667 [00:00<01:36, 17.18it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,650 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,765 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 4/1667 [00:00<02:32, 10.89it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,877 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:01,999 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 6/1667 [00:00<02:53,  9.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:02,114 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:02,228 >> Initializing global attention on CLS token...\n",
            "\n",
            "  0% 8/1667 [00:00<02:59,  9.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:02,346 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 9/1667 [00:00<03:03,  9.02it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:02,463 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 10/1667 [00:01<03:04,  8.98it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:02,576 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 11/1667 [00:01<03:06,  8.90it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:02,695 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 12/1667 [00:01<03:09,  8.75it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:02,811 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 13/1667 [00:01<03:10,  8.70it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:02,931 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 14/1667 [00:01<03:14,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:03,053 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 15/1667 [00:01<03:11,  8.61it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:03,165 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 16/1667 [00:01<03:17,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:03,297 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 17/1667 [00:01<03:17,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:03,413 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 18/1667 [00:02<03:15,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:03,532 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 19/1667 [00:02<03:17,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:03,655 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 20/1667 [00:02<03:15,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:03,768 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 21/1667 [00:02<03:15,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:03,891 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 22/1667 [00:02<03:18,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,012 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 23/1667 [00:02<03:15,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,133 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 24/1667 [00:02<03:19,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,260 >> Initializing global attention on CLS token...\n",
            "\n",
            "  1% 25/1667 [00:02<03:19,  8.21it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,377 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 26/1667 [00:02<03:17,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,499 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 27/1667 [00:03<03:17,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,615 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 28/1667 [00:03<03:15,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,737 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 29/1667 [00:03<03:16,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,854 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 30/1667 [00:03<03:13,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:04,967 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 31/1667 [00:03<03:13,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:05,089 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 32/1667 [00:03<03:13,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:05,205 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 33/1667 [00:03<03:12,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:05,327 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 34/1667 [00:03<03:15,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:05,447 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 35/1667 [00:04<03:12,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:05,558 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 36/1667 [00:04<03:11,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:05,679 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 37/1667 [00:04<03:12,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:05,795 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 38/1667 [00:04<03:11,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:05,912 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 39/1667 [00:04<03:13,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,039 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 40/1667 [00:04<03:12,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,151 >> Initializing global attention on CLS token...\n",
            "\n",
            "  2% 41/1667 [00:04<03:10,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,267 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 42/1667 [00:04<03:12,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,392 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 43/1667 [00:04<03:14,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,510 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 44/1667 [00:05<03:13,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,632 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 45/1667 [00:05<03:14,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,750 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 46/1667 [00:05<03:11,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,868 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 47/1667 [00:05<03:13,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:06,986 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 48/1667 [00:05<03:10,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:07,099 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 49/1667 [00:05<03:10,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:07,220 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 50/1667 [00:05<03:10,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:07,341 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 51/1667 [00:05<03:12,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:07,461 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 52/1667 [00:06<03:13,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:07,581 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 53/1667 [00:06<03:10,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:07,695 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 54/1667 [00:06<03:10,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:07,816 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 55/1667 [00:06<03:12,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:07,933 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 56/1667 [00:06<03:12,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:08,062 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 57/1667 [00:06<03:14,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:08,177 >> Initializing global attention on CLS token...\n",
            "\n",
            "  3% 58/1667 [00:06<03:11,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:08,291 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 59/1667 [00:06<03:10,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:08,413 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 60/1667 [00:07<03:11,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:08,530 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 61/1667 [00:07<03:08,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:08,646 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 62/1667 [00:07<03:10,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:08,768 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 63/1667 [00:07<03:10,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:08,883 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 64/1667 [00:07<03:08,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,002 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 65/1667 [00:07<03:09,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,119 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 66/1667 [00:07<03:07,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,233 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 67/1667 [00:07<03:07,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,354 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 68/1667 [00:07<03:09,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,472 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 69/1667 [00:08<03:06,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,585 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 70/1667 [00:08<03:05,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,703 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 71/1667 [00:08<03:07,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,820 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 72/1667 [00:08<03:05,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:09,934 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 73/1667 [00:08<03:06,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:10,056 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 74/1667 [00:08<03:07,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:10,171 >> Initializing global attention on CLS token...\n",
            "\n",
            "  4% 75/1667 [00:08<03:04,  8.61it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:10,286 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 76/1667 [00:08<03:10,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:10,418 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 77/1667 [00:09<03:16,  8.11it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:10,544 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 78/1667 [00:09<03:12,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:10,663 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 79/1667 [00:09<03:11,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:10,784 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 80/1667 [00:09<03:09,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:10,897 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 81/1667 [00:09<03:08,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,016 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 82/1667 [00:09<03:07,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,131 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 83/1667 [00:09<03:04,  8.58it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,243 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 84/1667 [00:09<03:05,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,366 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 85/1667 [00:09<03:06,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,482 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 86/1667 [00:10<03:05,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,599 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 87/1667 [00:10<03:05,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,719 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 88/1667 [00:10<03:05,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,833 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 89/1667 [00:10<03:02,  8.63it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:11,945 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 90/1667 [00:10<03:03,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:12,066 >> Initializing global attention on CLS token...\n",
            "\n",
            "  5% 91/1667 [00:10<03:04,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:12,181 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 92/1667 [00:10<03:02,  8.61it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:12,298 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 93/1667 [00:10<03:04,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:12,415 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 94/1667 [00:10<03:02,  8.61it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:12,532 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 95/1667 [00:11<03:05,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:12,655 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 96/1667 [00:11<03:04,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:12,772 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 97/1667 [00:11<03:07,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:12,896 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 98/1667 [00:11<03:05,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,008 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 99/1667 [00:11<03:04,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,128 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 100/1667 [00:11<03:06,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,247 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 101/1667 [00:11<03:03,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,362 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 102/1667 [00:11<03:04,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,480 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 103/1667 [00:12<03:05,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,599 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 104/1667 [00:12<03:03,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,713 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 105/1667 [00:12<03:03,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,835 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 106/1667 [00:12<03:04,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:13,951 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 107/1667 [00:12<03:01,  8.58it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:14,064 >> Initializing global attention on CLS token...\n",
            "\n",
            "  6% 108/1667 [00:12<03:02,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:14,187 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 109/1667 [00:12<03:03,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:14,301 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 110/1667 [00:12<03:01,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:14,415 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 111/1667 [00:13<03:00,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:14,534 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 112/1667 [00:13<03:06,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:14,659 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 113/1667 [00:13<03:04,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:14,776 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 114/1667 [00:13<03:05,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:14,899 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 115/1667 [00:13<03:03,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,012 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 116/1667 [00:13<03:02,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,131 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 117/1667 [00:13<03:04,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,252 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 118/1667 [00:13<03:03,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,369 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 119/1667 [00:13<03:05,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,495 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 120/1667 [00:14<03:05,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,613 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 121/1667 [00:14<03:04,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,728 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 122/1667 [00:14<03:03,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,848 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 123/1667 [00:14<03:02,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:15,963 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 124/1667 [00:14<03:00,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:16,077 >> Initializing global attention on CLS token...\n",
            "\n",
            "  7% 125/1667 [00:14<03:00,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:16,197 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 126/1667 [00:14<03:02,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:16,319 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 127/1667 [00:14<03:03,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:16,441 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 128/1667 [00:15<03:02,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:16,553 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 129/1667 [00:15<03:05,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:16,689 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 130/1667 [00:15<03:08,  8.15it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:16,807 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 131/1667 [00:15<03:05,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:16,928 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 132/1667 [00:15<03:05,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:17,046 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 133/1667 [00:15<03:02,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:17,163 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 134/1667 [00:15<03:04,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:17,282 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 135/1667 [00:15<03:01,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:17,400 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 136/1667 [00:15<03:02,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:17,521 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 137/1667 [00:16<03:03,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:17,643 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 138/1667 [00:16<03:05,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:17,769 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 139/1667 [00:16<03:04,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:17,882 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 140/1667 [00:16<03:01,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,001 >> Initializing global attention on CLS token...\n",
            "\n",
            "  8% 141/1667 [00:16<03:00,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,118 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 142/1667 [00:16<03:01,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,237 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 143/1667 [00:16<02:59,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,350 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 144/1667 [00:16<02:58,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,468 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 145/1667 [00:17<02:59,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,586 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 146/1667 [00:17<02:58,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,701 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 147/1667 [00:17<03:03,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,833 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 148/1667 [00:17<03:01,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:18,947 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 149/1667 [00:17<02:58,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:19,059 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 150/1667 [00:17<02:57,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:19,177 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 151/1667 [00:17<02:57,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:19,292 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 152/1667 [00:17<02:56,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:19,410 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 153/1667 [00:17<02:58,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:19,533 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 154/1667 [00:18<02:57,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:19,646 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 155/1667 [00:18<02:57,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:19,762 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 156/1667 [00:18<02:57,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:19,884 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 157/1667 [00:18<02:58,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,000 >> Initializing global attention on CLS token...\n",
            "\n",
            "  9% 158/1667 [00:18<02:56,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,114 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 159/1667 [00:18<02:56,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,237 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 160/1667 [00:18<02:58,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,354 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 161/1667 [00:18<02:57,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,475 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 162/1667 [00:19<02:59,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,592 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 163/1667 [00:19<02:56,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,704 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 164/1667 [00:19<02:57,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,833 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 165/1667 [00:19<03:01,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:20,953 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 166/1667 [00:19<02:59,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:21,071 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 167/1667 [00:19<02:59,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:21,187 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 168/1667 [00:19<02:57,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:21,308 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 169/1667 [00:19<02:58,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:21,427 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 170/1667 [00:20<02:56,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:21,541 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 171/1667 [00:20<02:56,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:21,662 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 172/1667 [00:20<02:56,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:21,777 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 173/1667 [00:20<02:56,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:21,897 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 174/1667 [00:20<02:57,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,016 >> Initializing global attention on CLS token...\n",
            "\n",
            " 10% 175/1667 [00:20<02:54,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,129 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 176/1667 [00:20<02:54,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,249 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 177/1667 [00:20<02:55,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,369 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 178/1667 [00:20<02:56,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,488 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 179/1667 [00:21<02:56,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,604 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 180/1667 [00:21<02:54,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,718 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 181/1667 [00:21<02:54,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,840 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 182/1667 [00:21<02:57,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:22,964 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 183/1667 [00:21<02:58,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:23,086 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 184/1667 [00:21<02:57,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:23,200 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 185/1667 [00:21<02:56,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:23,324 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 186/1667 [00:21<03:00,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:23,446 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 187/1667 [00:22<02:56,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:23,563 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 188/1667 [00:22<02:57,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:23,685 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 189/1667 [00:22<02:56,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:23,799 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 190/1667 [00:22<02:54,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:23,918 >> Initializing global attention on CLS token...\n",
            "\n",
            " 11% 191/1667 [00:22<02:56,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,038 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 192/1667 [00:22<02:53,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,150 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 193/1667 [00:22<02:52,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,271 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 194/1667 [00:22<02:53,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,385 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 195/1667 [00:22<02:51,  8.58it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,499 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 196/1667 [00:23<02:50,  8.62it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,617 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 197/1667 [00:23<02:50,  8.62it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,730 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 198/1667 [00:23<02:49,  8.69it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,845 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 199/1667 [00:23<02:50,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:24,966 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 200/1667 [00:23<02:52,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:25,083 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 201/1667 [00:23<02:51,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:25,201 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 202/1667 [00:23<02:51,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:25,316 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 203/1667 [00:23<02:51,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:25,436 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 204/1667 [00:24<02:55,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:25,559 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 205/1667 [00:24<02:52,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:25,672 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 206/1667 [00:24<02:52,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:25,797 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 207/1667 [00:24<02:54,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:25,914 >> Initializing global attention on CLS token...\n",
            "\n",
            " 12% 208/1667 [00:24<02:50,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,025 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 209/1667 [00:24<02:52,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,152 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 210/1667 [00:24<02:53,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,268 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 211/1667 [00:24<02:51,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,386 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 212/1667 [00:24<02:53,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,508 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 213/1667 [00:25<02:52,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,622 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 214/1667 [00:25<02:51,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,742 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 215/1667 [00:25<02:51,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,857 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 216/1667 [00:25<02:49,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:26,972 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 217/1667 [00:25<02:51,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:27,097 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 218/1667 [00:25<02:54,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:27,218 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 219/1667 [00:25<02:51,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:27,332 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 220/1667 [00:25<02:50,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:27,452 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 221/1667 [00:26<02:51,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:27,569 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 222/1667 [00:26<02:49,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:27,683 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 223/1667 [00:26<02:50,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:27,807 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 224/1667 [00:26<02:51,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:27,924 >> Initializing global attention on CLS token...\n",
            "\n",
            " 13% 225/1667 [00:26<02:51,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,048 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 226/1667 [00:26<02:52,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,168 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 227/1667 [00:26<02:53,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,287 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 228/1667 [00:26<02:51,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,406 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 229/1667 [00:26<02:53,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,530 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 230/1667 [00:27<02:51,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,644 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 231/1667 [00:27<02:50,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,764 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 232/1667 [00:27<02:51,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,881 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 233/1667 [00:27<02:48,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:28,998 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 234/1667 [00:27<02:50,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:29,119 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 235/1667 [00:27<02:49,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:29,234 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 236/1667 [00:27<02:48,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:29,353 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 237/1667 [00:27<02:50,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:29,477 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 238/1667 [00:28<02:49,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:29,589 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 239/1667 [00:28<02:49,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:29,712 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 240/1667 [00:28<02:49,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:29,828 >> Initializing global attention on CLS token...\n",
            "\n",
            " 14% 241/1667 [00:28<02:47,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:29,943 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 242/1667 [00:28<02:47,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:30,066 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 243/1667 [00:28<02:49,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:30,182 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 244/1667 [00:28<02:48,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:30,302 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 245/1667 [00:28<02:50,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:30,427 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 246/1667 [00:29<02:50,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:30,542 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 247/1667 [00:29<02:50,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:30,666 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 248/1667 [00:29<02:50,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:30,783 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 249/1667 [00:29<02:48,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:30,898 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 250/1667 [00:29<02:49,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,025 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 251/1667 [00:29<02:48,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,139 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 252/1667 [00:29<02:47,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,258 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 253/1667 [00:29<02:48,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,380 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 254/1667 [00:29<02:50,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,500 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 255/1667 [00:30<02:50,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,624 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 256/1667 [00:30<02:50,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,740 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 257/1667 [00:30<02:47,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,855 >> Initializing global attention on CLS token...\n",
            "\n",
            " 15% 258/1667 [00:30<02:47,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:31,978 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 259/1667 [00:30<02:47,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:32,094 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 260/1667 [00:30<02:45,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:32,212 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 261/1667 [00:30<02:48,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:32,337 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 262/1667 [00:30<02:47,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:32,454 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 263/1667 [00:31<02:47,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:32,571 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 264/1667 [00:31<02:46,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:32,692 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 265/1667 [00:31<02:46,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:32,807 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 266/1667 [00:31<02:44,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:32,926 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 267/1667 [00:31<02:48,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:33,052 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 268/1667 [00:31<02:46,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:33,164 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 269/1667 [00:31<02:45,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:33,284 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 270/1667 [00:31<02:47,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:33,405 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 271/1667 [00:31<02:47,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:33,530 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 272/1667 [00:32<02:48,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:33,650 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 273/1667 [00:32<02:46,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:33,764 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 274/1667 [00:32<02:45,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:33,886 >> Initializing global attention on CLS token...\n",
            "\n",
            " 16% 275/1667 [00:32<02:46,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,003 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 276/1667 [00:32<02:43,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,116 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 277/1667 [00:32<02:43,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,239 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 278/1667 [00:32<02:43,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,352 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 279/1667 [00:32<02:42,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,470 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 280/1667 [00:33<02:44,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,594 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 281/1667 [00:33<02:45,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,712 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 282/1667 [00:33<02:43,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,826 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 283/1667 [00:33<02:45,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:34,954 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 284/1667 [00:33<02:44,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:35,068 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 285/1667 [00:33<02:42,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:35,180 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 286/1667 [00:33<02:42,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:35,303 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 287/1667 [00:33<02:44,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:35,422 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 288/1667 [00:34<02:44,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:35,545 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 289/1667 [00:34<02:45,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:35,664 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 290/1667 [00:34<02:43,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:35,778 >> Initializing global attention on CLS token...\n",
            "\n",
            " 17% 291/1667 [00:34<02:44,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:35,902 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 292/1667 [00:34<02:45,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,021 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 293/1667 [00:34<02:42,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,134 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 294/1667 [00:34<02:43,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,257 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 295/1667 [00:34<02:42,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,372 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 296/1667 [00:34<02:41,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,489 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 297/1667 [00:35<02:42,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,613 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 298/1667 [00:35<02:43,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,731 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 299/1667 [00:35<02:41,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,849 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 300/1667 [00:35<02:44,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:36,973 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 301/1667 [00:35<02:44,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:37,094 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 302/1667 [00:35<02:44,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:37,211 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 303/1667 [00:35<02:41,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:37,324 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 304/1667 [00:35<02:42,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:37,448 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 305/1667 [00:36<02:46,  8.20it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:37,574 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 306/1667 [00:36<02:43,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:37,693 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 307/1667 [00:36<02:43,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:37,811 >> Initializing global attention on CLS token...\n",
            "\n",
            " 18% 308/1667 [00:36<02:40,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:37,923 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 309/1667 [00:36<02:39,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:38,044 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 310/1667 [00:36<02:41,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:38,162 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 311/1667 [00:36<02:40,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:38,282 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 312/1667 [00:36<02:41,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:38,399 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 313/1667 [00:36<02:42,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:38,521 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 314/1667 [00:37<02:42,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:38,645 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 315/1667 [00:37<02:42,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:38,763 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 316/1667 [00:37<02:40,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:38,881 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 317/1667 [00:37<02:41,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,002 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 318/1667 [00:37<02:40,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,116 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 319/1667 [00:37<02:39,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,238 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 320/1667 [00:37<02:39,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,352 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 321/1667 [00:37<02:37,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,471 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 322/1667 [00:38<02:50,  7.89it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,619 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 323/1667 [00:38<02:46,  8.06it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,733 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 324/1667 [00:38<02:44,  8.16it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,857 >> Initializing global attention on CLS token...\n",
            "\n",
            " 19% 325/1667 [00:38<02:44,  8.18it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:39,974 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 326/1667 [00:38<02:41,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:40,096 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 327/1667 [00:38<02:43,  8.22it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:40,215 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 328/1667 [00:38<02:39,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:40,329 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 329/1667 [00:38<02:39,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:40,451 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 330/1667 [00:39<02:40,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:40,569 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 331/1667 [00:39<02:41,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:40,697 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 332/1667 [00:39<02:41,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:40,813 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 333/1667 [00:39<02:37,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:40,926 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 334/1667 [00:39<02:38,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:41,050 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 335/1667 [00:39<02:38,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:41,166 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 336/1667 [00:39<02:36,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:41,284 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 337/1667 [00:39<02:40,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:41,410 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 338/1667 [00:40<02:40,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:41,540 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 339/1667 [00:40<02:54,  7.63it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:41,684 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 340/1667 [00:40<02:48,  7.90it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:41,804 >> Initializing global attention on CLS token...\n",
            "\n",
            " 20% 341/1667 [00:40<02:46,  7.97it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:41,923 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 342/1667 [00:40<02:42,  8.14it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:42,042 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 343/1667 [00:40<02:41,  8.18it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:42,160 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 344/1667 [00:40<02:38,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:42,274 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 345/1667 [00:40<02:37,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:42,399 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 346/1667 [00:40<02:39,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:42,516 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 347/1667 [00:41<02:37,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:42,643 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 348/1667 [00:41<02:42,  8.11it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:42,765 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 349/1667 [00:41<02:39,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:42,880 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 350/1667 [00:41<02:38,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,002 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 351/1667 [00:41<02:37,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,117 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 352/1667 [00:41<02:35,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,237 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 353/1667 [00:41<02:36,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,353 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 354/1667 [00:41<02:33,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,466 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 355/1667 [00:42<02:36,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,592 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 356/1667 [00:42<02:38,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,714 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 357/1667 [00:42<02:36,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,834 >> Initializing global attention on CLS token...\n",
            "\n",
            " 21% 358/1667 [00:42<02:36,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:43,951 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 359/1667 [00:42<02:34,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:44,065 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 360/1667 [00:42<02:34,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:44,185 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 361/1667 [00:42<02:35,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:44,304 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 362/1667 [00:42<02:33,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:44,422 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 363/1667 [00:43<02:36,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:44,544 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 364/1667 [00:43<02:33,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:44,661 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 365/1667 [00:43<02:38,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:44,792 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 366/1667 [00:43<02:37,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:44,907 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 367/1667 [00:43<02:35,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,027 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 368/1667 [00:43<02:37,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,149 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 369/1667 [00:43<02:34,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,262 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 370/1667 [00:43<02:34,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,384 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 371/1667 [00:43<02:33,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,499 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 372/1667 [00:44<02:33,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,622 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 373/1667 [00:44<02:37,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,751 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 374/1667 [00:44<02:37,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,867 >> Initializing global attention on CLS token...\n",
            "\n",
            " 22% 375/1667 [00:44<02:35,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:45,989 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 376/1667 [00:44<02:34,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:46,103 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 377/1667 [00:44<02:31,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:46,216 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 378/1667 [00:44<02:32,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:46,340 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 379/1667 [00:44<02:32,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:46,455 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 380/1667 [00:45<02:30,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:46,568 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 381/1667 [00:45<02:31,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:46,692 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 382/1667 [00:45<02:35,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:46,817 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 383/1667 [00:45<02:32,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:46,932 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 384/1667 [00:45<02:32,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:47,054 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 385/1667 [00:45<02:32,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:47,169 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 386/1667 [00:45<02:30,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:47,287 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 387/1667 [00:45<02:31,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:47,402 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 388/1667 [00:45<02:28,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:47,514 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 389/1667 [00:46<02:34,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:47,650 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 390/1667 [00:46<02:33,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:47,766 >> Initializing global attention on CLS token...\n",
            "\n",
            " 23% 391/1667 [00:46<02:32,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:47,882 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 392/1667 [00:46<02:30,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,001 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 393/1667 [00:46<02:31,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,117 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 394/1667 [00:46<02:30,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,236 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 395/1667 [00:46<02:29,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,350 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 396/1667 [00:46<02:28,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,469 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 397/1667 [00:47<02:29,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,586 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 398/1667 [00:47<02:27,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,699 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 399/1667 [00:47<02:28,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,821 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 400/1667 [00:47<02:34,  8.20it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:48,951 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 401/1667 [00:47<02:33,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:49,072 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 402/1667 [00:47<02:32,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:49,189 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 403/1667 [00:47<02:28,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:49,300 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 404/1667 [00:47<02:28,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:49,422 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 405/1667 [00:48<02:29,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:49,542 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 406/1667 [00:48<02:28,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:49,656 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 407/1667 [00:48<02:30,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:49,783 >> Initializing global attention on CLS token...\n",
            "\n",
            " 24% 408/1667 [00:48<02:29,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:49,899 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 409/1667 [00:48<02:30,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,021 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 410/1667 [00:48<02:30,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,136 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 411/1667 [00:48<02:27,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,250 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 412/1667 [00:48<02:28,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,371 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 413/1667 [00:48<02:27,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,483 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 414/1667 [00:49<02:25,  8.62it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,597 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 415/1667 [00:49<02:25,  8.61it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,717 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 416/1667 [00:49<02:26,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,833 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 417/1667 [00:49<02:26,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:50,950 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 418/1667 [00:49<02:26,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:51,069 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 419/1667 [00:49<02:25,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:51,182 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 420/1667 [00:49<02:24,  8.64it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:51,299 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 421/1667 [00:49<02:26,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:51,418 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 422/1667 [00:50<02:24,  8.62it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:51,530 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 423/1667 [00:50<02:24,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:51,650 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 424/1667 [00:50<02:25,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:51,765 >> Initializing global attention on CLS token...\n",
            "\n",
            " 25% 425/1667 [00:50<02:24,  8.62it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:51,882 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 426/1667 [00:50<02:26,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,005 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 427/1667 [00:50<02:26,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,120 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 428/1667 [00:50<02:26,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,241 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 429/1667 [00:50<02:26,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,356 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 430/1667 [00:50<02:23,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,468 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 431/1667 [00:51<02:27,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,599 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 432/1667 [00:51<02:27,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,715 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 433/1667 [00:51<02:25,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,829 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 434/1667 [00:51<02:25,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:52,950 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 435/1667 [00:51<02:27,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:53,070 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 436/1667 [00:51<02:25,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:53,188 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 437/1667 [00:51<02:26,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:53,310 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 438/1667 [00:51<02:25,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:53,422 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 439/1667 [00:52<02:24,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:53,540 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 440/1667 [00:52<02:24,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:53,657 >> Initializing global attention on CLS token...\n",
            "\n",
            " 26% 441/1667 [00:52<02:23,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:53,771 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 442/1667 [00:52<02:25,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:53,899 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 443/1667 [00:52<02:25,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,014 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 444/1667 [00:52<02:27,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,137 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 445/1667 [00:52<02:27,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,263 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 446/1667 [00:52<02:27,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,381 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 447/1667 [00:52<02:26,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,504 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 448/1667 [00:53<02:26,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,619 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 449/1667 [00:53<02:24,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,734 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 450/1667 [00:53<02:24,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,858 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 451/1667 [00:53<02:25,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:54,974 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 452/1667 [00:53<02:24,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:55,096 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 453/1667 [00:53<02:25,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:55,218 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 454/1667 [00:53<02:24,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:55,336 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 455/1667 [00:53<02:24,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:55,452 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 456/1667 [00:54<02:23,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:55,567 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 457/1667 [00:54<02:24,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:55,694 >> Initializing global attention on CLS token...\n",
            "\n",
            " 27% 458/1667 [00:54<02:24,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:55,810 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 459/1667 [00:54<02:26,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:55,940 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 460/1667 [00:54<02:26,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:56,057 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 461/1667 [00:54<02:27,  8.19it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:56,186 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 462/1667 [00:54<02:27,  8.16it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:56,305 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 463/1667 [00:54<02:26,  8.22it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:56,427 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 464/1667 [00:55<02:25,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:56,542 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 465/1667 [00:55<02:23,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:56,657 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 466/1667 [00:55<02:21,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:56,777 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 467/1667 [00:55<02:22,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:56,893 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 468/1667 [00:55<02:21,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,009 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 469/1667 [00:55<02:21,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,131 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 470/1667 [00:55<02:24,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,258 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 471/1667 [00:55<02:24,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,376 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 472/1667 [00:55<02:22,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,496 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 473/1667 [00:56<02:23,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,614 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 474/1667 [00:56<02:21,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,728 >> Initializing global attention on CLS token...\n",
            "\n",
            " 28% 475/1667 [00:56<02:21,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,850 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 476/1667 [00:56<02:20,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:57,965 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 477/1667 [00:56<02:19,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:58,079 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 478/1667 [00:56<02:19,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:58,202 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 479/1667 [00:56<02:21,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:58,324 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 480/1667 [00:56<02:20,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:58,437 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 481/1667 [00:57<02:20,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:58,562 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 482/1667 [00:57<02:20,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:58,675 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 483/1667 [00:57<02:19,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:58,795 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 484/1667 [00:57<02:22,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:58,922 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 485/1667 [00:57<02:21,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,035 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 486/1667 [00:57<02:20,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,157 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 487/1667 [00:57<02:21,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,275 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 488/1667 [00:57<02:18,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,387 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 489/1667 [00:57<02:18,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,508 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 490/1667 [00:58<02:19,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,625 >> Initializing global attention on CLS token...\n",
            "\n",
            " 29% 491/1667 [00:58<02:19,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,746 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 492/1667 [00:58<02:19,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,862 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 493/1667 [00:58<02:20,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:21:59,988 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 494/1667 [00:58<02:19,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:00,102 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 495/1667 [00:58<02:18,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:00,220 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 496/1667 [00:58<02:21,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:00,346 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 497/1667 [00:58<02:19,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:00,459 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 498/1667 [00:59<02:18,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:00,582 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 499/1667 [00:59<02:19,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:00,697 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 500/1667 [00:59<02:17,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:00,811 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 501/1667 [00:59<02:16,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:00,930 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 502/1667 [00:59<02:17,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:01,047 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 503/1667 [00:59<02:15,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:01,160 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 504/1667 [00:59<02:18,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:01,290 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 505/1667 [00:59<02:18,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:01,406 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 506/1667 [00:59<02:16,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:01,521 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 507/1667 [01:00<02:16,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:01,643 >> Initializing global attention on CLS token...\n",
            "\n",
            " 30% 508/1667 [01:00<02:17,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:01,759 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 509/1667 [01:00<02:15,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:01,876 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 510/1667 [01:00<02:19,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,005 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 511/1667 [01:00<02:18,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,119 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 512/1667 [01:00<02:17,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,241 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 513/1667 [01:00<02:19,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,366 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 514/1667 [01:00<02:19,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,489 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 515/1667 [01:01<02:19,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,603 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 516/1667 [01:01<02:17,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,725 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 517/1667 [01:01<02:18,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,843 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 518/1667 [01:01<02:17,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:02,960 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 519/1667 [01:01<02:16,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:03,083 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 520/1667 [01:01<02:16,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:03,198 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 521/1667 [01:01<02:15,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:03,318 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 522/1667 [01:01<02:22,  8.04it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:03,453 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 523/1667 [01:02<02:18,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:03,566 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 524/1667 [01:02<02:17,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:03,687 >> Initializing global attention on CLS token...\n",
            "\n",
            " 31% 525/1667 [01:02<02:17,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:03,808 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 526/1667 [01:02<02:20,  8.13it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:03,934 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 527/1667 [01:02<02:17,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:04,047 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 528/1667 [01:02<02:14,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:04,165 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 529/1667 [01:02<02:15,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:04,282 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 530/1667 [01:02<02:13,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:04,398 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 531/1667 [01:02<02:15,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:04,523 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 532/1667 [01:03<02:16,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:04,648 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 533/1667 [01:03<02:16,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:04,764 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 534/1667 [01:03<02:14,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:04,880 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 535/1667 [01:03<02:15,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,004 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 536/1667 [01:03<02:16,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,124 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 537/1667 [01:03<02:15,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,246 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 538/1667 [01:03<02:15,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,363 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 539/1667 [01:03<02:14,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,484 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 540/1667 [01:04<02:16,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,608 >> Initializing global attention on CLS token...\n",
            "\n",
            " 32% 541/1667 [01:04<02:15,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,737 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 542/1667 [01:04<02:18,  8.11it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,853 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 543/1667 [01:04<02:15,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:05,967 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 544/1667 [01:04<02:14,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:06,091 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 545/1667 [01:04<02:14,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:06,206 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 546/1667 [01:04<02:14,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:06,332 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 547/1667 [01:04<02:14,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:06,447 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 548/1667 [01:05<02:14,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:06,571 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 549/1667 [01:05<02:15,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:06,690 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 550/1667 [01:05<02:12,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:06,803 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 551/1667 [01:05<02:11,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:06,923 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 552/1667 [01:05<02:11,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:07,037 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 553/1667 [01:05<02:11,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:07,159 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 554/1667 [01:05<02:13,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:07,284 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 555/1667 [01:05<02:12,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:07,397 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 556/1667 [01:05<02:11,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:07,519 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 557/1667 [01:06<02:15,  8.18it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:07,646 >> Initializing global attention on CLS token...\n",
            "\n",
            " 33% 558/1667 [01:06<02:16,  8.14it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:07,774 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 559/1667 [01:06<02:17,  8.06it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:07,897 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 560/1667 [01:06<02:15,  8.18it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,018 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 561/1667 [01:06<02:14,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,134 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 562/1667 [01:06<02:11,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,247 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 563/1667 [01:06<02:11,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,374 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 564/1667 [01:06<02:12,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,490 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 565/1667 [01:07<02:11,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,607 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 566/1667 [01:07<02:10,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,727 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 567/1667 [01:07<02:10,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,843 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 568/1667 [01:07<02:08,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:08,957 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 569/1667 [01:07<02:09,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:09,080 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 570/1667 [01:07<02:10,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:09,197 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 571/1667 [01:07<02:08,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:09,311 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 572/1667 [01:07<02:08,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:09,431 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 573/1667 [01:08<02:09,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:09,548 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 574/1667 [01:08<02:08,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:09,664 >> Initializing global attention on CLS token...\n",
            "\n",
            " 34% 575/1667 [01:08<02:09,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:09,788 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 576/1667 [01:08<02:09,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:09,904 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 577/1667 [01:08<02:07,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:10,019 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 578/1667 [01:08<02:08,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:10,142 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 579/1667 [01:08<02:08,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:10,257 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 580/1667 [01:08<02:10,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:10,385 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 581/1667 [01:08<02:11,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:10,506 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 582/1667 [01:09<02:11,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:10,633 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 583/1667 [01:09<02:13,  8.12it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:10,755 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 584/1667 [01:09<02:10,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:10,868 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 585/1667 [01:09<02:12,  8.16it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,001 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 586/1667 [01:09<02:11,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,115 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 587/1667 [01:09<02:08,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,229 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 588/1667 [01:09<02:07,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,349 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 589/1667 [01:09<02:08,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,467 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 590/1667 [01:10<02:06,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,580 >> Initializing global attention on CLS token...\n",
            "\n",
            " 35% 591/1667 [01:10<02:08,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,708 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 592/1667 [01:10<02:09,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,825 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 593/1667 [01:10<02:07,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:11,940 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 594/1667 [01:10<02:06,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:12,058 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 595/1667 [01:10<02:06,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:12,173 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 596/1667 [01:10<02:05,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:12,288 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 597/1667 [01:10<02:06,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:12,412 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 598/1667 [01:10<02:06,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:12,527 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 599/1667 [01:11<02:05,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:12,646 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 600/1667 [01:11<02:06,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:12,765 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 601/1667 [01:11<02:05,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:12,880 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 602/1667 [01:11<02:05,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,002 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 603/1667 [01:11<02:05,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,116 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 604/1667 [01:11<02:04,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,235 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 605/1667 [01:11<02:05,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,353 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 606/1667 [01:11<02:03,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,466 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 607/1667 [01:12<02:03,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,586 >> Initializing global attention on CLS token...\n",
            "\n",
            " 36% 608/1667 [01:12<02:05,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,704 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 609/1667 [01:12<02:05,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,827 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 610/1667 [01:12<02:07,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:13,949 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 611/1667 [01:12<02:05,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:14,067 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 612/1667 [01:12<02:06,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:14,190 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 613/1667 [01:12<02:06,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:14,306 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 614/1667 [01:12<02:05,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:14,428 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 615/1667 [01:13<02:07,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:14,549 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 616/1667 [01:13<02:04,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:14,663 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 617/1667 [01:13<02:05,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:14,789 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 618/1667 [01:13<02:06,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:14,906 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 619/1667 [01:13<02:04,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,024 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 620/1667 [01:13<02:05,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,145 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 621/1667 [01:13<02:03,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,259 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 622/1667 [01:13<02:04,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,382 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 623/1667 [01:13<02:04,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,498 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 624/1667 [01:14<02:02,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,617 >> Initializing global attention on CLS token...\n",
            "\n",
            " 37% 625/1667 [01:14<02:03,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,738 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 626/1667 [01:14<02:04,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,858 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 627/1667 [01:14<02:04,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:15,981 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 628/1667 [01:14<02:04,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:16,097 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 629/1667 [01:14<02:02,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:16,220 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 630/1667 [01:14<02:06,  8.21it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:16,344 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 631/1667 [01:14<02:04,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:16,459 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 632/1667 [01:15<02:03,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:16,580 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 633/1667 [01:15<02:03,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:16,696 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 634/1667 [01:15<02:01,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:16,809 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 635/1667 [01:15<02:04,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:16,951 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 636/1667 [01:15<02:07,  8.12it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:17,069 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 637/1667 [01:15<02:04,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:17,181 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 638/1667 [01:15<02:04,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:17,305 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 639/1667 [01:15<02:03,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:17,421 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 640/1667 [01:16<02:05,  8.21it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:17,550 >> Initializing global attention on CLS token...\n",
            "\n",
            " 38% 641/1667 [01:16<02:03,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:17,664 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 642/1667 [01:16<02:01,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:17,778 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 643/1667 [01:16<02:02,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:17,906 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 644/1667 [01:16<02:05,  8.15it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,031 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 645/1667 [01:16<02:03,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,152 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 646/1667 [01:16<02:06,  8.05it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,281 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 647/1667 [01:16<02:03,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,392 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 648/1667 [01:16<02:02,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,517 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 649/1667 [01:17<02:02,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,634 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 650/1667 [01:17<02:01,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,753 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 651/1667 [01:17<02:01,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,869 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 652/1667 [01:17<02:01,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:18,990 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 653/1667 [01:17<02:00,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:19,108 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 654/1667 [01:17<01:59,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:19,223 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 655/1667 [01:17<02:00,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:19,345 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 656/1667 [01:17<01:59,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:19,460 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 657/1667 [01:18<01:58,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:19,578 >> Initializing global attention on CLS token...\n",
            "\n",
            " 39% 658/1667 [01:18<01:59,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:19,695 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 659/1667 [01:18<01:58,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:19,812 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 660/1667 [01:18<01:58,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:19,933 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 661/1667 [01:18<01:59,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,051 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 662/1667 [01:18<01:57,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,168 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 663/1667 [01:18<01:59,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,290 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 664/1667 [01:18<01:58,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,403 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 665/1667 [01:18<01:58,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,524 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 666/1667 [01:19<01:58,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,639 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 667/1667 [01:19<01:56,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,757 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 668/1667 [01:19<01:57,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,877 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 669/1667 [01:19<01:57,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:20,990 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 670/1667 [01:19<01:57,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:21,112 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 671/1667 [01:19<01:58,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:21,230 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 672/1667 [01:19<01:57,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:21,346 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 673/1667 [01:19<01:57,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:21,469 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 674/1667 [01:20<01:58,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:21,592 >> Initializing global attention on CLS token...\n",
            "\n",
            " 40% 675/1667 [01:20<01:59,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:21,711 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 676/1667 [01:20<01:57,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:21,827 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 677/1667 [01:20<01:58,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:21,950 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 678/1667 [01:20<01:58,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:22,068 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 679/1667 [01:20<01:57,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:22,190 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 680/1667 [01:20<01:58,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:22,307 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 681/1667 [01:20<01:56,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:22,421 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 682/1667 [01:21<01:56,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:22,541 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 683/1667 [01:21<01:56,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:22,659 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 684/1667 [01:21<01:56,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:22,782 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 685/1667 [01:21<01:58,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:22,906 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 686/1667 [01:21<01:56,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,017 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 687/1667 [01:21<01:58,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,147 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 688/1667 [01:21<01:57,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,262 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 689/1667 [01:21<01:57,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,385 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 690/1667 [01:21<01:57,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,504 >> Initializing global attention on CLS token...\n",
            "\n",
            " 41% 691/1667 [01:22<01:56,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,620 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 692/1667 [01:22<01:56,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,743 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 693/1667 [01:22<01:56,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,859 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 694/1667 [01:22<01:56,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:23,982 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 695/1667 [01:22<01:56,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:24,104 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 696/1667 [01:22<01:57,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:24,225 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 697/1667 [01:22<01:57,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:24,346 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 698/1667 [01:22<01:58,  8.20it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:24,472 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 699/1667 [01:23<01:58,  8.17it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:24,591 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 700/1667 [01:23<01:56,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:24,710 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 701/1667 [01:23<01:57,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:24,830 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 702/1667 [01:23<01:55,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:24,944 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 703/1667 [01:23<01:55,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:25,068 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 704/1667 [01:23<01:57,  8.21it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:25,191 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 705/1667 [01:23<01:55,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:25,311 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 706/1667 [01:23<01:55,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:25,428 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 707/1667 [01:24<01:53,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:25,540 >> Initializing global attention on CLS token...\n",
            "\n",
            " 42% 708/1667 [01:24<01:53,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:25,663 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 709/1667 [01:24<01:53,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:25,777 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 710/1667 [01:24<01:51,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:25,894 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 711/1667 [01:24<01:53,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,016 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 712/1667 [01:24<01:52,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,129 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 713/1667 [01:24<01:52,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,252 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 714/1667 [01:24<01:52,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,368 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 715/1667 [01:24<01:51,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,482 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 716/1667 [01:25<01:51,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,602 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 717/1667 [01:25<01:51,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,715 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 718/1667 [01:25<01:49,  8.63it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,829 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 719/1667 [01:25<01:50,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:26,951 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 720/1667 [01:25<01:54,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:27,079 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 721/1667 [01:25<01:53,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:27,195 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 722/1667 [01:25<01:52,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:27,314 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 723/1667 [01:25<01:51,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:27,429 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 724/1667 [01:26<01:50,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:27,544 >> Initializing global attention on CLS token...\n",
            "\n",
            " 43% 725/1667 [01:26<01:50,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:27,665 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 726/1667 [01:26<01:52,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:27,785 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 727/1667 [01:26<01:52,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:27,907 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 728/1667 [01:26<01:51,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,024 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 729/1667 [01:26<01:51,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,142 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 730/1667 [01:26<01:53,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,267 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 731/1667 [01:26<01:51,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,381 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 732/1667 [01:26<01:50,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,502 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 733/1667 [01:27<01:51,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,623 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 734/1667 [01:27<01:52,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,747 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 735/1667 [01:27<01:51,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,861 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 736/1667 [01:27<01:50,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:28,979 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 737/1667 [01:27<01:50,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:29,095 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 738/1667 [01:27<01:48,  8.58it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:29,206 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 739/1667 [01:27<01:48,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:29,329 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 740/1667 [01:27<01:48,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:29,443 >> Initializing global attention on CLS token...\n",
            "\n",
            " 44% 741/1667 [01:28<01:47,  8.64it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:29,555 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 742/1667 [01:28<01:47,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:29,679 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 743/1667 [01:28<01:49,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:29,796 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 744/1667 [01:28<01:47,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:29,910 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 745/1667 [01:28<01:48,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,034 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 746/1667 [01:28<01:48,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,147 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 747/1667 [01:28<01:46,  8.61it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,260 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 748/1667 [01:28<01:47,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,383 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 749/1667 [01:28<01:49,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,502 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 750/1667 [01:29<01:47,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,615 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 751/1667 [01:29<01:48,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,737 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 752/1667 [01:29<01:48,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,857 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 753/1667 [01:29<01:48,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:30,975 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 754/1667 [01:29<01:46,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:31,091 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 755/1667 [01:29<01:48,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:31,213 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 756/1667 [01:29<01:48,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:31,335 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 757/1667 [01:29<01:48,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:31,456 >> Initializing global attention on CLS token...\n",
            "\n",
            " 45% 758/1667 [01:30<01:49,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:31,574 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 759/1667 [01:30<01:47,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:31,688 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 760/1667 [01:30<01:48,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:31,812 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 761/1667 [01:30<01:49,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:31,932 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 762/1667 [01:30<01:47,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,050 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 763/1667 [01:30<01:48,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,171 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 764/1667 [01:30<01:46,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,283 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 765/1667 [01:30<01:47,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,408 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 766/1667 [01:30<01:47,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,525 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 767/1667 [01:31<01:46,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,641 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 768/1667 [01:31<01:46,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,765 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 769/1667 [01:31<01:46,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,878 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 770/1667 [01:31<01:44,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:32,991 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 771/1667 [01:31<01:44,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:33,113 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 772/1667 [01:31<01:45,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:33,229 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 773/1667 [01:31<01:44,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:33,342 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 774/1667 [01:31<01:45,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:33,468 >> Initializing global attention on CLS token...\n",
            "\n",
            " 46% 775/1667 [01:32<01:46,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:33,585 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 776/1667 [01:32<01:44,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:33,699 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 777/1667 [01:32<01:44,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:33,819 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 778/1667 [01:32<01:45,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:33,936 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 779/1667 [01:32<01:44,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:34,056 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 780/1667 [01:32<01:44,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:34,171 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 781/1667 [01:32<01:44,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:34,292 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 782/1667 [01:32<01:45,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:34,410 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 783/1667 [01:33<01:45,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:34,535 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 784/1667 [01:33<01:45,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:34,651 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 785/1667 [01:33<01:44,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:34,767 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 786/1667 [01:33<01:44,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:34,889 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 787/1667 [01:33<01:44,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,004 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 788/1667 [01:33<01:43,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,118 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 789/1667 [01:33<01:42,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,238 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 790/1667 [01:33<01:42,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,354 >> Initializing global attention on CLS token...\n",
            "\n",
            " 47% 791/1667 [01:33<01:42,  8.58it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,471 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 792/1667 [01:34<01:46,  8.19it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,607 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 793/1667 [01:34<01:45,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,721 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 794/1667 [01:34<01:43,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,835 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 795/1667 [01:34<01:43,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:35,959 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 796/1667 [01:34<01:43,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:36,074 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 797/1667 [01:34<01:43,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:36,196 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 798/1667 [01:34<01:43,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:36,311 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 799/1667 [01:34<01:42,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:36,427 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 800/1667 [01:35<01:43,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:36,553 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 801/1667 [01:35<01:45,  8.22it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:36,676 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 802/1667 [01:35<01:44,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:36,800 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 803/1667 [01:35<01:43,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:36,913 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 804/1667 [01:35<01:41,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,025 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 805/1667 [01:35<01:41,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,148 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 806/1667 [01:35<01:42,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,264 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 807/1667 [01:35<01:41,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,385 >> Initializing global attention on CLS token...\n",
            "\n",
            " 48% 808/1667 [01:35<01:41,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,498 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 809/1667 [01:36<01:42,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,627 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 810/1667 [01:36<01:44,  8.19it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,750 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 811/1667 [01:36<01:42,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,863 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 812/1667 [01:36<01:44,  8.16it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:37,998 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 813/1667 [01:36<01:43,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:38,112 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 814/1667 [01:36<01:42,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:38,233 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 815/1667 [01:36<01:42,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:38,349 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 816/1667 [01:36<01:40,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:38,461 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 817/1667 [01:37<01:40,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:38,586 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 818/1667 [01:37<01:45,  8.06it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:38,719 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 819/1667 [01:37<01:43,  8.17it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:38,840 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 820/1667 [01:37<01:42,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:38,956 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 821/1667 [01:37<01:41,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:39,077 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 822/1667 [01:37<01:41,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:39,194 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 823/1667 [01:37<01:39,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:39,306 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 824/1667 [01:37<01:39,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:39,427 >> Initializing global attention on CLS token...\n",
            "\n",
            " 49% 825/1667 [01:38<01:38,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:39,541 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 826/1667 [01:38<01:40,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:39,671 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 827/1667 [01:38<01:41,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:39,792 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 828/1667 [01:38<01:40,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:39,908 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 829/1667 [01:38<01:39,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,029 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 830/1667 [01:38<01:40,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,146 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 831/1667 [01:38<01:38,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,259 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 832/1667 [01:38<01:38,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,379 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 833/1667 [01:38<01:38,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,494 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 834/1667 [01:39<01:37,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,608 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 835/1667 [01:39<01:42,  8.11it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,749 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 836/1667 [01:39<01:40,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,865 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 837/1667 [01:39<01:39,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:40,978 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 838/1667 [01:39<01:38,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:41,100 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 839/1667 [01:39<01:38,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:41,214 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 840/1667 [01:39<01:38,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:41,338 >> Initializing global attention on CLS token...\n",
            "\n",
            " 50% 841/1667 [01:39<01:37,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:41,451 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 842/1667 [01:40<01:39,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:41,580 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 843/1667 [01:40<01:39,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:41,699 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 844/1667 [01:40<01:39,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:41,817 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 845/1667 [01:40<01:39,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:41,942 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 846/1667 [01:40<01:39,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:42,063 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 847/1667 [01:40<01:38,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:42,180 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 848/1667 [01:40<01:37,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:42,295 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 849/1667 [01:40<01:37,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:42,418 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 850/1667 [01:41<01:37,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:42,535 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 851/1667 [01:41<01:37,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:42,664 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 852/1667 [01:41<01:38,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:42,782 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 853/1667 [01:41<01:37,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:42,897 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 854/1667 [01:41<01:36,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,015 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 855/1667 [01:41<01:36,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,130 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 856/1667 [01:41<01:34,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,243 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 857/1667 [01:41<01:34,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,363 >> Initializing global attention on CLS token...\n",
            "\n",
            " 51% 858/1667 [01:41<01:34,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,482 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 859/1667 [01:42<01:35,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,600 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 860/1667 [01:42<01:34,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,719 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 861/1667 [01:42<01:37,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,847 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 862/1667 [01:42<01:37,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:43,966 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 863/1667 [01:42<01:36,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:44,084 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 864/1667 [01:42<01:36,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:44,200 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 865/1667 [01:42<01:35,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:44,320 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 866/1667 [01:42<01:34,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:44,435 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 867/1667 [01:43<01:33,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:44,548 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 868/1667 [01:43<01:33,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:44,670 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 869/1667 [01:43<01:34,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:44,786 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 870/1667 [01:43<01:34,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:44,909 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 871/1667 [01:43<01:34,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,026 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 872/1667 [01:43<01:32,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,138 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 873/1667 [01:43<01:33,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,263 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 874/1667 [01:43<01:34,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,382 >> Initializing global attention on CLS token...\n",
            "\n",
            " 52% 875/1667 [01:43<01:33,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,500 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 876/1667 [01:44<01:34,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,618 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 877/1667 [01:44<01:33,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,733 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 878/1667 [01:44<01:33,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,854 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 879/1667 [01:44<01:35,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:45,983 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 880/1667 [01:44<01:36,  8.19it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:46,108 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 881/1667 [01:44<01:35,  8.21it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:46,225 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 882/1667 [01:44<01:33,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:46,338 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 883/1667 [01:44<01:32,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:46,458 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 884/1667 [01:45<01:33,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:46,574 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 885/1667 [01:45<01:32,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:46,695 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 886/1667 [01:45<01:32,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:46,811 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 887/1667 [01:45<01:32,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:46,936 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 888/1667 [01:45<01:34,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:47,057 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 889/1667 [01:45<01:32,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:47,170 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 890/1667 [01:45<01:32,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:47,291 >> Initializing global attention on CLS token...\n",
            "\n",
            " 53% 891/1667 [01:45<01:32,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:47,407 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 892/1667 [01:45<01:31,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:47,525 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 893/1667 [01:46<01:32,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:47,645 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 894/1667 [01:46<01:30,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:47,758 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 895/1667 [01:46<01:30,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:47,880 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 896/1667 [01:46<01:31,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,004 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 897/1667 [01:46<01:33,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,129 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 898/1667 [01:46<01:33,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,246 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 899/1667 [01:46<01:31,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,358 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 900/1667 [01:46<01:30,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,478 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 901/1667 [01:47<01:30,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,593 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 902/1667 [01:47<01:29,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,706 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 903/1667 [01:47<01:30,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,831 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 904/1667 [01:47<01:30,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:48,947 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 905/1667 [01:47<01:29,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:49,067 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 906/1667 [01:47<01:29,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:49,185 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 907/1667 [01:47<01:29,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:49,299 >> Initializing global attention on CLS token...\n",
            "\n",
            " 54% 908/1667 [01:47<01:28,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:49,414 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 909/1667 [01:48<01:28,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:49,534 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 910/1667 [01:48<01:28,  8.58it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:49,648 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 911/1667 [01:48<01:27,  8.62it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:49,762 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 912/1667 [01:48<01:28,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:49,884 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 913/1667 [01:48<01:28,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,001 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 914/1667 [01:48<01:28,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,118 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 915/1667 [01:48<01:29,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,238 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 916/1667 [01:48<01:27,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,351 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 917/1667 [01:48<01:27,  8.58it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,471 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 918/1667 [01:49<01:27,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,587 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 919/1667 [01:49<01:26,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,701 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 920/1667 [01:49<01:28,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,830 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 921/1667 [01:49<01:28,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:50,945 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 922/1667 [01:49<01:28,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:51,065 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 923/1667 [01:49<01:30,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:51,189 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 924/1667 [01:49<01:28,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:51,302 >> Initializing global attention on CLS token...\n",
            "\n",
            " 55% 925/1667 [01:49<01:28,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:51,426 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 926/1667 [01:50<01:27,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:51,540 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 927/1667 [01:50<01:27,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:51,662 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 928/1667 [01:50<01:28,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:51,780 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 929/1667 [01:50<01:27,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:51,895 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 930/1667 [01:50<01:28,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,023 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 931/1667 [01:50<01:29,  8.20it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,145 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 932/1667 [01:50<01:28,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,266 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 933/1667 [01:50<01:28,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,383 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 934/1667 [01:50<01:26,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,497 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 935/1667 [01:51<01:26,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,620 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 936/1667 [01:51<01:27,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,738 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 937/1667 [01:51<01:26,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,858 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 938/1667 [01:51<01:27,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:52,979 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 939/1667 [01:51<01:25,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:53,091 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 940/1667 [01:51<01:26,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:53,223 >> Initializing global attention on CLS token...\n",
            "\n",
            " 56% 941/1667 [01:51<01:28,  8.20it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:53,342 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 942/1667 [01:51<01:27,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:53,461 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 943/1667 [01:52<01:27,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:53,580 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 944/1667 [01:52<01:26,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:53,696 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 945/1667 [01:52<01:28,  8.17it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:53,828 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 946/1667 [01:52<01:28,  8.16it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:53,952 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 947/1667 [01:52<01:27,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:54,072 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 948/1667 [01:52<01:27,  8.21it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:54,190 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 949/1667 [01:52<01:26,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:54,306 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 950/1667 [01:52<01:25,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:54,427 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 951/1667 [01:53<01:25,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:54,542 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 952/1667 [01:53<01:24,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:54,664 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 953/1667 [01:53<01:25,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:54,784 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 954/1667 [01:53<01:24,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:54,897 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 955/1667 [01:53<01:24,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,017 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 956/1667 [01:53<01:23,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,132 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 957/1667 [01:53<01:22,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,245 >> Initializing global attention on CLS token...\n",
            "\n",
            " 57% 958/1667 [01:53<01:23,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,369 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 959/1667 [01:53<01:23,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,485 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 960/1667 [01:54<01:22,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,599 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 961/1667 [01:54<01:22,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,719 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 962/1667 [01:54<01:22,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,835 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 963/1667 [01:54<01:22,  8.58it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:55,949 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 964/1667 [01:54<01:22,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:56,068 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 965/1667 [01:54<01:21,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:56,182 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 966/1667 [01:54<01:20,  8.66it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:56,296 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 967/1667 [01:54<01:21,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:56,420 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 968/1667 [01:55<01:22,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:56,537 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 969/1667 [01:55<01:21,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:56,652 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 970/1667 [01:55<01:21,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:56,774 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 971/1667 [01:55<01:21,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:56,887 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 972/1667 [01:55<01:20,  8.61it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,000 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 973/1667 [01:55<01:20,  8.62it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,119 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 974/1667 [01:55<01:20,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,235 >> Initializing global attention on CLS token...\n",
            "\n",
            " 58% 975/1667 [01:55<01:19,  8.67it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,350 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 976/1667 [01:55<01:23,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,482 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 977/1667 [01:56<01:22,  8.38it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,596 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 978/1667 [01:56<01:21,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,711 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 979/1667 [01:56<01:21,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,832 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 980/1667 [01:56<01:21,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:57,950 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 981/1667 [01:56<01:20,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:58,064 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 982/1667 [01:56<01:20,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:58,184 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 983/1667 [01:56<01:20,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:58,299 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 984/1667 [01:56<01:20,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:58,421 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 985/1667 [01:57<01:21,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:58,542 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 986/1667 [01:57<01:21,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:58,658 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 987/1667 [01:57<01:19,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:58,774 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 988/1667 [01:57<01:20,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:58,896 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 989/1667 [01:57<01:19,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,010 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 990/1667 [01:57<01:19,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,123 >> Initializing global attention on CLS token...\n",
            "\n",
            " 59% 991/1667 [01:57<01:18,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,242 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 992/1667 [01:57<01:18,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,357 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 993/1667 [01:57<01:18,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,475 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 994/1667 [01:58<01:19,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,594 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 995/1667 [01:58<01:18,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,706 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 996/1667 [01:58<01:17,  8.63it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,822 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 997/1667 [01:58<01:19,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:22:59,952 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 998/1667 [01:58<01:20,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:00,074 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 999/1667 [01:58<01:19,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:00,190 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1000/1667 [01:58<01:19,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:00,310 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1001/1667 [01:58<01:18,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:00,424 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1002/1667 [01:59<01:18,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:00,544 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1003/1667 [01:59<01:18,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:00,661 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1004/1667 [01:59<01:17,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:00,774 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1005/1667 [01:59<01:18,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:00,898 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1006/1667 [01:59<01:18,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,013 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1007/1667 [01:59<01:17,  8.57it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,127 >> Initializing global attention on CLS token...\n",
            "\n",
            " 60% 1008/1667 [01:59<01:16,  8.60it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,248 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1009/1667 [01:59<01:17,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,364 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1010/1667 [01:59<01:16,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,478 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1011/1667 [02:00<01:17,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,605 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1012/1667 [02:00<01:17,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,720 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1013/1667 [02:00<01:16,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,834 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1014/1667 [02:00<01:17,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:01,958 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1015/1667 [02:00<01:17,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:02,077 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1016/1667 [02:00<01:17,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:02,193 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1017/1667 [02:00<01:17,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:02,314 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1018/1667 [02:00<01:16,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:02,433 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1019/1667 [02:01<01:18,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:02,555 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1020/1667 [02:01<01:16,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:02,669 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1021/1667 [02:01<01:16,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:02,792 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1022/1667 [02:01<01:16,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:02,909 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1023/1667 [02:01<01:16,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,030 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1024/1667 [02:01<01:17,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,152 >> Initializing global attention on CLS token...\n",
            "\n",
            " 61% 1025/1667 [02:01<01:16,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,265 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1026/1667 [02:01<01:16,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,388 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1027/1667 [02:01<01:16,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,504 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1028/1667 [02:02<01:17,  8.21it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,637 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1029/1667 [02:02<01:17,  8.22it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,753 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1030/1667 [02:02<01:15,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,869 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1031/1667 [02:02<01:15,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:03,989 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1032/1667 [02:02<01:15,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:04,107 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1033/1667 [02:02<01:15,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:04,229 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1034/1667 [02:02<01:16,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:04,347 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1035/1667 [02:02<01:14,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:04,460 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1036/1667 [02:03<01:14,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:04,581 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1037/1667 [02:03<01:15,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:04,703 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1038/1667 [02:03<01:14,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:04,817 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1039/1667 [02:03<01:15,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:04,941 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1040/1667 [02:03<01:14,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:05,055 >> Initializing global attention on CLS token...\n",
            "\n",
            " 62% 1041/1667 [02:03<01:14,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:05,177 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1042/1667 [02:03<01:14,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:05,298 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1043/1667 [02:03<01:13,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:05,411 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1044/1667 [02:04<01:14,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:05,535 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1045/1667 [02:04<01:14,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:05,657 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1046/1667 [02:04<01:14,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:05,772 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1047/1667 [02:04<01:14,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:05,897 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1048/1667 [02:04<01:14,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,015 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1049/1667 [02:04<01:15,  8.24it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,143 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1050/1667 [02:04<01:15,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,260 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1051/1667 [02:04<01:13,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,381 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1052/1667 [02:04<01:13,  8.33it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,497 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1053/1667 [02:05<01:12,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,612 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1054/1667 [02:05<01:13,  8.30it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,741 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1055/1667 [02:05<01:13,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,856 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1056/1667 [02:05<01:12,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:06,979 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1057/1667 [02:05<01:13,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:07,097 >> Initializing global attention on CLS token...\n",
            "\n",
            " 63% 1058/1667 [02:05<01:12,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:07,211 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1059/1667 [02:05<01:12,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:07,333 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1060/1667 [02:05<01:12,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:07,450 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1061/1667 [02:06<01:11,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:07,570 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1062/1667 [02:06<01:12,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:07,687 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1063/1667 [02:06<01:11,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:07,805 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1064/1667 [02:06<01:11,  8.46it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:07,925 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1065/1667 [02:06<01:11,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,040 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1066/1667 [02:06<01:10,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,159 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1067/1667 [02:06<01:10,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,276 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1068/1667 [02:06<01:09,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,388 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1069/1667 [02:06<01:09,  8.56it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,508 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1070/1667 [02:07<01:10,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,624 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1071/1667 [02:07<01:09,  8.59it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,739 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1072/1667 [02:07<01:11,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,871 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1073/1667 [02:07<01:11,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:08,985 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1074/1667 [02:07<01:09,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:09,097 >> Initializing global attention on CLS token...\n",
            "\n",
            " 64% 1075/1667 [02:07<01:09,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:09,218 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1076/1667 [02:07<01:09,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:09,333 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1077/1667 [02:07<01:08,  8.62it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:09,447 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1078/1667 [02:08<01:09,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:09,571 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1079/1667 [02:08<01:09,  8.47it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:09,686 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1080/1667 [02:08<01:08,  8.52it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:09,805 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1081/1667 [02:08<01:09,  8.44it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:09,926 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1082/1667 [02:08<01:09,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:10,048 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1083/1667 [02:08<01:09,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:10,166 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1084/1667 [02:08<01:10,  8.32it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:10,286 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1085/1667 [02:08<01:09,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:10,401 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1086/1667 [02:08<01:09,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:10,529 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1087/1667 [02:09<01:10,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:10,647 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1088/1667 [02:09<01:09,  8.39it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:10,762 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1089/1667 [02:09<01:09,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:10,891 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1090/1667 [02:09<01:09,  8.26it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,014 >> Initializing global attention on CLS token...\n",
            "\n",
            " 65% 1091/1667 [02:09<01:09,  8.25it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,129 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1092/1667 [02:09<01:08,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,245 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1093/1667 [02:09<01:08,  8.42it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,366 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1094/1667 [02:09<01:08,  8.40it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,482 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1095/1667 [02:10<01:07,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,603 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1096/1667 [02:10<01:08,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,724 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1097/1667 [02:10<01:08,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,845 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1098/1667 [02:10<01:08,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:11,967 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1099/1667 [02:10<01:09,  8.23it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:12,093 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1100/1667 [02:10<01:09,  8.21it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:12,210 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1101/1667 [02:10<01:08,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:12,332 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1102/1667 [02:10<01:08,  8.28it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:12,449 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1103/1667 [02:11<01:07,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:12,573 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1104/1667 [02:11<01:07,  8.29it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:12,690 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1105/1667 [02:11<01:07,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:12,810 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1106/1667 [02:11<01:07,  8.27it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:12,931 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1107/1667 [02:11<01:06,  8.45it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,044 >> Initializing global attention on CLS token...\n",
            "\n",
            " 66% 1108/1667 [02:11<01:05,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,164 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1109/1667 [02:11<01:06,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,284 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1110/1667 [02:11<01:07,  8.31it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,409 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1111/1667 [02:11<01:06,  8.37it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,523 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1112/1667 [02:12<01:05,  8.51it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,636 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1113/1667 [02:12<01:04,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,755 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1114/1667 [02:12<01:04,  8.54it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,869 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1115/1667 [02:12<01:05,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:13,990 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1116/1667 [02:12<01:05,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:14,113 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1117/1667 [02:12<01:05,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:14,232 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1118/1667 [02:12<01:05,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:14,350 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1119/1667 [02:12<01:05,  8.41it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:14,468 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1120/1667 [02:13<01:05,  8.36it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:14,594 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1121/1667 [02:13<01:05,  8.35it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:14,709 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1122/1667 [02:13<01:05,  8.34it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:14,831 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1123/1667 [02:13<01:04,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:14,945 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1124/1667 [02:13<01:03,  8.50it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:15,060 >> Initializing global attention on CLS token...\n",
            "\n",
            " 67% 1125/1667 [02:13<01:03,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:15,181 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 1126/1667 [02:13<01:04,  8.43it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:15,298 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 1127/1667 [02:13<01:03,  8.53it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:15,412 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 1128/1667 [02:14<01:03,  8.48it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:15,535 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 1129/1667 [02:14<01:03,  8.49it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:15,652 >> Initializing global attention on CLS token...\n",
            "\n",
            " 68% 1130/1667 [02:14<01:02,  8.55it/s]\u001b[A[INFO|modeling_longformer.py:1936] 2023-01-15 17:23:15,764 >> Initializing global attention on CLS token...\n"
          ]
        }
      ]
    }
  ]
}