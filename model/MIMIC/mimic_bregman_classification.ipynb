{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/MIMIC/mimic_bregman_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpOzUfD53lts"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ7IqoZXAVjg",
        "outputId": "9fee53c4-b2f4-4f31-acb9-58c086f1dae4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "br19zMT6AoKn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielsaggau/IR_LDC.git"
      ],
      "metadata": {
        "id": "rUv6Xu7wSD_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/mimic.jsonl.zip -d content\n",
        "with open('content/mimic.jsonl') as f:\n",
        "    data = [json.loads(line) for line in f]"
      ],
      "metadata": {
        "id": "JQcrNr7mnbcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move(\"/content/content/mimic.jsonl\", \"/content/IR_LDC/model/MIMIC\")\n",
        "dataset = load_dataset(\"/content/IR_LDC/model/MIMIC/mimic-dataset.py\")"
      ],
      "metadata": {
        "id": "swGB8CPVAzg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_test = dataset['train'].train_test_split(test_size=0.1)\n",
        "dataset_test = dataset_train_test['test']\n",
        "dataset_sp = dataset_train_test['train'].train_test_split(test_size=0.1/0.9)\n",
        "dataset_train = dataset_sp['train']\n",
        "dataset_validation = dataset_sp['test']"
      ],
      "metadata": {
        "id": "4GCgc1F3AYoH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('XXXX')\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/bregman_mimic_FT\", use_auth_token=True, use_fast=True)"
      ],
      "metadata": {
        "id": "z4j3EoP3BHTb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/bregman_mimic_FT\", use_auth_token=True,num_labels=19, problem_type='multi_label_classification')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G1YMr3-HKa0",
        "outputId": "46a5899d-e2ab-4c87-e924-87657450a0b0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/bregman_mimic_FT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) # fp16"
      ],
      "metadata": {
        "id": "-QVq_XkgFeuh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset['train']"
      ],
      "metadata": {
        "id": "fikpTYlCTVvF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = train_dataset.features['labels'].feature.num_classes\n",
        "label_ids = train_dataset.features['labels'].feature.names\n",
        "\n",
        "label_names = label_ids\n",
        "label_list = list(range(num_labels))"
      ],
      "metadata": {
        "id": "hX_UfmDUTRg7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   def preprocess_function(examples):\n",
        "        # Tokenize the texts\n",
        "        batch = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            truncation=True)\n",
        "        \n",
        "        batch = tokenizer.pad(\n",
        "            batch,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            pad_to_multiple_of=8,\n",
        "        )\n",
        "        batch[\"label_ids\"] = [[1.0 if label in labels else 0.0 for label in label_list] for labels in examples[\"labels\"]]\n",
        "        return batch"
      ],
      "metadata": {
        "id": "01gQrulMS4-w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = dataset.map(preprocess_function, batched=True, remove_columns=['labels'])"
      ],
      "metadata": {
        "id": "67R1GBXKOcvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   from transformers import EvalPrediction\n",
        "   def compute_metrics(p: EvalPrediction):\n",
        "        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "        preds = (expit(logits) > 0.5).astype(int)\n",
        "        label_ids = (p.label_ids > 0.5).astype(int)\n",
        "        macro_f1 = f1_score(y_true=label_ids, y_pred=preds, average='macro', zero_division=0)\n",
        "        micro_f1 = f1_score(y_true=label_ids, y_pred=preds, average='micro', zero_division=0)\n",
        "        return {'macro_f1': macro_f1, 'micro_f1': micro_f1}"
      ],
      "metadata": {
        "id": "DGA0EtELOM0p"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hNSQkTYLSrx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/clongformer_mimic_classification_bregman\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=6,\n",
        "    per_device_eval_batch_size=6,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    metric_for_best_model=\"micro_f1\",\n",
        "    greater_is_better=True,\n",
        "    load_best_model_at_end = True,\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"mimic_bregman\")"
      ],
      "metadata": {
        "id": "rciMxXfoOioO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']"
      ],
      "metadata": {
        "id": "7skfxV63SYaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "BePx6berkGDC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert pooling\n",
        "import torch\n",
        "from torch import nn\n",
        "class BertMeanPooler(nn.Module):\n",
        "          def __init__(self, config):\n",
        "             super().__init__()\n",
        "             self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "             self.activation = nn.Tanh()\n",
        "\n",
        "          def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "              mean_token_tensor = hidden_states.mean(dim=1)\n",
        "              pooled_output = self.dense(mean_token_tensor)\n",
        "              pooled_output = self.activation(pooled_output)\n",
        "              return pooled_output\n",
        "model.bert.pooler = BertMeanPooler(model.config)\n",
        "print('model mean pooler loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Lumuo9RX9O",
        "outputId": "a75947a5-a40d-4dfb-fe43-6042fe26b20d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model mean pooler loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "C7svTN1ARXAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from scipy.special import expit\n",
        "from sklearn.metrics import f1_score\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    compute_metrics=compute_metrics,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_data['test'],\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,    \n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8P9bv9MzOsgR",
        "outputId": "ac611b9f-9f2a-4632-fae6-7b1db3ccf6b2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning https://huggingface.co/danielsaggau/clongformer_mimic_classification_bregman into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/danielsaggau/clongformer_mimic_classification_bregman into local empty directory.\n",
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 30000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 50000\n",
            "  Number of trainable parameters = 109496851\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221218_101841-30jmnifc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/danielsaggau/huggingface/runs/30jmnifc\" target=\"_blank\">mimic_bregman</a></strong> to <a href=\"https://wandb.ai/danielsaggau/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30000' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30000/50000 2:27:28 < 1:38:19, 3.39 it/s, Epoch 6/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Micro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.368400</td>\n",
              "      <td>0.367972</td>\n",
              "      <td>0.643399</td>\n",
              "      <td>0.710382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.349900</td>\n",
              "      <td>0.358627</td>\n",
              "      <td>0.661550</td>\n",
              "      <td>0.714283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.323900</td>\n",
              "      <td>0.361013</td>\n",
              "      <td>0.684809</td>\n",
              "      <td>0.725553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.306400</td>\n",
              "      <td>0.370703</td>\n",
              "      <td>0.681337</td>\n",
              "      <td>0.723355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.272000</td>\n",
              "      <td>0.388913</td>\n",
              "      <td>0.687537</td>\n",
              "      <td>0.722388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.242700</td>\n",
              "      <td>0.409592</td>\n",
              "      <td>0.682667</td>\n",
              "      <td>0.717118</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 6\n",
            "Saving model checkpoint to /clongformer_mimic_classification_bregman/checkpoint-5000\n",
            "Configuration saved in /clongformer_mimic_classification_bregman/checkpoint-5000/config.json\n",
            "Model weights saved in /clongformer_mimic_classification_bregman/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in /clongformer_mimic_classification_bregman/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in /clongformer_mimic_classification_bregman/checkpoint-5000/special_tokens_map.json\n",
            "tokenizer config file saved in /clongformer_mimic_classification_bregman/tokenizer_config.json\n",
            "Special tokens file saved in /clongformer_mimic_classification_bregman/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 6\n",
            "Saving model checkpoint to /clongformer_mimic_classification_bregman/checkpoint-10000\n",
            "Configuration saved in /clongformer_mimic_classification_bregman/checkpoint-10000/config.json\n",
            "Model weights saved in /clongformer_mimic_classification_bregman/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in /clongformer_mimic_classification_bregman/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in /clongformer_mimic_classification_bregman/checkpoint-10000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 6\n",
            "Saving model checkpoint to /clongformer_mimic_classification_bregman/checkpoint-15000\n",
            "Configuration saved in /clongformer_mimic_classification_bregman/checkpoint-15000/config.json\n",
            "Model weights saved in /clongformer_mimic_classification_bregman/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in /clongformer_mimic_classification_bregman/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in /clongformer_mimic_classification_bregman/checkpoint-15000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 6\n",
            "Saving model checkpoint to /clongformer_mimic_classification_bregman/checkpoint-20000\n",
            "Configuration saved in /clongformer_mimic_classification_bregman/checkpoint-20000/config.json\n",
            "Model weights saved in /clongformer_mimic_classification_bregman/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in /clongformer_mimic_classification_bregman/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in /clongformer_mimic_classification_bregman/checkpoint-20000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 6\n",
            "Saving model checkpoint to /clongformer_mimic_classification_bregman/checkpoint-25000\n",
            "Configuration saved in /clongformer_mimic_classification_bregman/checkpoint-25000/config.json\n",
            "Model weights saved in /clongformer_mimic_classification_bregman/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in /clongformer_mimic_classification_bregman/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in /clongformer_mimic_classification_bregman/checkpoint-25000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 6\n",
            "Saving model checkpoint to /clongformer_mimic_classification_bregman/checkpoint-30000\n",
            "Configuration saved in /clongformer_mimic_classification_bregman/checkpoint-30000/config.json\n",
            "Model weights saved in /clongformer_mimic_classification_bregman/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in /clongformer_mimic_classification_bregman/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in /clongformer_mimic_classification_bregman/checkpoint-30000/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /clongformer_mimic_classification_bregman/checkpoint-15000 (score: 0.7255528816596112).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=30000, training_loss=0.31416226857503254, metrics={'train_runtime': 9117.1754, 'train_samples_per_second': 32.905, 'train_steps_per_second': 5.484, 'total_flos': 4.736721881088e+16, 'train_loss': 0.31416226857503254, 'epoch': 6.0})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=tokenized_data['validation'])"
      ],
      "metadata": {
        "id": "G-34xgBJI_Zx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "0d0834d3-adfb-4a4f-896c-34926c0c78ca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1667' max='1667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1667/1667 02:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.35984161496162415,\n",
              " 'eval_macro_f1': 0.6777420171475501,\n",
              " 'eval_micro_f1': 0.7257478529594579,\n",
              " 'eval_runtime': 126.0732,\n",
              " 'eval_samples_per_second': 79.319,\n",
              " 'eval_steps_per_second': 13.222,\n",
              " 'epoch': 6.0}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}