{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/MIMIC/MIMIC_SimCSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t-yCziDJmc6"
      },
      "outputs": [],
      "source": [
        "!git clone https://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@github.com/danielsaggau/IR_LDC.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7S-RrH1fJL2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081becc7-6b8b-423b-9474-98af4419cdec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8YSn6IXqJob7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ea753c-2c65-4f87-b131-6523e0056844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IR_LDC\n"
          ]
        }
      ],
      "source": [
        "%cd IR_LDC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY5AQdi6JqXD"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sbK_e2VeJJiT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from sentence_transformers import models, losses\n",
        "from sentence_transformers import LoggingHandler, SentenceTransformer, InputExample\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import gzip\n",
        "import sys\n",
        "import tqdm\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/mimic.jsonl.zip -d content\n",
        "with open('/content/drive/MyDrive/mimic.jsonl/mimic.jsonl') as f:\n",
        "    data = [json.loads(line) for line in f]"
      ],
      "metadata": {
        "id": "W3Gcs_-Mq-kt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "MsCGtw6TLl0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move(\"/content/drive/MyDrive/mimic.jsonl/mimic.jsonl\", \"/content/IR_LDC/model/MIMIC\")\n",
        "dataset = load_dataset(\"/content/IR_LDC/model/MIMIC/mimic-dataset.py\")"
      ],
      "metadata": {
        "id": "cUar2-w6LUYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_test = dataset['train'].train_test_split(test_size=0.1)\n",
        "dataset_test = dataset_train_test['test']\n",
        "dataset_sp = dataset_train_test['train'].train_test_split(test_size=0.1/0.9)\n",
        "dataset_train = dataset_sp['train']\n",
        "dataset_validation = dataset_sp['test']"
      ],
      "metadata": {
        "id": "TkANswL2nL32"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('your_file.txt', 'w') as f:\n",
        "    for line in dataset['train']['text']:\n",
        "        f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "VyP4Yq99rKCl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYXut72dJIxi",
        "outputId": "0f3f29f5-9b6f-48e0-b393-3aeb09379cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/bionlp_bluebert_pubmed_mimic_uncased_L-12_H-768_A-12. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/bionlp_bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Training parameters\n",
        "#access=\"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\"\n",
        "model_name = \"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\"\n",
        "train_batch_size = 4\n",
        "max_seq_length = 4096\n",
        "num_epochs = 1\n",
        "\n",
        "\n",
        "# Save path to store our model\n",
        "output_name = ''\n",
        "if len(sys.argv) >= 3:\n",
        "    output_name = \"-\"+sys.argv[2].replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "model_output_path = 'output/train_simcse{}-{}'.format(output_name, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "\n",
        "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
        "model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################# Read the train corpus  #################\n",
        "filepath = \"your_file.txt\"\n",
        "train_samples = []\n",
        "with gzip.open(filepath, 'rt', encoding='utf8') if filepath.endswith('.gz') else open(filepath, encoding='utf8') as fIn:\n",
        "    for line in tqdm.tqdm(fIn, desc='Read file'):\n",
        "        line = line.strip()\n",
        "        if len(line) >= 10:\n",
        "            train_samples.append(InputExample(texts=[line, line]))\n",
        "\n",
        "logging.info(\"Train sentences: {}\".format(len(train_samples)))\n",
        "\n",
        "# We train our model using the MultipleNegativesRankingLoss\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size, drop_last=True)"
      ],
      "metadata": {
        "id": "wHTBCW8VsBg2",
        "outputId": "967c24ec-155f-403b-c883-c2964503db26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Read file: 6456859it [00:16, 681698.83it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from enum import Enum\n",
        "from typing import Iterable, Dict\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, InputExample,  models\n",
        "from torch.utils.data import DataLoader\n",
        "from model.loss.cos_sim import cos_sim\n",
        "\n",
        "class AddProjection(nn.Module):\n",
        "   def __init__(self, model: SentenceTransformer, mlp_dim=512,embedding_size=5120): #removed sentence_embedding_dimension\n",
        "       super(AddProjection, self).__init__()\n",
        "       self.model = SentenceTransformer('bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12')\n",
        "       embedding_size = embedding_size\n",
        "       mlp_dim =  self.model.get_sentence_embedding_dimension() \n",
        "       #self.model.fc = nn.Identity()\n",
        "       self.projection = nn.Sequential(\n",
        "           nn.Linear(in_features=mlp_dim, out_features=mlp_dim),\n",
        "           nn.BatchNorm1d(mlp_dim),\n",
        "           nn.ReLU(),\n",
        "           nn.Linear(in_features=mlp_dim, out_features=embedding_size),\n",
        "           nn.BatchNorm1d(embedding_size),\n",
        "       )\n",
        "\n",
        "   def forward(self, a: Tensor):\n",
        "       if not isinstance(a, torch.Tensor):\n",
        "          a = torch.tensor(a)\n",
        "       if len(a.shape) == 1:\n",
        "          a = a.unsqueeze(0)\n",
        "       a = self.projection(a)\n",
        "       return a\n",
        "\n",
        "class BregmanRankingLoss(nn.Module):\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  def __init__(self, model: SentenceTransformer, sigma, temperature, batch_size, lambda1, lambda2 ,feat_dim=512, scale: float = 20.0, similarity_fct = cos_sim):\n",
        "        \"\"\"\n",
        "        :param model: SentenceTransformer model\n",
        "        :param scale: Output of similarity function is multiplied by scale value\n",
        "        :param similarity_fct: similarity function between sentence embeddings. By default, cos_sim. Can also be set to dot product (and then set scale to 1)\n",
        "        \"\"\"\n",
        "        super(BregmanRankingLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.projection = AddProjection(self,model)\n",
        "        self.sigma = sigma\n",
        "        self.temperature = temperature\n",
        "        self.batch_size = batch_size\n",
        "        self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.lambda1=lambda1\n",
        "        self.lambda2=lambda2\n",
        "        self.scale = scale\n",
        "        self.similarity_fct = similarity_fct\n",
        "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "\n",
        "  def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=torch.long)#, dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "\n",
        "  def b_sim(self, features):\n",
        "        mm = torch.max(features, dim=1)\n",
        "        indx_max_features = mm[1]\n",
        "        max_features = mm[0].reshape(-1, 1)\n",
        "        # Compute the number of active subnets in one batch\n",
        "        eye = torch.eye(features.shape[1]).to(device)\n",
        "        one = eye[indx_max_features].to(device)\n",
        "        num_max = torch.sum(one, dim=0)\n",
        "        dist_matrix = max_features - features[:, indx_max_features]\n",
        "        sigma = torch.tensor([self.sigma]).to(device)\n",
        "        sig2 = 2 * torch.pow(sigma, 2)\n",
        "        sim_matrix = torch.exp(torch.div(-dist_matrix, sig2))\n",
        "\n",
        "        return sim_matrix, num_max\n",
        "\n",
        "  def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
        "\n",
        "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features] # get output main model\n",
        "        embeddings_a = reps[0] \n",
        "        embeddings_b = torch.cat(reps[1:])\n",
        "\n",
        "        scores = self.similarity_fct(embeddings_a, embeddings_b) * self.scale\n",
        "        labels = torch.tensor(range(len(scores)), dtype=torch.long, device=scores.device)  # Example a[i] should match with b[i]\n",
        "        rloss = self.cross_entropy_loss(scores, labels)\n",
        "        \n",
        "        # bregman part \n",
        "\n",
        "        N = 2 * self.batch_size\n",
        "        z1 = self.projection(embeddings_a)\n",
        "        z2 = self.projection(embeddings_b)\n",
        "        features = torch.cat((z1, z2), dim=0)\n",
        "\n",
        "        ###################################################\n",
        "        ### Computing Similarity Matrix ###################\n",
        "        sim_matrix, num_max = self.b_sim(features)\n",
        "        sim_matrix = sim_matrix / self.temperature\n",
        "        ###################################################\n",
        "        #sim_matrix = self.similarity_f(out.unsqueeze(1), out.unsqueeze(0)) / self.temperature\n",
        "\n",
        "        pos_ab = torch.diag(sim_matrix, self.batch_size)\n",
        "        pos_ba = torch.diag(sim_matrix, -self.batch_size)\n",
        "\n",
        "        positives = torch.cat((pos_ab, pos_ba), dim=0).reshape(N, 1)\n",
        "        negatives = sim_matrix[self.mask].reshape(N, -1)\n",
        "\n",
        "        blabel = torch.zeros(N, dtype=torch.long).to(device)\n",
        "        bscores = torch.cat((positives, negatives), dim=1)\n",
        "        bloss = self.criterion(bscores, blabel).to(device)\n",
        "        bloss /= N\n",
        "        loss = self.lambda1* bloss + self.lambda2 * rloss \n",
        "        loss = loss.to(device)\n",
        "        return loss\n",
        "\n",
        "\n",
        "train_loss = BregmanRankingLoss(model=model, batch_size=2,temperature=0.1, sigma=2 ,lambda1=1, lambda2=10) "
      ],
      "metadata": {
        "id": "JpCzr3zNX6Yl",
        "outputId": "d1c3067d-6ac0-459c-8700-f271bc6542f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/bionlp_bluebert_pubmed_mimic_uncased_L-12_H-768_A-12. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/bionlp_bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
        "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
        "\n",
        "# Train the model\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          epochs=num_epochs,\n",
        "          warmup_steps=warmup_steps,\n",
        "          callback=\"epoch\",\n",
        "          output_path='/content/drive/MyDrive/bregman_mimic_1',\n",
        "          optimizer_params={'lr': 3e-5},\n",
        "          checkpoint_path='/content/drive/MyDrive/bregman_mimic_1',\n",
        "          show_progress_bar=True,\n",
        "          checkpoint_save_steps=10000,\n",
        "          save_best_model=True,\n",
        "          use_amp=True  # Set to True, if your GPU supports FP16 cores\n",
        "          )\n",
        "     "
      ],
      "metadata": {
        "id": "pkaHH4QxX5bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}