{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/model/MIMIC/mimic_bregman_classification_modded_fullseq_length.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bpOzUfD53lts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a004c0-76a6-438c-d1cd-d90eafdd6a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed datasets-2.8.0 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 sentence_transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ7IqoZXAVjg",
        "outputId": "d0e9f431-27b5-48a9-bcd9-f72fe4560887"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "br19zMT6AoKn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/danielsaggau/IR_LDC.git"
      ],
      "metadata": {
        "id": "rUv6Xu7wSD_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IR_LDC/model/Longformer/Longformer/biobert_to_longformer.py"
      ],
      "metadata": {
        "id": "KQFwlBIsoVVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/mimic.jsonl.zip -d content"
      ],
      "metadata": {
        "id": "JQcrNr7mnbcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy(\"/content/content/mimic.jsonl\", \"/content/IR_LDC/model/MIMIC\")\n",
        "dataset = load_dataset(\"/content/IR_LDC/model/MIMIC/mimic-dataset.py\")"
      ],
      "metadata": {
        "id": "swGB8CPVAzg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_test = dataset['train'].train_test_split(test_size=0.1)\n",
        "dataset_test = dataset_train_test['test']\n",
        "dataset_sp = dataset_train_test['train'].train_test_split(test_size=0.1/0.9)\n",
        "dataset_train = dataset_sp['train']\n",
        "dataset_validation = dataset_sp['test']"
      ],
      "metadata": {
        "id": "4GCgc1F3AYoH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/models/bio-longformer\", use_auth_token=True, use_fast=True)"
      ],
      "metadata": {
        "id": "z4j3EoP3BHTb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"/content/models/bio-longformer\", use_auth_token=True,num_labels=19, problem_type='multi_label_classification')"
      ],
      "metadata": {
        "id": "_G1YMr3-HKa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) # fp16"
      ],
      "metadata": {
        "id": "-QVq_XkgFeuh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset['train']"
      ],
      "metadata": {
        "id": "fikpTYlCTVvF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = train_dataset.features['labels'].feature.num_classes\n",
        "label_ids = train_dataset.features['labels'].feature.names\n",
        "\n",
        "label_names = label_ids\n",
        "label_list = list(range(num_labels))"
      ],
      "metadata": {
        "id": "hX_UfmDUTRg7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   def preprocess_function(examples):\n",
        "        # Tokenize the texts\n",
        "        batch = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding='max_length',\n",
        "            max_length=2048, # for full run 4096\n",
        "            truncation=True)\n",
        "        \n",
        "        batch = tokenizer.pad(\n",
        "            batch,\n",
        "            padding='max_length',\n",
        "            max_length=2048, # for full run 4096\n",
        "            pad_to_multiple_of=8,\n",
        "        )\n",
        "        batch[\"label_ids\"] = [[1.0 if label in labels else 0.0 for label in label_list] for labels in examples[\"labels\"]]\n",
        "        return batch"
      ],
      "metadata": {
        "id": "01gQrulMS4-w"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = dataset.map(preprocess_function, batched=True, remove_columns=['labels'])"
      ],
      "metadata": {
        "id": "67R1GBXKOcvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   from transformers import EvalPrediction\n",
        "   def compute_metrics(p: EvalPrediction):\n",
        "        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "        preds = (expit(logits) > 0.5).astype(int)\n",
        "        label_ids = (p.label_ids > 0.5).astype(int)\n",
        "        macro_f1 = f1_score(y_true=label_ids, y_pred=preds, average='macro', zero_division=0)\n",
        "        micro_f1 = f1_score(y_true=label_ids, y_pred=preds, average='micro', zero_division=0)\n",
        "        return {'macro_f1': macro_f1, 'micro_f1': micro_f1}"
      ],
      "metadata": {
        "id": "DGA0EtELOM0p"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"modded_mimic_frozen\",\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=6,\n",
        "    per_device_eval_batch_size=6,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    metric_for_best_model=\"micro_f1\",\n",
        "    greater_is_better=True,\n",
        "    load_best_model_at_end = True,\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"mimic_modded_frozen\")"
      ],
      "metadata": {
        "id": "rciMxXfoOioO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f193e28-0c50-41a4-b681-d7c021d438ac"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']"
      ],
      "metadata": {
        "id": "7skfxV63SYaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "BePx6berkGDC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert pooling\n",
        "import torch\n",
        "from torch import nn\n",
        "class BertMeanPooler(nn.Module):\n",
        "          def __init__(self, config):\n",
        "             super().__init__()\n",
        "             self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "             self.activation = nn.Tanh()\n",
        "\n",
        "          def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "              mean_token_tensor = hidden_states.mean(dim=1)\n",
        "              pooled_output = self.dense(mean_token_tensor)\n",
        "              pooled_output = self.activation(pooled_output)\n",
        "              return pooled_output\n",
        "model.longformer.pooler = BertMeanPooler(model.config)\n",
        "print('model mean pooler loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Lumuo9RX9O",
        "outputId": "b761f102-25db-4d13-ef87-0ab53a9bfa8c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model mean pooler loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.longformer.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "_VOcllLyBG9d"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "rKImX0daCVPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "C7svTN1ARXAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from scipy.special import expit\n",
        "from sklearn.metrics import f1_score\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    compute_metrics=compute_metrics,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_data['test'],\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,    \n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8P9bv9MzOsgR",
        "outputId": "d42937df-424a-48ef-a482-eb0046c8f109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/modded_mimic_frozen is already a clone of https://huggingface.co/danielsaggau/modded_mimic_frozen. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/modded_mimic_frozen is already a clone of https://huggingface.co/danielsaggau/modded_mimic_frozen. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text, summary_id. If text, summary_id are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 30000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 50000\n",
            "  Number of trainable parameters = 605203\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "Initializing global attention on CLS token...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='137' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  137/50000 00:54 < 5:36:53, 2.47 it/s, Epoch 0.03/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n",
            "Initializing global attention on CLS token...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=tokenized_data['validation'])"
      ],
      "metadata": {
        "id": "G-34xgBJI_Zx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "0d0834d3-adfb-4a4f-896c-34926c0c78ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: summary_id, text. If summary_id, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1667' max='1667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1667/1667 02:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.35984161496162415,\n",
              " 'eval_macro_f1': 0.6777420171475501,\n",
              " 'eval_micro_f1': 0.7257478529594579,\n",
              " 'eval_runtime': 126.0732,\n",
              " 'eval_samples_per_second': 79.319,\n",
              " 'eval_steps_per_second': 13.222,\n",
              " 'epoch': 6.0}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}