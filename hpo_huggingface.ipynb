{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQAKVklyOYm7kYNHRuVRKj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/hpo_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjsGCbKz2_3d"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
        "                          Trainer, TrainingArguments)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "dataset = load_dataset('glue', 'mrpc')\n",
        "metric = load_metric('glue', 'mrpc')\n"
      ],
      "metadata": {
        "id": "jX7xN2wC3_x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(examples):\n",
        "    outputs = tokenizer(\n",
        "        examples['sentence1'], examples['sentence2'], truncation=True)\n",
        "    return outputs\n",
        "\n",
        "encoded_dataset = dataset.map(encode, batched=True)\n"
      ],
      "metadata": {
        "id": "qbN0owHQKM6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\n",
        "        'distilbert-base-uncased', return_dict=True)\n"
      ],
      "metadata": {
        "id": "_SpZIXhIKOSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "# Evaluate during training and a bit more often\n",
        "# than the default to be able to prune bad trials early.\n",
        "# Disabling tqdm is a matter of preference."
      ],
      "metadata": {
        "id": "uL0kv-bDKP0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"test\", evaluation_strategy=\"steps\", eval_steps=500, disable_tqdm=True)\n",
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        "    greater_is_better = True\n",
        ")\n",
        "# Default objective is the sum of all metrics\n",
        "# when metrics are provided, so we have to maximize it.\n",
        "trainer.hyperparameter_search(\n",
        "    direction=\"maximize\", \n",
        "    backend=\"ray\", \n",
        "    n_trials=10 # number of trials\n",
        ")"
      ],
      "metadata": {
        "id": "9TqQTzVBKRuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    # Choose among many libraries:\n",
        "    # https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\n",
        "    search_alg=HyperOptSearch(metric=\"objective\", mode=\"max\"),\n",
        "    # Choose among schedulers:\n",
        "    # https://docs.ray.io/en/latest/tune/api_docs/schedulers.html\n",
        "    scheduler=ASHAScheduler(metric=\"objective\", mode=\"max\"))"
      ],
      "metadata": {
        "id": "sFrYb6cgVI1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIp1uyZ4VRLh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}