{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bregman_Sbert",
      "provenance": [],
      "authorship_tag": "ABX9TyOKl+ooxJTMKYSFNd4DkhhC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/IR_LDC/blob/main/Bregman_Sbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lTQ3DF3tivPs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from typing import Union, Tuple, List, Iterable, Dict\n",
        "import torch.nn.functional as F\n",
        "from enum import Enum\n",
        "#from Sentence_Transformer import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TripletDistanceMetric(Enum):\n",
        "    \"\"\"\n",
        "    The metric for the triplet loss\n",
        "    \"\"\"\n",
        "    COSINE = lambda x, y: 1 - F.cosine_similarity(x, y)\n",
        "    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n",
        "    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)"
      ],
      "metadata": {
        "id": "fSq9llxhot5e"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements triplet loss. Given a triplet of (anchor, positive, negative),\n",
        "    the loss minimizes the distance between anchor and positive while it maximizes the distance\n",
        "    between anchor and negative. It compute the following loss function:\n",
        "    loss = max(||anchor - positive|| - ||anchor - negative|| + margin, 0).\n",
        "    Margin is an important hyperparameter and needs to be tuned respectively.\n",
        "    For further details, see: https://en.wikipedia.org/wiki/Triplet_loss\n",
        "    :param model: SentenceTransformerModel\n",
        "    :param distance_metric: Function to compute distance between two embeddings. The class TripletDistanceMetric contains common distance metrices that can be used.\n",
        "    :param triplet_margin: The negative should be at least this much further away from the anchor than the positive.\n",
        "    Example::\n",
        "        from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses\n",
        "        from sentence_transformers.readers import InputExample\n",
        "        model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "        train_examples = [InputExample(texts=['Anchor 1', 'Positive 1', 'Negative 1']),\n",
        "            InputExample(texts=['Anchor 2', 'Positive 2', 'Negative 2'])]\n",
        "        train_dataset = SentencesDataset(train_examples, model)\n",
        "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
        "        train_loss = losses.TripletLoss(model=model)\n",
        "    \"\"\"\n",
        "    def __init__(self, model: SentenceTransformer, distance_metric=TripletDistanceMetric.EUCLIDEAN, triplet_margin: float = 5):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.distance_metric = distance_metric\n",
        "        self.triplet_margin = triplet_margin\n",
        "\n",
        "\n",
        "    def get_config_dict(self):\n",
        "        distance_metric_name = self.distance_metric.__name__\n",
        "        for name, value in vars(TripletDistanceMetric).items():\n",
        "            if value == self.distance_metric:\n",
        "                distance_metric_name = \"TripletDistanceMetric.{}\".format(name)\n",
        "                break\n",
        "\n",
        "        return {'distance_metric': distance_metric_name, 'triplet_margin': self.triplet_margin}\n",
        "\n",
        "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
        "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
        "\n",
        "        rep_anchor, rep_pos, rep_neg = reps\n",
        "        distance_pos = self.distance_metric(rep_anchor, rep_pos)\n",
        "        distance_neg = self.distance_metric(rep_anchor, rep_neg)\n",
        "\n",
        "        losses = F.relu(distance_pos - distance_neg + self.triplet_margin)\n",
        "        return losses.mean()"
      ],
      "metadata": {
        "id": "dNFTBRKEtCxD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses\n",
        "from sentence_transformers.readers import InputExample\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ZJdyEl1vp31k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "train_examples = [InputExample(texts=['Anchor 1', 'Positive 1', 'Negative 1']),\n",
        "InputExample(texts=['Anchor 2', 'Positive 2', 'Negative 2'])]\n",
        "train_dataset = SentencesDataset(train_examples, model)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\n",
        "train_loss = losses.TripletLoss(model=model, distance_metric = TripletDistanceMetric.MANHATTAN)"
      ],
      "metadata": {
        "id": "BEkrTxrLrpeZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
      ],
      "metadata": {
        "id": "MfqEmNo1r43K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.', \n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "#Sentences are encoded by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "4Z13Z85XsHJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Two lists of sentences\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome']\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjBcG1M9swo3",
        "outputId": "2bf136b9-57bd-42ea-ae96-048b6ed9bd2a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.3298\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.2403\n",
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "      # Augmentation for text so masking \n",
        "      '''\n",
        "      params::tau augmentation function \n",
        "      Q: Masking in contrastive is just masking correlated samples\n",
        "      Q: Check SBERT\n",
        "      '''\n",
        "\n",
        "      # projection \n",
        "\n",
        "      '''\n",
        "      params::f projection function \n",
        "      params:: \n",
        "      '''\n",
        "\n",
        "      #subnetworks \n",
        "      \n",
        "      '''\n",
        "      params:: d bregman subnetworks \n",
        "      params:: \n",
        "      '''\n",
        "\n",
        "      #concatenation\n",
        "\n",
        "      '''\n",
        "      function::\n",
        "      '''\n",
        "\n",
        "      # computing triplet loss \n",
        "\n",
        "\n",
        "      # Computing bregman loss \n",
        "    def Divergence():\n",
        "      '''params:: logits_b\n",
        "         params:: labels_b\n",
        "      '''\n",
        "\n",
        "     # combine the two losses\n",
        "\n",
        "     TripletBregmanloss = lambda * TripletLoss + BregmanLoss #(lambda =2 in other paper) \n",
        "\n",
        "\n",
        "     # SGD update\n",
        "     TripletBregmanloss.backward()\n",
        "     update(model.params)\n",
        "  \n",
        "  def D(o1, o2):\n",
        "    p_star = argmax(o1)\n",
        "    q_star = argmax(o2)\n",
        "\n",
        "    # Bregman divergence (Eq.6)\n",
        "    return o1[p_star] - o1[q_star]\n",
        "\n",
        "\n",
        "# conversion D to phi \n",
        "def phi(D, sigma): \n",
        "  \n",
        "phi =\n"
      ],
      "metadata": {
        "id": "VQGO78WNysCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pairwise Divergence for computation "
      ],
      "metadata": {
        "id": "WuCK6CTBZ3l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BregmannLoss(nn.Module):\n",
        "      \"\"\" The Bregman loss should take a triplet (anchor, negative, positive) computing the loss for all valid triplets\n",
        "      Arguments: \n",
        "      :param model: \n",
        "      :param embedding:\n",
        "      negative\n",
        "      Returns:\n",
        "      Example:: \n",
        "      \"\"\"\n",
        "      def __init__(self, batch_size, temperature, sigma):\n",
        "        super(BregmanLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.sigma = sigma\n",
        "        self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "        \n",
        "      def pairwise_divergence(embeddings, squared =False):\n",
        "          max_output = torch.argmax(embeddings,1, output_type=torch.Tensor.int32)\n",
        "          one_to_n = torch.range(torch.Tensor.int(embedding)[0], output_type=torch.Tensor.int32)\n",
        "          max_indices = \n",
        "          max_val =\n",
        "          max_val_repeated =\n",
        "          repeated_max_out =\n",
        "          repeated_one_to_n = \n",
        "\n",
        "      def forward(self, out_a, out_b): \n",
        "        similarity_matrix = ()\n",
        "        features = torch.cat((out_a, out_b), dim=0)\n",
        "        \n",
        "#computing triplet \n",
        "        anchor_positive = torch.diag()\n",
        "        anchor_negative = torch.diag()\n",
        "\n",
        "      return div_matrix"
      ],
      "metadata": {
        "id": "Dv5cTTIJjsDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking triplets \n"
      ],
      "metadata": {
        "id": "WVFZJEhZZs8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "      def get_anchor_positive_mask(labels): \n",
        "        # indices equal =\n",
        "        indicies_not_equal = torch.logical_not(indicies_equal)\n",
        "        labels_equal = torch.equal(torch.unsqueeze(labels,0), torch.expand_dims(labels,1))\n",
        "        mask = torch.logical_not(indicies_not_equal, labels_equal)\n",
        "        return mask\n",
        "\n",
        "      def get_anchor_negative_mask(labels):\n",
        "        labels_equal = torch.equal(torch.unsquezee(labels, 0), torch.unsqueeze(labels,1))\n",
        "        mask = tf.logical_not(labels_equal)\n",
        "        return mask\n",
        "\n",
        "      def get_triplet_mask(labels):\n",
        "        # indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
        "        indicies_equal = torch.double(torch.eye(torch.size(labels)[0]), torch.gt) # what is t and what does this do ?\n",
        "        indicies_not_equal = torch.logical_not(indicies_equal)\n",
        "        i_not_j = torch.unsqueeze(indicies_not_equal, 2)\n",
        "        i_not_k = torch.unsqueeze(indicies_not_equal, 1)\n",
        "        j_not_k = torch.unsqueeze(indicies_not_equal, 0)\n",
        "        distinct_indicies = torch.logical_and(torch_logical_and(i_not_j, i_not_k, j_not_k))\n",
        "        label_equal = torch.equal(torch.unsqueeze(labels, 0),torch.unsqueeze(labels,1))\n",
        "        i_equal_j = torch.unsqueeze(label_equal, 2)\n",
        "        i_equal_k = torch.unsqueeze(label_equal, 1) #what does this mean? \n",
        "        valid_labels = torch.logical_and(i_equal_j, torch.logical_not(i_equal_k))\n",
        "        mask = torch.logical_and(distinct_indicies, valid_labels)  \n",
        "        return mask \n"
      ],
      "metadata": {
        "id": "IGtGJzX_ZsS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Compute Batch All Triplet Loss "
      ],
      "metadata": {
        "id": "5eGdarQpaEZB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUwcnXuvaD5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Bregman Loss "
      ],
      "metadata": {
        "id": "gZAk5nJ1Zwe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n"
      ],
      "metadata": {
        "id": "MM8o1SDgZ9De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code snippet kubrac\n",
        "def _pairwise_divergences(embed):\n",
        "\n",
        "    max_out = tf.math.argmax(embed, 1, output_type=tf.dtypes.int32)\n",
        "    one_to_n = tf.range(tf.shape(embed)[0], dtype=tf.dtypes.int32)\n",
        "    max_indices = tf.transpose(tf.stack([one_to_n, max_out]))\n",
        "    max_values = tf.gather_nd(embed, max_indices)\n",
        "    max_values_repeated = tf.transpose(tf.reshape(tf.tile(max_values, [tf.shape(embed)[0]]), [tf.shape(embed)[0], tf.shape(embed)[0]]))\n",
        "    repeated_max_out = tf.tile(max_out, [tf.shape(embed)[0]])\n",
        "    repeated_one_to_n = tf.tile(one_to_n, [tf.shape(embed)[0]])\n",
        "    mat_rotn = tf.reshape(tf.transpose(tf.reshape(repeated_one_to_n, [tf.shape(embed)[0], tf.shape(embed)[0]])), [-1])\n",
        "    new_max_indices = tf.transpose(tf.stack([mat_rotn, repeated_max_out]))\n",
        "    new_max_values = tf.gather_nd(embed, new_max_indices)\n",
        "    reshaped_new_max_values = tf.reshape(new_max_values, [tf.shape(embed)[0], tf.shape(embed)[0]])\n",
        "    div_matrix = tf.maximum(tf.subtract(max_values_repeated, reshaped_new_max_values), 0.0)  \n",
        "    \n",
        "#    #for differentiability, this version uses softmax instead of argmax\n",
        "#    sftmx = tf.nn.softmax(tf.multiply(1.0, embed))\n",
        "#    ES = tf.linalg.matmul(embed, sftmx, transpose_b=True)\n",
        "#    one_vec = tf.reshape(tf.ones([tf.shape(embed)[0]]), [1, tf.shape(embed)[0]])\n",
        "#    diag_ES = tf.reshape(tf.linalg.diag_part(ES), [1, tf.shape(embed)[0]])\n",
        "#    max_outputs = tf.linalg.matmul(diag_ES, one_vec, transpose_a=True)\n",
        "#    div_matrix = tf.maximum(tf.subtract(max_outputs, ES), 0.0)\n",
        "    \n",
        "    return div_matrix"
      ],
      "metadata": {
        "id": "MvG4UdNm2N-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # code snippet rezaei\n",
        "class BregmanLoss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature, sigma):\n",
        "        super(BregmanLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.sigma = sigma\n",
        "\n",
        "        self.mask = self.mask_correlated_samples(batch_size)\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "        \n",
        "        #self.similarity_f = nn.CosineSimilarity(dim=2)\n",
        "\n",
        "    def mask_correlated_samples(self, batch_size):\n",
        "        N = 2 * batch_size\n",
        "        mask = torch.ones((N, N), dtype=bool)\n",
        "        mask = mask.fill_diagonal_(0)\n",
        "        for i in range(batch_size):\n",
        "            mask[i, batch_size + i] = 0\n",
        "            mask[batch_size + i, i] = 0\n",
        "        return mask\n",
        "    \n",
        "    def b_sim(self, features):\n",
        "        mm = torch.max(features, dim=1)\n",
        "        indx_max_features = mm[1]\n",
        "        max_features = mm[0].reshape(-1, 1)\n",
        "        \n",
        "        # Compute the number of active subnets in one batch\n",
        "        eye = torch.eye(features.shape[1])\n",
        "        one = eye[indx_max_features]\n",
        "        num_max = torch.sum(one, dim=0)\n",
        "        \n",
        "        dist_matrix = max_features - features[:, indx_max_features]\n",
        "        \n",
        "        case = 2\n",
        "        if case == 0:\n",
        "            m2 = torch.divide(dist_matrix, torch.max(dist_matrix))\n",
        "            sim_matrix = torch.divide(torch.tensor([1]).to(features.device), m2 + 1)\n",
        "            \n",
        "        if case == 1:\n",
        "            gamma = torch.tensor([1]).to(features.device)\n",
        "            sim_matrix = torch.exp(torch.mul(-dist_matrix, gamma))\n",
        "            \n",
        "        if case == 2:\n",
        "            sigma = torch.tensor([self.sigma]).to(features.device)\n",
        "            sig2 = 2 * torch.pow(sigma, 2)\n",
        "            sim_matrix = torch.exp(torch.div(-dist_matrix, sig2))\n",
        "        \n",
        "        if case == 3:\n",
        "            sim_matrix = 1 - dist_matrix\n",
        "            \n",
        "        return sim_matrix, num_max\n",
        "\n",
        "    def forward(self, out_a, out_b):\n",
        "        \n",
        "        N = 2 * self.batch_size\n",
        "\n",
        "        features = torch.cat((out_a, out_b), dim=0)\n",
        "        \n",
        "        ###################################################\n",
        "        ### Computing Similarity Matrix ###################\n",
        "        sim_matrix, num_max = self.b_sim(features)\n",
        "        sim_matrix = sim_matrix / self.temperature\n",
        "        ###################################################\n",
        "        #sim_matrix = self.similarity_f(out.unsqueeze(1), out.unsqueeze(0)) / self.temperature\n",
        "\n",
        "        pos_ab = torch.diag(sim_matrix, self.batch_size)\n",
        "        pos_ba = torch.diag(sim_matrix, -self.batch_size)\n",
        "\n",
        "        positives = torch.cat((pos_ab, pos_ba), dim=0).reshape(N, 1)\n",
        "        negatives = sim_matrix[self.mask].reshape(N, -1)\n",
        "\n",
        "        labels = torch.zeros(N, dtype=torch.long).to(features.device)\n",
        "        logits = torch.cat((positives, negatives), dim=1)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        loss /= N\n",
        "        return loss, num_max"
      ],
      "metadata": {
        "id": "Y-ESO5tsvJNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # code snippet rezaei\n",
        "  # train for one epoch to learn unique features\n",
        "    def train(self, data_loader, epoch):\n",
        "        self.model.train()\n",
        "        batch_size = data_loader.batch_size\n",
        "        #bloss = BregMarginLoss(batch_size)\n",
        "        bloss = BregmanLoss(batch_size, self.temperature, self.sigma)\n",
        "        nt_xent = NT_Xent(batch_size, self.temperature)\n",
        "        \n",
        "        total_loss, total_num, tot_max, train_bar = 0.0, 0, 0, tqdm(data_loader)\n",
        "        tot_bloss, tot_nt_xent = 0.0, 0.0\n",
        "        num_max = torch.tensor([0])\n",
        "        for [aug_1, aug_2], target in train_bar:\n",
        "            aug_1, aug_2 = aug_1.to(self.device), aug_2.to(self.device)\n",
        "            feature_1, out_1 = self.model(aug_1)\n",
        "            feature_2, out_2 = self.model(aug_2)\n",
        "\n",
        "            # compute loss\n",
        "            loss, num_max = bloss(out_1, out_2)\n",
        "            tot_bloss += loss.item() * batch_size\n",
        "            if self.mixed_loss:\n",
        "                loss1 = nt_xent(feature_1, feature_2)\n",
        "                tot_nt_xent += loss1.item() * batch_size\n",
        "                loss = loss + self.lmbda * loss1\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            tot_max += num_max\n",
        "            total_num += batch_size\n",
        "            total_loss += loss.item() * batch_size\n",
        "            train_bar.set_description(\n",
        "                '{}Train{} {}Epoch:{} [{}/{}] {}Loss:{}  {:.4f} {}Active Subs:{} [{}/{}]'\n",
        "                .format(\n",
        "                    bcolors.OKCYAN, bcolors.ENDC,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    epoch,\n",
        "                    self.epochs,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    total_loss / total_num,\n",
        "                    bcolors.WARNING, bcolors.ENDC,\n",
        "                    len(torch.where(tot_max>10)[0]),\n",
        "                    tot_max.shape[0]))\n",
        "            \n",
        "        # warmup with nt_xent loss for the first 50 epochs\n",
        "        #if epoch >= 100:\n",
        "        self.scheduler.step()\n",
        "\n",
        "        return (total_loss/total_num,\n",
        "                tot_bloss/total_num,\n",
        "                tot_nt_xent/total_num,\n",
        "                self.scheduler.get_last_lr()[0])"
      ],
      "metadata": {
        "id": "krEYMA_-u59O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BregmanTriplet_loss(nn.Module):\n",
        "  '''\n",
        "  Arguments \n",
        "  param:: lambda \n",
        "  param::\n",
        "\n",
        "  Example \n",
        "  \n",
        "  '''"
      ],
      "metadata": {
        "id": "H-ftfW5LzqkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subnetworks"
      ],
      "metadata": {
        "id": "GksZFWiaENXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computation \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R8-N-AbLJHO4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}